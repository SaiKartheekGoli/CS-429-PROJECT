Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.49.172.162

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,374 total.  This list may not reflect recent changes .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: 

Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 110 pages are in this category, out of  110 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 20 subcategories, out of 20 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoTOC does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 10,533 total.


Title: Log in

Content: 

Title: None

Content: Populated by pages where  Template:Large category TOC  has been used  by {{ CatAutoTOC }}  on a  category with 10,001–20,000 pages . 
 Purge page to update totals 
 This category has the following 200 subcategories, out of 987 total.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: Recent changes

Content: This is a list of recent changes to Wikipedia.


Title: None

Content: 
 This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to  Wikipedia ? See the  contributing to Wikipedia  page or  our tutorial  for everything you need to know to get started. For a listing of  internal project pages  of interest, see the  department directory .
 For a listing of ongoing discussions and current requests, see the  Dashboard . 
 Welcome to the  community bulletin board , which is a page used for announcements from  WikiProjects  and other groups. Included here are coordinated efforts, events, projects, and other general announcements.
 Yearly or infrequent events 
 Monthly or continuous events 
 
 Also consider posting WikiProject, Task Force, and Collaboration news at  The Signpost' s  WikiProject Report  page. 
 Please include your signature when adding a listing here. 
 
 Latest  tech news  from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you.  Translations  are available.
 Problems 
 Changes later this week 
 Future changes 
 Tech news  prepared by  Tech News writers  and posted by  bot  •  Contribute  •  Translate  •  Get help  •  Give feedback  •  Subscribe or unsubscribe . 
 
 Discussions in the following areas have requested wider attention via  Requests for comment :
 
 
You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See  Wikipedia:Maintenance  or the   Task Center  for further information.)
 
Help counter  systemic bias  by creating  new articles on important women .
 
Help improve  popular pages , especially  those of low quality .
 This week's  article for improvement  is:
 Antarctic 
 Previous selections:
 Fire safety   · 
 Jean Gabin   ·  
 Abdalá Bucaram 
 This week's  backlog  of the week is:
 Category:Wikipedia requested audio of animals 
 To monitor or copy edit the upcoming  featured article  (FA) that will appear on the Main page, place  {{ FA-tomorrow }}  (with the four  curly braces ) on your user page. Help  Wikipedia  put its best foot forward.
 
 .mw-parser-output .sister-bar{display:flex;justify-content:center;align-items:baseline;font-size:88%;background-color:#fdfdfd;border:1px solid #a2a9b1;clear:both;margin:1em 0 0;padding:0 2em}.mw-parser-output .sister-bar-header{margin:0 1em 0 0.5em;padding:0.2em 0;flex:0 0 auto;min-height:24px;line-height:22px}.mw-parser-output .sister-bar-content{display:flex;flex-flow:row wrap;flex:0 1 auto;align-items:baseline;padding:0.2em 0;column-gap:1em;margin:0;list-style:none}.mw-parser-output .sister-bar-item{display:flex;align-items:baseline;margin:0.15em 0;min-height:24px;text-align:left}.mw-parser-output .sister-bar-logo{width:22px;line-height:22px;margin:0 0.2em;text-align:right}.mw-parser-output .sister-bar-link{margin:0 0.2em;text-align:left}@media screen and (max-width:960px){.mw-parser-output .sister-bar{flex-flow:column wrap;margin:1em auto 0}.mw-parser-output .sister-bar-header{flex:0 1}.mw-parser-output .sister-bar-content{flex:1;border-top:1px solid #a2a9b1;margin:0;list-style:none}.mw-parser-output .sister-bar-item{flex:0 0 20em;min-width:20em}}.mw-parser-output .navbox+link+.sister-bar,.mw-parser-output .navbox+style+.sister-bar,.mw-parser-output .portal-bar+link+.sister-bar,.mw-parser-output .portal-bar+style+.sister-bar,.mw-parser-output .sister-bar+.navbox-styles+.navbox,.mw-parser-output .sister-bar+.navbox-styles+.portal-bar{margin-top:-1px}   Kindness Campaign 


Title: None

Content: .mw-parser-output .introtosingle__main{position:relative;box-sizing:border-box;box-shadow:2px 2px 2px #CCC;max-width:100%;overflow:hidden;border:1px solid black;margin:auto;padding-bottom:20px}.mw-parser-output .introtosingle__main p{margin-bottom:2.0em}.mw-parser-output .introtosingle__main-withbackground{background-image:url("https://upload.wikimedia.org/wikipedia/commons/d/d9/Wikipedia-logo-v2-o10.svg");background-position:center -500px;background-repeat:no-repeat;background-size:auto 150%}.mw-parser-output .introtosingle__main-title{font-size:250%;line-height:150%;background:#777;color:#FFF;text-align:center;align-items:center;justify-content:center}.mw-parser-output .introtosingle__lead{background-color:#EEE;background-color:rgba(221,221,221,0.5);padding:30px 60px;margin-bottom:15px}.mw-parser-output .introtosingle__base{box-sizing:border-box;max-width:1100px;min-height:55px;margin:auto;padding:5px 20px;font-size:1.1em;background:#EEE;border:1px solid lightgrey}.mw-parser-output .introtosingle__columns{display:flex;flex-direction:row;flex-wrap:wrap-reverse;justify-content:center}.mw-parser-output .introtosingle__columns-left,.mw-parser-output .introtosingle__columns-left-noborder,.mw-parser-output .introtosingle__columns-right{display:inline-block;flex:1 1 0;align-self:flex-end;vertical-align:top;min-width:200px;max-width:300px;padding:10px}.mw-parser-output .introtosingle__columns-left{text-align:right;justify-content:right;border-right:solid 1px #ddd}.mw-parser-output .introtosingle__columns-left-noborder{text-align:right;justify-content:right}.mw-parser-output .introtosingle__columns-right{text-align:left;justify-content:left}@media screen and (min-width:1101px){.mw-parser-output .introtosingle__main{max-width:1100px}} 
 Wikipedia  is made by  people like  you . 
 Get started 
 Policies and Guidelines 
 Editing 
 Referencing 
 Images 
 Tables 
 Editing 
 Referencing 
 Images 
 Tables 
 Talk pages 
 Navigating Wikipedia Manual of Style Conclusion 
 
 View all as single page 
 For more training information, see also: 
 Full help contents page 
 Training for students 
 A single-page guide to contributing 
 A training adventure game 
 Resources for new editors 
 


Title: None

Content: 
 .mw-parser-output .helpContents-wrapper{display:flex;flex-wrap:wrap}.mw-parser-output .helpContents-header{flex:1 0 100%;padding-bottom:1em;border-bottom:1px solid #a2a9b1;text-align:center}.mw-parser-output .helpContents-section{position:relative;flex:1 0 50%;min-width:380px;box-sizing:border-box;margin:1em 0;padding-left:30px;padding-right:20px}.mw-parser-output .helpContents-section h2{font-size:21px;padding:0;margin-top:1em;margin-bottom:0.25em;line-height:1.5}.mw-parser-output .helpContents-section ul{margin-left:0;padding-left:0}.mw-parser-output .helpContents-icon{position:absolute;top:25px;left:0;line-height:1;opacity:0.7}.mw-parser-output .helpContents-additional-search{text-align:center;margin-bottom:1em} 
 This page provides  help with the most common questions about  Wikipedia .
 You can also search Wikipedia's help pages using the search box below, or browse the  Help menu  or the  Help directory .
 The  Readers' FAQ  and our  about page  contain the most commonly sought information about Wikipedia.
 For simple searches, there is a search box at the top of every page. Type what you are looking for in the box. Partial matches will appear in a dropdown list. Select any page in the list to go to that page. Or, select the magnifying glass "Go" button, or press  .mw-parser-output .keyboard-key{border:1px solid #aaa;border-radius:0.2em;box-shadow:0.1em 0.1em 0.2em rgba(0,0,0,0.1);background-color:#f9f9f9;background-image:linear-gradient(to bottom,#eee,#f9f9f9,#eee);color:#000;padding:0.1em 0.3em;font-family:inherit;font-size:0.85em} ↵ Enter , to go to a full search result. For advanced searches, see  Help:Searching .
 There are other ways to browse and explore Wikipedia articles; many can be found at  Wikipedia:Contents . See our  disclaimer  for cautions about Wikipedia's limitations.
 For  mobile access , press the  mobile view  link at the very bottom of every  desktop view  page.
 Contributing is easy: see  how to edit a page . For a quick summary on participating, see  contributing to Wikipedia , and for a friendly tutorial, see  our introduction . For a listing of introductions and tutorials by topic, see  getting started . The  Simplified Manual of Style  and  Cheatsheet  can remind you of basic wiki markup.
 Be bold  in improving articles! When adding facts, please  provide references  so others may verify them. If you are affiliated with the article subject, please see our  conflict of interest guideline .
 The  simple guide to vandalism cleanup  can help you undo malicious edits.
 If you're looking for places you can help out, the  Task Center  is the place to go, or check out what else is happening at the  community portal . You can practice editing and experiment in  a  sandbox your  sandbox .
 If there is a problem with an article about yourself, a family member, a friend or a colleague, please read  Biographies of living persons/Help .
 If you spot a problem with an article, you can fix it directly, by clicking on the "Edit" link at the top of that page.  See the "edit an article" section of this page  for more information.
 If you don't feel ready to fix the article yourself, post a message on the article's  talk page . This will bring the matter to the attention of others who work on that article. There is a "Talk" link at the beginning of  every  article page.
 You can  contact us . If it's an article about you or your organization, see  Contact us – Subjects .
 Check  Your first article  to see if your topic is appropriate, then the  Article wizard  will walk you through creating the article.
 Once you have created an article, see  Writing better articles  for guidance on how to improve it and what to include (like reference  citations ).
 For contributing images, audio or video files, see the  Introduction to uploading images . Then the  Upload wizard  will guide you through that process.
 Answers to common problems can be found at  frequently asked questions .
 Or check out  where to ask questions or make comments .
 New users should seek help at the  Teahouse  if they're having problems editing Wikipedia.  
 More complex questions can be posed at the  Help desk . Volunteers will respond as soon as they're able.
 Or  ask for help on your talk page  and a volunteer will visit you there!
 You can get live help with editing in the  help chatroom .
 For help with technical issues, ask at the  Village pump .
 If  searching Wikipedia  has not answered your question ( for example, questions like "Which country has the world's largest fishing fleet?" ), try the  Reference Desk . Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need.
 Where to request changing your account name depends on the type of rename:
 Search  Frequently Asked Questions 
 Search the  help desk archives 
 


Title: None

Content: 
 Introduction 
 Readers 
How to report a problem with an article, or find out more information.
 Article subjects 
Problems with articles about you, your company, or somebody you represent.
 Licensing 
How to copy Wikipedia's information, donate your own, or report unlicensed use of your information.
 Donors 
Find out about the process, how to donate, and information about how your money is spent.
 Press and partnerships 
If you're a member of the press looking to contact Wikipedia, or have a business proposal for us.
 
 Back to main page 
 
 Thank you for your interest in contacting Wikipedia. Before proceeding, some important disclaimers:
 The links on the left should direct you to how to contact us or resolve problems. If you cannot find your issue listed there, you can email helpful, experienced volunteers at  .mw-parser-output .monospaced{font-family:monospace,monospace} info-en wikimedia.org . Please refrain from emailing about disagreements with content; they will not be resolved via email.
 
 
 
 
 


Title: None

Content: 
 
 
 Wikipedia  is a  free online encyclopedia  that anyone can edit, and  millions already have .
 Wikipedia's purpose  is to benefit readers by presenting information on all branches of  knowledge . Hosted by the  Wikimedia Foundation , it consists of  freely editable  content, whose articles also have numerous links to guide readers towards more information.
 Written collaboratively  by largely anonymous volunteers known as  Wikipedians , Wikipedia articles can be edited by anyone with  Internet access  (and who is not presently  blocked ), except in  limited cases  where  editing  is restricted to prevent disruption or  vandalism . Since its creation on  January 15, 2001 , it has grown into the world's  largest  reference  website , attracting  over a billion visitors monthly . Wikipedia currently has more than  sixty-two million articles  in  more than 300 languages , including 6,813,365 articles in  English , with 123,552 active contributors in the past month.
 Wikipedia's fundamental principles are summarized in its  five pillars . The  Wikipedia community  has developed many  policies and guidelines , although editors do not need to be familiar with them before contributing.
 Anyone  can  edit  Wikipedia's text, references, and images. What is written is more important than who writes it. The content must conform with Wikipedia's  policies , including being  verifiable  by published  sources . Editors'  opinions , beliefs, personal experiences,  unreviewed research , libelous material, and  copyright violations  will not remain. Wikipedia's software allows easy reversal of errors, and experienced editors  watch  and  patrol  bad edits.
 Wikipedia differs from printed references in important ways . It is continually created and updated, and encyclopedic articles on new events appear within minutes rather than months or years. Because anyone can improve Wikipedia, it has become more comprehensive than any other encyclopedia. Its contributors enhance its articles' quality and quantity, and remove misinformation, errors and  vandalism . Any reader can fix a mistake or add more information to what has already been written (see  Researching with Wikipedia ). 
 Begin by simply clicking the  Edit  or  Edit source   buttons or the pencil icon    at the top of any non-protected page or section. Wikipedia has tested the  wisdom of the crowd  since 2001 and found that it succeeds.


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: 

Title: None

Content: This category contains  project pages  which have been  protected  from  page moves . They can still be moved by  administrators . Fully protected pages – those listed in  Category:Wikipedia protected pages  and its subcategories – are also move-protected; they are protected from both editing  and  moves.
 Use {{ pp-move }} or {{ pp-move-indef }} to add pages to this category. This should only be done if the page is in fact protected – adding the template does not in itself protect the page. 
 The following 200 pages are in this category, out of approximately 583 total.  This list may not reflect recent changes .


Title: User contributions for Somebodyidkfkdt

Content: 

Title: None

Content: 
 This category has the following 2 subcategories, out of 2 total.
 The following 139 pages are in this category, out of  139 total.  This list may not reflect recent changes .


Title: None

Content: This category contains the  Help:Contents , its subpages and other pages used to index wikipedia's help pages.
 The following 15 pages are in this category, out of  15 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 5 subcategories, out of 5 total.
 The following 124 pages are in this category, out of  124 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 13 subcategories, out of 13 total.
 The following 52 pages are in this category, out of  52 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 110 pages are in this category, out of  110 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: 

Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: Recent changes

Content: This is a list of recent changes to Wikipedia.


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: 

Title: None

Content: 
This page provides documentation for the experimental  Wikipedia:File Upload Wizard , which is currently in testing stage.
 The wizard consists of one normal wiki page, currently located at  Wikipedia:File Upload Wizard , a page of client-side Javascript code, currently at  MediaWiki:FileUploadWizard.js , and a corresponding .css page, currently at  MediaWiki:FileUploadWizard.css . Almost all of the text content used by the wizard (forms, prompts, warnings etc.) is contained as standard wikitext in the main page, inside hidden  <div> -elements and nested tables. Interactive elements that cannot be created by normal wiki text ( <form>  elements, buttons, input text fields, dropdowns boxes etc.) are created by the Javascript when the page loads. Empty  <span id="placeholderXYZ"/>  elements in the wikitext mark the positions where they will be inserted. To activate the Javascript, the wikipage must be loaded with a  withJS=  parameter.
 The Javascript code has been tested with the Firefox 10.0 browser so far. It makes some use of the  jQuery  library, which is commonly used in Wikipedia scripts. Data used during the input and upload process are stored in a Javascript object named  window.fuw . A representation of the input data present at any given time during the input process is cached in  window.fuw.opts , and data representing the current status of various warning conditions (e.g. bad filenames, missing target articles etc.) are stored in  window.fuw.warn .
 The most important functions of the script are:
 In the current testing version, this function is also called at the end of each  fuwUpdateOptions()  call, in order to maintain a constant preview of the output for testing purposes. In the final version, it will be sufficient to have it called only once, from the onClick event of the submit button (after  validateInput()  has returned true).
 The questionnaire page currently contains three separate  <form>  elements. The first,  <form id="fuwTargetForm"> , is the one that is actually used for uploading. The only overt element contained in it is the file selection box. All other upload parameters are present in the form of  <input type="hidden"/>  elements, whose values are filled in prior to uploading by the  fuw.collectInput()  function.
 The upload is done via a standard  form.submit()  from this target form, with the form's action set to the  //en.wikipedia.org/w/api.php  interface. The API's return message is diverted into a hidden  <iframe>  element, whose  onLoad  event then triggers the function that displays the success message and hides the main questionnaire (thus "faking" an asynchronous AJAX call – this is necessary because a normal AJAX call cannot access file upload data).
 The second  <form>  element,  <form id="fuwScriptForm"> , contains all the other visible controls of the input questionnaire.
 In the testing version, there is a third  <form>  element,  <form id="fuwTestForm"> , which contains the preview fields at the bottom of the page.
 Apart from the main upload action, the script sends additional server requests in the following situations. All of these are submitted via asynchronous  AJAX  requests to the  api.php  interface, using the  jQuery  ajax wrapper  $.ajax([...]) .
 The questionnaire currently has twelve sub-sections designed for twelve different types of files, five of them free and seven non-free.
 The script attempts to recognize the following status levels of the current user, based on information taken partly from the standard  mw.config  variables and partly from a call to the API:
 The script will stop and only display a notification message if run by a logged-out user, since logged-out users can't upload files.
 The script will display a notification that users who are not yet autoconfirmed can't upload files locally. However, the rest of the script will run normally for them, with only the "local upload" submit button greyed out but the "upload to Commons" button active.
 The following distinctions can also be made but do not currently lead to any actual differences in behaviour of the script:
 Status set if the user has fewer than 100 edits. Such users might be offered a somewhat more gentle and detailed set of instructions in the future.
 Status set if the user has had more than 3 image-related standard warnings on their talkpage among the latest 30 talkpage edits. Such users might be offered a somewhat more urgently worded set of copyright-related instructions in the future.
 All other users; these users might be offered a somewhat shorter and less wordy set of instructions in the future.
 The wizard produces image description pages following the standard format:
 For free files, it uses the standard {{ Information }} template. For non-free files, it uses a new {{ Non-free use rationale 2 }} template, differing from the more common {{ Non-free use rationale }} in a few details. Its parameters are designed to correspond to the individual NFCCs more closely than the old template, and also to be compatible with those of the free {{ Information }} template wherever possible.
 The fair use rationale template uses the following parameters:
 The different sub-options described above are chosen in such a way that each of them corresponds to a typical situation where certain FUR fields are either important or trivial/predictable. The wizard will automatically provide brief pre-filled standard statements (sometimes as short as "n.a.") for the predictable parameters in each case. The combination of pre-filled and user-provided parts is as follows ("minimality" always requires manual input):


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: Log in

Content: 

Title: None

Content: 
 The  World Intellectual Property Organization Copyright Treaty  ( WIPO Copyright Treaty  or  WCT ) is an international  treaty  on  copyright law  adopted by the member states of the  World Intellectual Property Organization  (WIPO) in 1996. It provides additional protections for  copyright  to respond to advances in information technology since the formation of previous copyright treaties before it. [4]  As of August 2023, the treaty has 115 contracting parties. [5]  The WCT and  WIPO Performances and Phonograms Treaty , are together termed WIPO "internet treaties". [6] 
 During the earlier stages of negotiations, the WCT was seen as a protocol to the  Berne Convention , constituting an update of that agreement since the 1971 Stockholm Conference. [7]  However, as any amendment to the Berne Convention required unanimous consent of all parties, the WCT was conceptualized as an additional treaty which supplemented the Berne Convention. [8]  The collapse of negotiations around the extension of the Berne Convention during the 1980s saw the shifting of the forum to the GATT, resulting in  the TRIPS Agreement . [9] [10]  Thus, the nature of any copyright treaty by the  World Intellectual Property Organization  became considerably narrower, being limited to addressing the challenges posed by digital technologies.
 The WCT emphasizes the incentive nature of copyright protection, claiming its importance to creative endeavours. [7]  It ensures that  computer programs  are protected as literary works (Article 4), and that the arrangement and selection of material in  databases  is protected (Article 5). It provides authors of works with control over their rental and distribution in Articles 6 to 8, which they may not have under the  Berne Convention  alone. It also  prohibits circumvention of technological measures  for the protection of works (Article 11) and unauthorized modification of rights management information contained in works (Article 12).
 The treaty has been criticised for being too broad (for example in its prohibition of circumvention of technical protection measures, even where such circumvention is used in the pursuit of legal and fair use rights) and for applying a "one size fits all" standard to all signatory countries, despite their widely differing stages of economic development and knowledge industry.
 The WIPO Copyright Treaty is implemented in United States law by the  Digital Millennium Copyright Act  (DMCA). By Decision 2000/278/EC of 16 March 2000, the  Council of the European Union  approved the treaty on behalf of the European Community.  European Union Directives  which largely cover the subject matter of the treaty are:  Directive 91/250/EC , creating copyright protection for software;  Directive 96/9/EC  on copyright protection for databases; and  Directive 2001/29/EC , prohibiting devices for circumventing "technical protection measures", such as  digital rights management  (also known as DRM).


Title: None

Content: The Internet is not controlled by a single person or body in the way that a corporation or a state might be. Instead, different functions are carried out by different bodies, in a way which is still evolving. This category pulls together articles on the organization of the Internet and on the bodies and groups which have a role in its development and operation. 
 The use of the word "governance" here probably implies more power than actually exists in these bodies, as they essentially represent just a collection of standards for sharing traffic between networks. 
 This category has the following 4 subcategories, out of 4 total.
 The following 54 pages are in this category, out of  54 total.  This list may not reflect recent changes .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: Edit instructions 
 Armed conflicts and attacks 
 International relations  
 Politics and elections  
 Armed conflicts and attacks 
 Disasters and accidents  
 Politics and elections  
 Armed conflicts and attacks 
 Disasters and accidents 
 International relations 
 Law and crime 
 Armed conflicts and attacks 
 Business and economy 
 International relations 
 Law and crime 
 Politics and elections 
 Armed conflicts and attacks 
 Disasters and accidents 
 International relations 
 Sports 
 Armed conflicts and attacks 
 Business and economy 
 Disasters and accidents 
 International relations 
 Law and crime 
 Politics and elections 
 Science and technology 
 Sports 
 
 Armed conflicts and attacks 
 Business and economy 
 Disasters and accidents 
 Politics and elections 
 Science and technology 
 Sports 


Title: None

Content: 
 Explore the vast knowledge of  Wikipedia  through these helpful resources. If you have a specific topic in mind, use Wikipedia's search box. If you don't know exactly what you are looking for or wish to explore broad areas, click on a link in the header menu at the top of this page, or begin your browsing below:
 Wikipedia organizes its content into distinct subject classifications, each with further subdivisions.
 Explore the diverse cultures, arts, beliefs, and customs of human societies.
 Discover the wonders of Earth's lands, features, inhabitants, and planetary phenomena.
 Learn about physical, mental, and social well-being.
 Dive into the past through written records and scholarly exploration.
 Explore chronological events through our comprehensive timelines.
 Stay up-to-date with encyclopedia entries covering ongoing events.
 Delve into diverse human actions, from leisure and entertainment to industry and warfare.
 Explore the study of quantity, structure, space, and change.
 Understand natural phenomena through empirical evidence, observations, and experiments.
 Learn about collective entities, ethnic groups, and nations.
 Dive deep into fundamental questions about existence, knowledge, values, and more.
 Access comprehensive information collections compiled for easy retrieval.
 Refer to various third-party classification systems linked to Wikipedia articles.
 Access sources on specific topics for further reading or verification.
 Explore social-cultural systems, beliefs, ethics, and more.
 Understand collectives, social interactions, political authority, and cultural norms.
 Learn about techniques, skills, methods, and processes in technology and science.
 Get summaries of broad topics with links to subtopics, biographies, and related articles.
 Explore topics in outline format, linking to more detailed articles.
 Find enumerations of specific types, such as lists of countries and people.
 Access featured articles, images, news, and more through thematic portals.
 Access lists of terms with definitions through alphabetical glossaries.
 Browse Wikipedia's category pages, which index articles by subject.
 Explore subjects that demand high-quality articles, grouped by importance.
 Discover Wikipedia's best, reviewed and vetted for quality.
 Explore well-written, factually accurate articles that meet editorial standards.
 Listen to Wikipedia articles as spoken word recordings.
 Browse Wikipedia's articles alphabetically.
 Topics 
 Types 
 Places, people and times 
 Indices 


Title: None

Content: Kathleen Ferrier  (22 April 1912 – 8 October 1953) was an English  contralto  who achieved an international reputation as a stage, concert and recording artist. During the Second World War she performed regularly with the  Council for the Encouragement of Music and the Arts . In 1946 she made her stage debut as Lucretia in the world premiere of  Benjamin Britten 's  The Rape of Lucretia , and a year later she appeared as Orfeo in  Christoph Willibald Gluck 's  Orfeo ed Euridice . As a recitalist, Ferrier's repertoire included works by  Bach ,  Brahms ,  Mahler  and  Elgar . Forming working relationships with the conductors  John Barbirolli  and  Bruno Walter  and the accompanist  Gerald Moore , she became known internationally through her three tours of the United States and her many visits to continental Europe.  She continued to perform and record  after being diagnosed with breast cancer in 1951. Among her many memorials, the  Kathleen Ferrier Memorial Scholarship Fund  makes annual awards to aspiring young singers. ( Full article... )
 April 22 
 Seventeen people have held the office of President and Chancellor of New York University  (NYU), a private  research university  in  New York City . The president of  New York University  is its  chief executive officer  and is elected by the university's board of trustees, of which the president is a member  ex officio . From NYU's foundation by  Albert Gallatin  in 1831 until 1956, the head of NYU was the  chancellor . That year, the office became known as "president and chancellor", or "president" for short. The president recommends persons to fill senior offices, including the  provost , executive vice president,  general counsel , and  deans , who are then appointed by the board. The president also presides over the  university senate  and confers all degrees, with the board's authorization and upon certification of a student by the faculty. The incumbent president,  Linda G. Mills   (pictured) , assumed office on July 1, 2023 and became NYU's first female president. ( Full list... )
 Pelophylax cypriensis , commonly known as the Cyprus water frog, is a species of frog in the family Ranidae, the  true frogs . It is  endemic  to Cyprus. It is a medium-sized frog, with females (body length up to 75 mm, 3.0 in) being larger than males (up to 65 mm, 2.6 in). The skin is rather warty and colouration varies widely. There are four unwebbed toes on the front legs and five webbed toes on the hindlegs. Males have paired external  vocal sacs . This Cyprus water frog was photographed under the Elia Bridge in  Limassol District , Cyprus.
 Photograph credit:  Charles J. Sharp Wikipedia is written by volunteer editors and hosted by the  Wikimedia Foundation , a non-profit organization that also hosts a range of other volunteer  projects :
 This Wikipedia is written in  English . Many  other Wikipedias are available ; some of the largest are listed below.


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: The  United Peoples' Party  (UPP) was founded by  Kazi Zafar Ahmed  in 1974. 
 Ahmed took the UPP to a coalition government with President  Ziaur Rahman  after he assumed the presidency through a referendum.  [1] 
Ahmed became Minister of Education, but left the coalition due to irreconcilable differences. 
However, many of his former colleagues joined Zia's new party, the  Bangladesh Nationalist Party  (BNP). Mr. Ahmed also played a leading role in the anti military role of President  Hussain Muhammad Ershad . But the period since 1975 in Bangladesh witnessed realignment of politics and leaders leaving their old parties and joining new ones. Ahmed dissolved his UPP and joined President Ershad's  Jatiya Party  (JP) [2] 
 
 This  Bangladesh -related article is a  stub . You can help Wikipedia by  expanding it .

Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: 
 To add a page to this category, use  {{ Pp }}  or one of the more specific  full-protection templates . This should only be done if the page is indeed protected; adding the template does not in itself protect the page.
 For a dynamic and sortable list of all protected pages, see  Special:ProtectedPages . For reports of protected pages, see  Wikipedia:Database reports#Protections .
 For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 This category has the following 12 subcategories, out of 12 total.
 The following 64 pages are in this category, out of  64 total.  This list may not reflect recent changes .


Title: None

Content: This category contains  project pages  which have been  protected  from  page moves . They can still be moved by  administrators . Fully protected pages – those listed in  Category:Wikipedia protected pages  and its subcategories – are also move-protected; they are protected from both editing  and  moves.
 Use {{ pp-move }} or {{ pp-move-indef }} to add pages to this category. This should only be done if the page is in fact protected – adding the template does not in itself protect the page. 
 The following 200 pages are in this category, out of approximately 583 total.  This list may not reflect recent changes .


Title: None

Content: 

 The  Wikimedia Foundation, Inc. , abbreviated  WMF , is an American  501(c)(3)   nonprofit organization  headquartered in  San Francisco ,  California , and registered there as  a charitable foundation . [5]  It is best known as the host of  Wikipedia , the seventh  most visited website  in the world. However, the foundation also hosts 14 other related content projects. It also supports the development of  MediaWiki , the  wiki  software that underpins them all. [6] [7] [8] 
 The Wikimedia Foundation was established, in 2003 in  St. Petersburg, Florida , by  Jimmy Wales  as a nonprofit way to fund  Wikipedia ,  Wiktionary , and other  crowdsourced  wiki projects. [1]  (Until then, they had been hosted by  Bomis , Wales's for-profit company.) [1]  The Foundation finances itself mainly through millions of small donations from Wikipedia readers, collected through email campaigns and annual fundraising banners placed on Wikipedia and its sister projects. [9]  These are complemented by grants from philanthropic organizations and tech companies, and starting in 2022, by services income from  Wikimedia Enterprise .
 The Foundation has grown rapidly throughout its existence. As of December 31, 2023, it has employed over 700 staff and contractors, with annual revenues of $180.2 million, annual expenses of $169 million, net assets of $255 million and a growing endowment, which surpassed $100 million in June 2021.
 Officially, the Wikimedia Foundation's mission is "to empower and engage people around the world to collect and develop educational content under a free license or in the public domain, and to disseminate it effectively and globally." [10] 
 To serve this mission, the Wikimedia Foundation provides the technical and organizational infrastructure to enable members of the public to develop wiki-based content in languages across the world. [10]  The foundation does not write or curate any of the content on the wikis itself. [11]  Instead, this is done by editors who work as volunteers, such as the  Wikipedians  who create and maintain Wikipedia. However, the foundation does collaborate with a network of individual volunteers and affiliated organizations, such as Wikimedia chapters, thematic organizations, user groups and other partners.
 The Wikimedia Foundation promises in its mission statement to make useful information from its projects available on the internet free of charge in perpetuity. [10]  It also engages in  political advocacy . [12]  The Foundation's strategic direction, formulated in 2017, envisages that it "will become the essential infrastructure of the ecosystem of free knowledge" by 2030. [13] 
 Jimmy Wales  and  Larry Sanger  founded Wikipedia in 2001 as a feeder project to supplement  Nupedia . The project was originally funded by  Bomis , Wales's for-profit business, and edited by a rapidly growing community of volunteer editors. The early community discussed a variety of ways to support the ongoing costs of upkeep, and was broadly opposed to running ads on the site, [14]  so the idea of setting up a charitable foundation gained prominence. [15]  That also addressed an open question of what entity should hold onto trademarks for the project.
 The name "Wikimedia", a  compound  of  wiki  and  media , was coined by American author  Sheldon Rampton  in a post to the English Wikipedia mailing list in March 2003, [16]  three months after  Wiktionary  became the second wiki-based project hosted on the original server. The Wikimedia Foundation itself was incorporated in  St. Petersburg, Florida  on June 20, 2003. [1] [17] [18]  A small fundraising campaign to keep the servers running was run in October 2003. [19]  In 2005, the Foundation was granted section  501(c)(3)  status by the U.S.  Internal Revenue Code  as a public charity, making donations to the Foundation  tax-deductible  for U.S. federal income tax purposes. [20]  Its  National Taxonomy of Exempt Entities  (NTEE) code is B60 ( Adult ,  Continuing education ). [21] [22] 
 The Foundation filed an application to trademark the name  Wikipedia   in the US  on September 14, 2004. The mark was granted registration status on January 10, 2006. Trademark protection was accorded also by Japan on December 16, 2004, and by the  European Union  on January 20, 2005. Subsets of Wikipedia were already being distributed in book and DVD form, and there were discussions about licensing the logo and wordmark. [23] 
 On December 11, 2006, the Foundation's board noted that it could not become a  membership organization , as initially planned but not implemented, due to an inability to meet the registration requirements of Florida statutory law. The bylaws were accordingly amended to remove all references to membership rights and activities. [24] 
 In 2007, the Foundation decided to move its headquarters from Florida to the  San Francisco Bay Area . Considerations cited for choosing San Francisco were proximity to like-minded organizations and potential partners, a better talent pool, as well as cheaper and more convenient international travel. [25] [26] [27]  The move was completed by January 31, 2008, into a headquarters on Stillman Street in San Francisco. [28]  It later moved to New Montgomery Street, and then to  One Montgomery Tower . [29] 
 On October 25, 2021, the Foundation launched  Wikimedia Enterprise , a commercial Wikimedia content delivery service aimed at groups that want to use high-volume APIs, starting with  Big Tech  enterprises. [30] [31]  In June 2022,  Google  and the  Internet Archive  were announced as the service's first customers, though only Google will pay for the service. [32]  The same announcement noted a shifting focus towards smaller companies with similar data needs, supporting the service through "a lot paying a little".
 Wikimedia Enterprise  is a commercial product by the Wikimedia Foundation to provide, in a more easily consumable way, the data of the Wikimedia projects, including  Wikipedia . [33]  It allows customers to retrieve data at large scale and high availability through different formats like  Web APIs , data snapshots or  streams .
 It was first announced in March 2021 [34] [35]  and launched on October 26, 2021. [36] [37] 
 Google  and the  Internet Archive  were its first customers, although Internet Archive is not paying for the product. [36]  A  New York Times Magazine  article was reporting that Wikimedia Enterprise made $3.1 million in total revenue in 2022. [33] 
 Content on most Wikimedia project  websites  is licensed for redistribution under  v4.0  of the  Attribution  and  Share-alike   Creative Commons licenses . The Foundation owns and operates 11 wikis that are written, curated, designed, and governed by their communities of volunteer editors. Any member of the public is welcome to contribute; registering a named user account is optional. These wikis follow a  free content  model, with the stated goal of disseminating knowledge to the world. They include, by launch date:
 Certain additional projects provide infrastructure or coordination of the free knowledge projects. These include:
 Wikimedia affiliates are independent and formally recognized groups of people working together to support and contribute to the Wikimedia movement. The Wikimedia Foundation officially recognizes three types of affiliates: chapters, thematic organizations, and user groups. Affiliates organize and engage in activities to support and contribute to the Wikimedia movement, such as regional conferences, outreach,  edit-a-thons ,  hackathons ,  public relations ,  public policy  advocacy,  GLAM  engagement, and  Wikimania . [38] [39] [40]  While many of these things are also done by individual contributors or less formal groups, they are not referred to as affiliates.
 Wikimedia chapters and thematic organizations are  incorporated  non-profit organizations. They are recognized by the Foundation as affiliates officially when its board does so. The board's decisions are based on recommendations of an  Affiliations Committee  (AffCom), composed of Wikimedia community members, which reports regularly to the board. The Affiliations Committee directly approves the recognition of unincorporated user groups. Affiliates are formally recognized by the Wikimedia Foundation, but are independent of it, with no legal control of or responsibility for Wikimedia projects and their content. [39] [40] [41] 
 The Foundation began recognizing chapters in 2004. [42]  In 2012, the Foundation approved, finalized and adopted the thematic organization and user group recognition models. An additional model for movement partners, was also approved, but as of May 19, 2022 [update]  has not yet been finalized or adopted. [40] [43] 
 Wikimania is an annual global conference for Wikimedians and Wikipedians, started in 2005. The first Wikimania was held in  Frankfurt , Germany, in 2005. Wikimania is organized by a committee supported usually by the local national chapter, with support from local institutions (such as a library or university) and usually from the Wikimedia Foundation. Wikimania has been held in cities such as  Buenos Aires , [44]   Cambridge , [45]   Haifa , [46]   Hong Kong , [47]   Taipei ,  London , [48]   Mexico City , [49]   Esino Lario ,  Italy , [50]   Montreal ,  Cape Town , and  Stockholm . The 2020 conference scheduled to take place in  Bangkok  was canceled due to the  COVID-19 pandemic , along with those of 2021 and 2022, which were held online as a series of virtual, interactive presentations. The in-person conference returned in 2023 when it was held in Singapore, at which  UNESCO  joined as a partner organization. [51] 
 The Wikimedia Foundation maintains the hardware that runs its projects in its own servers. It also maintains the MediaWiki platform and many other software libraries that run its projects.
 Wikipedia employed a single server until 2004 when the server setup was expanded into a distributed  multitier architecture . [52]  Server downtime in 2003 led to the first fundraising drive.
 By December 2009, Wikimedia ran on  co-located  servers, with 300 servers in Florida and 44 in  Amsterdam . [53]  In 2008, it also switched from multiple different  Linux  operating system vendors to  Ubuntu Linux . [54] [55]  In 2019, it switched to  Debian . [56] 
 By January 2013, Wikimedia transitioned to newer infrastructure in an  Equinix  facility in  Ashburn, Virginia , citing reasons of "more reliable connectivity" and "fewer  hurricanes ". [57] [58]  In years prior, the hurricane seasons had been a cause of distress. [59] 
 In October 2013, Wikimedia Foundation started looking for a second facility that would be used side by side with the main facility in Ashburn, citing reasons of redundancy (e.g.  emergency fallback ) and to prepare for simultaneous multi-datacenter service. [60] [61]  This followed a year in which a  fiber  cut caused the Wikimedia projects to be unavailable for one hour in August 2012. [62] [63] 
 Apart from the second facility for redundancy coming online in 2014, [64] [65]  the number of servers needed to run the infrastructure in a single facility has been mostly stable since 2009. As of November 2015, the main facility in Ashburn hosts 520 servers in total which includes servers for newer services besides Wikimedia project  wikis , such as  cloud services  (Toolforge) [66] [67]  and various services for metrics, monitoring, and other system administration. [68] 
 In 2017, Wikimedia Foundation deployed a caching cluster in an Equinix facility in  Singapore , the first of its kind in Asia. [69] 
 The operation of Wikimedia depends on  MediaWiki , a custom-made,  free  and  open-source   wiki software  platform written in  PHP  and built upon the  MariaDB  database since 2013; [70]  previously the MySQL  database  was used. [71]  The software incorporates programming features such as a  macro language ,  variables , a  transclusion  system for  templates , and  URL redirection . MediaWiki is licensed under the  GNU General Public License  and it is used by all Wikimedia projects.
 Originally, Wikipedia ran on  UseModWiki  written in  Perl  by  Clifford Adams  (Phase I), which initially required  CamelCase  for article hyperlinks; the double bracket style was incorporated later. Starting in January 2002 (Phase II), Wikipedia began running on a  PHP wiki  engine with a MySQL database; this software was custom-made for Wikipedia by  Magnus Manske . The Phase II software was repeatedly modified to accommodate the  exponentially increasing  demand. In July 2002 (Phase III), Wikipedia shifted to the third-generation software, MediaWiki, originally written by  Lee Daniel Crocker .
 Some MediaWiki extensions are  installed  to extend the functionality of MediaWiki software. In April 2005, an  Apache Lucene  extension [72] [73]  was added to MediaWiki's built-in search and Wikipedia switched from MySQL to  Lucene  and later switched to CirrusSearch which is based on  Elasticsearch  for searching. [74]  The Wikimedia Foundation also uses  CiviCRM [75]  and  WordPress . [76] 
 The Foundation published official Wikipedia  mobile apps  for  Android  and  iOS  devices and in March 2015, the apps were updated to include mobile user-friendly features. [77] 
 The Wikimedia Foundation mainly finances itself through donations from the public, collected through email campaigns and annual fundraising banners placed on Wikipedia, as well as grants from various tech companies and philanthropic organizations. [9] [79]  Campaigns for the Wikimedia Endowment have included emails asking donors to leave Wikimedia money in their will. [80] 
 As a 501(c)(3) charity, the Foundation is exempt from federal and state income tax. [81] [82]  It is not a private foundation, and contributions to it qualify as tax-deductible charitable contributions. [79]  In 2007, 2008 and 2009,  Charity Navigator  gave Wikimedia an overall rating of four out of four possible stars, [83]  increased from three to four stars in 2010. [84]  As of January 2020 [update] , the rating was still four stars (overall score 98.14 out of 100), based on data from FY2018. [85] 
 The Foundation also increases its revenue through  federal grants , sponsorship, services and brand merchandising. The Wikimedia  OAI-PMH  update feed service, targeted primarily at search engines and similar bulk analysis and republishing, was a source of revenue for a number of years. [86] [87]   DBpedia  was given access to this feed free of charge. [88]  An expanded version of data feeds and content services was launched in 2021 as Wikimedia Enterprise, an LLC subsidiary of the Foundation. [89] 
 In July 2014, the Foundation announced it would accept  Bitcoin  donations. [90]  In 2021,  cryptocurrencies  accounted for just 0.08% of all donations [91] [92]  and on May 1, 2022, the Foundation stopped accepting cryptocurrency donations, following a  Wikimedia community  vote. [92] [93] 
 The Foundation's net assets grew from an initial $57,000 at the end of its first fiscal year, ending June 30, 2004, [94]  to $53.5 million in mid-2014 [95] [96]  and $231 million (plus a $100 million endowment) by the end of June 2021; that year, the Foundation also announced plans to launch Wikimedia Enterprise, to let large organizations pay by volume for high-volume access to otherwise rate-limited APIs. [97] 
 In 2020, the Foundation donated $4.5 million to  Tides Advocacy  to create a "Knowledge Equity Fund", to provide grants to organizations whose work would not otherwise be covered by Wikimedia grants but addresses racial inequities in accessing and contributing to free knowledge resources. [98] [99] 
 In January 2016, the Foundation announced the creation of an  endowment  to safeguard its future. [100]  The Wikimedia Endowment was established as a donor-advised fund at the  Tides Foundation , with a stated goal to raise $100 million in the next 10 years. [101]   Craig Newmark  was one of the initial donors, giving $1 million. [102]   Peter Baldwin  and his wife,  Lisbet Rausing , donated $5 million to it in 2017. [103] 
 In 2018, major donations to the endowment were received from  Amazon  and  Facebook  ($1 million each) and  George Soros  ($2 million). [104] [105] [106]  In 2019, donations included $2 million from Google, [107]  $3.5 million more from Baldwin and Rausing, [103]  $2.5 million more from Newmark, [108]  and another $1 million from Amazon in October 2019 and again in September 2020. [109] [110] 
 As of 2023, [update]  the advisory board consists of  Jimmy Wales ,  Peter Baldwin , former Wikimedia Foundation Trustees  Patricio Lorente  and  Phoebe Ayers , former Wikimedia Foundation Board Visitor  Doron Weber  of the  Sloan Foundation , investor  Annette Campbell-White , venture capitalist Michael Kim, portfolio manager Alexander M. Farman-Farmaian, and strategist Lisa Lewin. [103] 
 The Foundation itself has provided annual grants of $5 million to its Endowment since 2016. [111]  These amounts have been recorded as part of the Foundation's "awards and grants" expenses. [112]  In September 2021, the Foundation announced that the Wikimedia Endowment had reached its initial $100 million fundraising goal in June 2021, five years ahead of its initial target. [4]  In January 2024, the endowment was reported to have a value of $140 million. [113] 
 The Foundation summarizes its assets in the "Statements of Activities" in its audited reports. These do not include funds in the Wikimedia Endowment, however expenses from the 2015–16 financial year onward include payments to the Wikimedia Endowment. [114] 
 A  plurality  of Wikimedia Foundation expenses are salaries and wages, followed by community and affiliate grants, contributions to the endowment, and other professional operating expenses and services. [115] [78] 
 The Wikimedia Foundation has received a steady stream of grants from other foundations throughout its history.
In 2008, the Foundation received a $40,000 grant from the  Open Society Institute  to create a printable version of Wikipedia. [116]  It also received a $262,000 grant from the  Stanton Foundation  to purchase  hardware , [117]  a $500,000 unrestricted grant from  Vinod  and  Neeru Khosla , [118]  who later that year joined the Foundation advisory board, [119]  and $177,376 from the historians  Lisbet Rausing  and  Peter Baldwin  ( Arcadia Fund ), among others. [117]  In March 2008, the Foundation announced what was then its largest donation yet: a three-year, $3 million grant from the  Sloan Foundation . [120] 
 In 2009, the Foundation received four grants. The first was a $890,000 Stanton Foundation grant to help study and simplify the user interface for first-time authors of Wikipedia. [121]  The second was a $300,000  Ford Foundation  grant in July 2009 for  Wikimedia Commons , to improve the interface for uploading multimedia files. [122]  In August 2009, the Foundation received a $500,000 grant from The William and Flora  Hewlett Foundation . [123]  Also in August 2009, the  Omidyar Network  committed up to $2 million over two years to Wikimedia. [124] 
 In 2010,  Google  donated $2 million [125]  and the Stanton Foundation granted $1.2 million to fund the Public Policy Initiative, a pilot program for what later became the Wikipedia Education Program (and the spin-off  Wiki Education Foundation ). [126] [127] [128] 
 In March 2011, the Sloan Foundation authorized another $3 million grant, to be funded over three years, with the first $1 million to come in July 2011 and the remaining $2 million to be funded in August 2012 and 2013. As a donor,  Doron Weber  from the Sloan Foundation gained Board Visitor status at the Wikimedia Foundation Board of Trustees. [129]  In August 2011, the Stanton Foundation pledged to fund a $3.6 million grant of which $1.8 million was funded and the remainder was to come in September 2012. As of 2011, this was the largest grant the Wikimedia Foundation had ever received. [130]  In November 2011, the Foundation received a $500,000 donation from the  Brin Wojcicki Foundation . [131] [132] 
 In 2012, the Foundation was awarded a grant of $1.25 million from  Lisbet Rausing [131]  and  Peter Baldwin  through the  Charities Aid Foundation , scheduled to be funded in five equal installments from 2012 through 2015. In 2014, the Foundation received the largest single gift in its history, a $5 million unrestricted donation from an anonymous donor supporting $1 million worth of expenses annually for the next five years. [133]  In March 2012, The  Gordon and Betty Moore Foundation , established by the  Intel  co-founder and his wife, awarded the Wikimedia Foundation a $449,636 grant to develop  Wikidata . [134]  This was part of a larger grant, much of which went to Wikimedia Germany, which took on ownership of the development effort. [135] 
 Between 2014 and 2015, the Foundation received $500,000 from the Monarch Fund, $100,000 from the Arcadia Fund and an undisclosed amount from the  Stavros Niarchos Foundation  to support the  Wikipedia Zero  initiative. [136] [137] [138] 
 In 2015, a grant agreement was reached with the  John S. and James L. Knight Foundation  to build a search engine called the " Knowledge Engine ", a project that  proved controversial . [139] [140]  In 2017, the Sloan Foundation awarded another $3 million grant for a three-year period, [129]  and Google donated another $1.1 million to the Foundation in 2019. [141] 
 The following have donated $500,000 or more each (2008–2019, not including gifts to the Wikimedia Endowment; list may be incomplete):
 The Foundation's  board of trustees  supervises the activities of the Foundation. The founding board had three members, to which two community-elected trustees were added. Starting in 2008 it was composed of ten members:
 Over time, the size of the board and details of the selection processes have evolved. As of 2020, the board may have up to 16 trustees: [144] 
 In 2015,  James Heilman , a trustee recently elected to the board by the community, [145]  was removed from his position by a vote of the rest of the board. [146] [147]  This decision generated dispute among members of the Wikipedia community. [148] [149]  Heilman later said that he "was given the option of resigning [by the Board] over the last few weeks. As a community elected member I see my mandate as coming from the community which elected me and thus declined to do so. I saw such a move as letting down those who elected me." [150]  He subsequently added that while on the Board, he had pushed for greater transparency regarding the Wikimedia Foundation's  Knowledge Engine  project and its financing, [151]  and indicated that his attempts to make public the  Knight Foundation  grant for the engine had been a factor in his dismissal. [152]  Heilman was reelected to the board by the community in 2017. [153] 
 In January 2016,  Arnnon Geshuri  joined the board before stepping down amid community controversy about a " no poach " agreement he executed when at  Google , which violated  United States antitrust law  and for which the participating companies paid US$415 million in a class action suit on behalf of affected employees. [154] [155] 
 As of January 2024, the board comprised six community-and-affiliate-selected trustees (Shani Evenstein Sigalov,  Dariusz Jemielniak ,  Rosie Stephenson-Goodknight , Victoria Doronina, Mike Peel and Lorenzo Losa); [156]  five Board-appointed trustees ( McKinsey & Company  director  Raju Narisetti , [157]  Bahraini human rights activist and blogger  Esra'a Al Shafei , [158]  technology officer Luis Bitencourt-Emilio, Nataliia Tymkiv, and financial expert Kathy Collins); and Wales. [143]  Tymkiv chairs the board, with Al Shafei and Sigalov as vice chairs. [159] 
 As of March 2024 there are six committees of the Board of Trustees: the Executive Committee (Chair: Nataliia Tymkiv, as the chair of the Board), the Audit Committee (Chair: Kathy Collins, appointed in 2023), the Governance Committee (Chair: Dariusz Jemielniak, appointed in 2021), the Talent and Culture Committee (Chair: Rosie Stephenson-Goodknight, appointed in 2023), the Community Affairs Committee (Chair: Shani Evenstein Sigalov, appointed in 2021), and the Product and Technology Committee (Chair: Lorenzo Losa, appointed in 2023). [160] 
 In 2004, the Foundation appointed Tim Starling as developer liaison to help improve the  MediaWiki  software, Daniel Mayer as chief financial officer ( finance ,  budgeting , and coordination of fund drives), and  Erik Möller  as content partnership coordinator. In May 2005, the Foundation announced seven more official appointments. [161] 
 In January 2006, the Foundation created a number of committees, including the Communication Committee, in an attempt to further organize activities somewhat handled by volunteers at that time. [162]  Starling resigned that month to spend more time on his PhD program.
 As of October 4, 2006 [update] , the Foundation had five paid employees: [163]  two programmers, an administrative assistant, a coordinator handling fundraising and grants, and an interim  executive director , [164]  Brad Patrick, previously the Foundation's  general counsel . Patrick ceased his activity as interim director in January 2007 and then resigned from his position as legal counsel, effective April 1, 2007. He was replaced by  Mike Godwin  who served as general counsel and legal coordinator from July 2007 [165]  to 2010.
 In January 2007, Carolyn Doran was named chief operating officer and Sandy Ordonez joined as  head of communications . [166]  Doran began working as a part-time bookkeeper in 2006 after being sent by a  temporary agency . Doran, found to have had a criminal record, [167]  left the Foundation in July 2007 and  Sue Gardner  was hired as consultant and special advisor; she became the executive director in December 2007. [168]  Florence Devouard cited Doran's departure from the organization as one of the reasons the Foundation took about seven months to release its fiscal 2007 financial audit. [169] 
 Danny Wool, officially the grant coordinator and also involved in  fundraising  and business development, resigned in March 2007. He accused Wales of misusing the Foundation's funds for recreational purposes and said that Wales had his Wikimedia credit card taken away in part because of his spending habits, a claim Wales denied. [170]  In February 2007, the Foundation added a position, chapters coordinator, and hired Delphine Ménard, [171]  who had been occupying the position as a volunteer since August 2005. Cary Bass was hired in March 2007 in the position of volunteer coordinator. In January 2008, the Foundation appointed Veronique Kessler as the new chief financial and operating officer, Kul Wadhwa as head of business development and Jay Walsh as head of communications.
 In March 2013, Gardner announced she would be leaving her position at the Foundation. [172]   Lila Tretikov  was appointed executive director in May 2014; [173] [174]  she resigned in March 2016. Former chief communications officer  Katherine Maher  (joined Wikimedia in 2014 [113] ) was appointed the interim executive director, a position made permanent in June 2016. [175]  Maher served as  executive director  until April 2021 [176] [177]  and is credited with building the Foundation  endowment  in her tenure. [113] 
 As of October 23, 2023, [update]  there were over 700 people working at the Foundation. [178]   Maryana Iskander  was named the incoming CEO in September 2021, and took over that role in January 2022. [179] 
 As of July 2022, the WMF has the following department structure: [180] 
 A number of disputes have resulted in  litigation [181] [182] [183] [184]  while others have not. [185]  Attorney Matt Zimmerman has said, "Without strong liability protection, it would be difficult for Wikipedia to continue to provide a platform for user-created encyclopedia content." [186] 
 In December 2011, the Foundation hired Washington, D.C., lobbyist  Dow Lohnes  Government Strategies LLC to lobby  Congress . [187]  At the time of the hire, the Foundation was concerned about a bill known as the  Stop Online Piracy Act . [188]  The communities were as well, organizing some of the most visible  protest  against the bill on the Internet alongside other popular websites.
 In October 2013, a German court ruled that the Wikimedia Foundation can be held liable for content added to Wikipedia when there has been a specific complaint; otherwise, the Wikimedia Foundation does not check the content Wikipedia publishes and has no duty to do so. [189] 
 In June 2014, Bildkonst Upphovsrätt i Sverige filed a copyright infringement lawsuit against  Wikimedia Sweden . [190] 
 On June 20, 2014, a defamation lawsuit (Law Division civil case No. L-1400-14) involving Wikipedia editors was filed with the Mercer County Superior Court in New Jersey seeking, inter alia, compensatory and punitive damages. [191] [192] 
 In a March 10, 2015, op-ed for  The New York Times , Wales and Tretikov announced the Foundation was filing  a lawsuit  against the  National Security Agency  and five other government agencies and officials, including  DOJ , calling into question its practice of  mass surveillance , which they argued infringed the constitutional rights of the Foundation's readers, editors and staff. They were joined in the suit by eight additional plaintiffs, including  Amnesty International  and  Human Rights Watch . [193] [194] [195]  On October 23, 2015, the  United States District Court for the District of Maryland  dismissed the suit  Wikimedia Foundation v. NSA  on grounds of  standing . U.S. District Judge  T. S. Ellis III  ruled that the plaintiffs could not plausibly prove they were subject to  upstream surveillance , and that their argument is "riddled with assumptions", "speculations" and "mathematical gymnastics". [196] [197]  The plaintiffs filed an appeal with the  United States Court of Appeals for the Fourth Circuit  on February 17, 2016. [198] 
 In September 2020, WMF's application to become an observer at the  World Intellectual Property Organization  (WIPO) was blocked after objections from the government of China [199]  over the existence of a Wikimedia Foundation affiliate in  Taiwan . [200]  In October 2021, WMF's second application was blocked by the government of China for the same reason. [201]  In May 2022, six Wikimedia movement affiliate chapters were blocked from being accredited to WIPO's Standing Committee on Copyright and Related Rights (SCCR) by China, claiming that the chapters were spreading disinformation. [202]  In July 2022, China blocked an application by seven Wikimedia chapters to be accredited as permanent observers to WIPO; [203]  China's position was supported by a number of other countries, including Russia, Pakistan, Iran, Algeria, Zimbabwe and Venezuela. [204] 
 In 2014, Jimmy Wales was confronted with allegations that WMF had "a miserable cost/benefit ratio and for years now has spent millions on software development without producing anything that actually works". He acknowledged that he had "been frustrated as well about the endless controversies about the rollout of inadequate software not developed with sufficient community consultation and without proper incremental rollout to catch show-stopping bugs". [205] 
 During the 2015 fundraising campaign, some members of the community voiced their concerns about the fundraising banners. They argued that they were obtrusive and could deceive potential donors by giving the impression that Wikipedia had immediate financial problems, which was not true. The Wikimedia Foundation vowed to improve wording on further fundraising campaigns to avoid these issues. [206]  Despite this, the Foundation has continued to come under criticism for running campaigns seemingly designed to "make[] its readers feel guilty." Such campaigns have additionally been condemned for, in 2021, being run in countries that had been badly affected by the  COVID-19 pandemic , such as  Argentina  and  Brazil , [207]  as well as for sparking fears in  India  that Wikipedia might be "dying". [208]  This is despite the Foundation being in ownership of "vast money reserves", in 2021 reaching its 10-year goal of compiling a $100 million endowment fund in only 5 years. [207] 
 In February 2017, an op-ed published by  The Signpost , the  English Wikipedia 's online newspaper, titled "Wikipedia has Cancer", [209] [210]  produced a debate in both the Wikipedian community and the wider public. The author criticized the Wikimedia Foundation for its ever-increasing annual spending, which, he argued, could put the project at financial risk should an unexpected event happen. The author proposed to cap spending, build up the endowment, and restructure the endowment so that WMF cannot dip into the principal when times get bad. [211] 
 Knowledge Engine was a  search engine  project initiated in 2015 by WMF to locate and display verifiable and trustworthy information on the Internet. [212]  The KE's goal was to be less reliant on traditional search engines. It was funded with a $250,000 grant from the  Knight Foundation . [213]  Some perceived the project as a scandal, mainly because it was conceived in secrecy, and the project proposal was even a surprise to some staff, in contrast with a general culture of transparency in the organization and on the projects. Some of the information available to the community was received through leaked documents published by  The Signpost  in 2016. [214] [212]  Following this dispute, Wikimedia Foundation Executive Director  Lila Tretikov  resigned. [215] [216] [217] 
 In 2022, in a recent "personal appeal" displayed in an advertising banner on Wikipedia, Jimmy Wales, one of the founders, emphasized that "Wikipedia is not for sale." This statement highlights the non-profit nature of the Wikimedia Foundation (WMF), a non-profit organization based in California that owns intellectual property assets, such as the Wikipedia name and branding. However, the WMF does not own or control the global communities that maintain the site. [218] 
 In 2022, the WMF announced new recipients for its "knowledge equity grants". As of last June, the WMF reported $239 million in net assets. It is expected to raise $174 million in revenue in the 2023. [218]  Despite expenses on the foundation staff's salaries, there's a significant surplus left. To manage these funds, the WMF has created an endowment composed of investments and cash. This is managed not by the WMF but by the  Tides Foundation , a charitable organization that channels funds to  social justice  causes and campaigns. [218] 
 The endowment aims to grow this capital to $130.4 million in the next fiscal year. Some of these funds are allocated to the knowledge equity fund, which provides grants. [218] 
 However, there has been some controversy over the administration of the funds. While the Tides Foundation has promised to become a more transparent  501(c)(3)  organization to reveal how it manages funds, details on expenses and salaries are still lacking seven years later. [218] 
 Additionally, the WMF's salary costs have risen from $7 million in 2010/11 to $88 million in 2021/22, yet only 2% of the raised money goes towards hosting costs. [218] 
 .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap} 37°47′21″N   122°24′12″W ﻿ / ﻿ 37.78917°N 122.40333°W ﻿ /  37.78917; -122.40333 


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,405 total.  This list may not reflect recent changes .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: 
 The following 200 pages are in this category, out of approximately 2,334,643 total.  This list may not reflect recent changes .


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoToc does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 590,637 total.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,405 total.  This list may not reflect recent changes .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: 

Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoTOC does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 10,572 total.


Title: Log in

Content: 

Title: None

Content: Populated by pages where  Template:Large category TOC  has been used  by {{ CatAutoTOC }}  on a  category with 10,001–20,000 pages . 
 Purge page to update totals 
 This category has the following 200 subcategories, out of 987 total.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: Edit instructions 
 Armed conflicts and attacks 
 Disasters and accidents 
 International relations  
 Politics and elections  
 Armed conflicts and attacks 
 Disasters and accidents  
 Politics and elections  
 Armed conflicts and attacks 
 Disasters and accidents 
 International relations 
 Law and crime 
 Armed conflicts and attacks 
 Business and economy 
 International relations 
 Law and crime 
 Politics and elections 
 Armed conflicts and attacks 
 Disasters and accidents 
 International relations 
 Sports 
 Armed conflicts and attacks 
 Business and economy 
 Disasters and accidents 
 International relations 
 Law and crime 
 Politics and elections 
 Science and technology 
 Sports 
 
 Armed conflicts and attacks 
 Business and economy 
 Disasters and accidents 
 Politics and elections 
 Science and technology 
 Sports 


Title: None

Content: 
 Explore the vast knowledge of  Wikipedia  through these helpful resources. If you have a specific topic in mind, use Wikipedia's search box. If you don't know exactly what you are looking for or wish to explore broad areas, click on a link in the header menu at the top of this page, or begin your browsing below:
 Wikipedia organizes its content into distinct subject classifications, each with further subdivisions.
 Explore the diverse cultures, arts, beliefs, and customs of human societies.
 Discover the wonders of Earth's lands, features, inhabitants, and planetary phenomena.
 Learn about physical, mental, and social well-being.
 Dive into the past through written records and scholarly exploration.
 Explore chronological events through our comprehensive timelines.
 Stay up-to-date with encyclopedia entries covering ongoing events.
 Delve into diverse human actions, from leisure and entertainment to industry and warfare.
 Explore the study of quantity, structure, space, and change.
 Understand natural phenomena through empirical evidence, observations, and experiments.
 Learn about collective entities, ethnic groups, and nations.
 Dive deep into fundamental questions about existence, knowledge, values, and more.
 Access comprehensive information collections compiled for easy retrieval.
 Refer to various third-party classification systems linked to Wikipedia articles.
 Access sources on specific topics for further reading or verification.
 Explore social-cultural systems, beliefs, ethics, and more.
 Understand collectives, social interactions, political authority, and cultural norms.
 Learn about techniques, skills, methods, and processes in technology and science.
 Get summaries of broad topics with links to subtopics, biographies, and related articles.
 Explore topics in outline format, linking to more detailed articles.
 Find enumerations of specific types, such as lists of countries and people.
 Access featured articles, images, news, and more through thematic portals.
 Access lists of terms with definitions through alphabetical glossaries.
 Browse Wikipedia's category pages, which index articles by subject.
 Explore subjects that demand high-quality articles, grouped by importance.
 Discover Wikipedia's best, reviewed and vetted for quality.
 Explore well-written, factually accurate articles that meet editorial standards.
 Listen to Wikipedia articles as spoken word recordings.
 Browse Wikipedia's articles alphabetically.
 Topics 
 Types 
 Places, people and times 
 Indices 


Title: None

Content: Kathleen Ferrier  (22 April 1912 – 8 October 1953) was an English  contralto  who achieved an international reputation as a stage, concert and recording artist. During the Second World War she performed regularly with the  Council for the Encouragement of Music and the Arts . In 1946 she made her stage debut as Lucretia in the world premiere of  Benjamin Britten 's  The Rape of Lucretia , and a year later she appeared as Orfeo in  Christoph Willibald Gluck 's  Orfeo ed Euridice . As a recitalist, Ferrier's repertoire included works by  Bach ,  Brahms ,  Mahler  and  Elgar . Forming working relationships with the conductors  John Barbirolli  and  Bruno Walter  and the accompanist  Gerald Moore , she became known internationally through her three tours of the United States and her many visits to continental Europe.  She continued to perform and record  after being diagnosed with breast cancer in 1951. Among her many memorials, the  Kathleen Ferrier Memorial Scholarship Fund  makes annual awards to aspiring young singers. ( Full article... )
 April 22 
 Seventeen people have held the office of President and Chancellor of New York University  (NYU), a private  research university  in  New York City . The president of  New York University  is its  chief executive officer  and is elected by the university's board of trustees, of which the president is a member  ex officio . From NYU's foundation by  Albert Gallatin  in 1831 until 1956, the head of NYU was the  chancellor . That year, the office became known as "president and chancellor", or "president" for short. The president recommends persons to fill senior offices, including the  provost , executive vice president,  general counsel , and  deans , who are then appointed by the board. The president also presides over the  university senate  and confers all degrees, with the board's authorization and upon certification of a student by the faculty. The incumbent president,  Linda G. Mills   (pictured) , assumed office on July 1, 2023 and became NYU's first female president. ( Full list... )
 Pelophylax cypriensis , commonly known as the Cyprus water frog, is a species of frog in the family Ranidae, the  true frogs . It is  endemic  to Cyprus. It is a medium-sized frog, with females (body length up to 75 mm, 3.0 in) being larger than males (up to 65 mm, 2.6 in). The skin is rather warty and colouration varies widely. There are four unwebbed toes on the front legs and five webbed toes on the hindlegs. Males have paired external  vocal sacs . This Cyprus water frog was photographed under the Elia Bridge in  Limassol District , Cyprus.
 Photograph credit:  Charles J. Sharp Wikipedia is written by volunteer editors and hosted by the  Wikimedia Foundation , a non-profit organization that also hosts a range of other volunteer  projects :
 This Wikipedia is written in  English . Many  other Wikipedias are available ; some of the largest are listed below.


Title: None

Content: Natural-language understanding  ( NLU ) or  natural-language interpretation  ( NLI ) [1]  is a subset of  natural-language processing  in  artificial intelligence  that deals with machine  reading comprehension . Natural-language understanding is considered an  AI-hard  problem. [2] 
 There is considerable commercial interest in the field because of its application to  automated reasoning , [3]   machine translation , [4]   question answering , [5]  news-gathering,  text categorization ,  voice-activation , archiving, and large-scale  content analysis .
 The program  STUDENT , written in 1964 by  Daniel Bobrow  for his PhD dissertation at  MIT , is one of the earliest known attempts at natural-language understanding by a computer. [6] [7] [8] [9] [10]  Eight years after  John McCarthy  coined the term  artificial intelligence , Bobrow's dissertation (titled  Natural Language Input for a Computer Problem Solving System ) showed how a computer could understand simple natural language input to solve algebra word problems.
 A year later, in 1965,  Joseph Weizenbaum  at MIT wrote  ELIZA , an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a  database  of real-world knowledge or a rich  lexicon . Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by  Ask.com . [11] 
 In 1969,  Roger Schank  at  Stanford University  introduced the  conceptual dependency theory  for natural-language understanding. [12]  This model, partially influenced by the work of  Sydney Lamb , was extensively used by Schank's students at  Yale University , such as  Robert Wilensky ,  Wendy Lehnert , and  Janet Kolodner .
 In 1970,  William A. Woods  introduced the  augmented transition network  (ATN) to represent natural language input. [13]  Instead of  phrase structure rules  ATNs used an equivalent set of  finite state automata  that were called recursively. ATNs and their more general format called "generalized ATNs" continued to be used for a number of years.
 In 1971,  Terry Winograd  finished writing  SHRDLU  for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field. [14] [15]  Winograd continued to be a major influence in the field with the publication of his book  Language as a Cognitive Process . [16]  At Stanford, Winograd would later advise  Larry Page , who co-founded  Google .
 In the 1970s and 1980s, the natural language processing group at  SRI International  continued research and development in the field. A number of commercial efforts based on the research were undertaken,  e.g. , in 1982  Gary Hendrix  formed  Symantec Corporation  originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse-driven  graphical user interfaces , Symantec changed direction. A number of other commercial efforts were started around the same time,  e.g. , Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems Corp. [17] [18]  In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnert. [19] 
 The third millennium saw the introduction of systems using machine learning for text classification, such as the IBM  Watson . However, experts debate how much "understanding" such systems demonstrate:  e.g. , according to  John Searle , Watson did not even understand the questions. [20] 
 John Ball , cognitive scientist and inventor of the  Patom Theory , supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and e-commerce, but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language that still defies conventional natural language processing. [ citation needed ]  According to Wibe Wagemans, "To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence – just like a 3-year-old does without guesswork." [21] 
 The umbrella term "natural-language understanding" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to  robots , to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real-world applications fall between the two extremes, for instance  text classification  for the automatic analysis of emails and their routing to a suitable department in a corporation does not require an in-depth understanding of the text, [22]  but needs to deal with a much larger vocabulary and more diverse syntax than the management of simple queries to database tables with fixed schemata.
 Throughout the years various attempts at processing natural language or  English-like  sentences presented to computers have taken place at varying degrees of complexity. Some attempts have not resulted in systems with deep understanding, but have helped overall system usability. For example,  Wayne Ratliff  originally developed the  Vulcan  program with an English-like syntax to mimic the English speaking computer in  Star Trek . Vulcan later became the  dBase  system whose easy-to-use syntax effectively launched the personal computer database industry. [23] [24]  Systems with an easy to use or English-like syntax are, however, quite distinct from systems that use a rich  lexicon  and include an internal  representation  (often as  first order logic ) of the semantics of natural language sentences.
 Hence the breadth and depth of "understanding" aimed at by a system determine both the complexity of the system (and the implied challenges) and the types of applications it can deal with. The "breadth" of a system is measured by the sizes of its vocabulary and grammar. The "depth" is measured by the degree to which its understanding approximates that of a fluent native speaker. At the narrowest and shallowest,  English-like  command interpreters require minimal complexity, but have a small range of applications. Narrow but deep systems explore and model mechanisms of understanding, [25]  but they still have limited application. Systems that attempt to understand the contents of a document such as a news release beyond simple keyword matching and to judge its suitability for a user are broader and require significant complexity, [26]  but they are still somewhat shallow. Systems that are both very broad and very deep are beyond the current state of the art.
 Regardless of the approach used, most natural-language-understanding systems share some common components. The system needs a  lexicon  of the language and a  parser  and  grammar  rules to break sentences into an internal representation. The construction of a rich lexicon with a suitable  ontology  requires significant effort,  e.g. , the  Wordnet  lexicon required many person-years of effort. [27] 
 The system also needs theory from  semantics  to guide the comprehension. The interpretation capabilities of a language-understanding system depend on the semantic theory it uses. Competing semantic theories of language have specific trade-offs in their suitability as the basis of computer-automated semantic interpretation. [28]  These range from  naive semantics  or  stochastic semantic analysis  to the use of  pragmatics  to derive meaning from context. [29] [30] [31]   Semantic parsers  convert natural-language texts into formal meaning representations. [32] 
 Advanced applications of natural-language understanding also attempt to incorporate logical  inference  within their framework. This is generally achieved by mapping the derived meaning into a set of assertions in  predicate logic , then using  logical deduction  to arrive at conclusions. Therefore, systems based on functional languages such as  Lisp  need to include a subsystem to represent logical assertions, while logic-oriented systems such as those using the language  Prolog  generally rely on an extension of the built-in logical representation framework. [33] [34] 
 The management of  context  in natural-language understanding can present special challenges. A large variety of examples and counter examples have resulted in multiple approaches to the  formal modeling  of context, each with specific strengths and weaknesses. [35] [36] 


Title: None

Content: 
 Speech recognition  is an  interdisciplinary  subfield of  computer science  and  computational linguistics  that develops  methodologies  and technologies that enable the recognition and  translation  of spoken language into text by computers. It is also known as  automatic speech recognition  ( ASR ),  computer speech recognition  or  speech to text  ( STT ). It incorporates knowledge and research in the  computer science ,  linguistics  and  computer engineering  fields. The reverse process is  speech synthesis .
 Some speech recognition systems require "training" (also called "enrollment") where an individual speaker reads text or isolated  vocabulary  into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker-independent" [1]  systems. Systems that use training are called "speaker dependent".
 Speech recognition applications include  voice user interfaces  such as voice dialing (e.g. "call home"), call routing (e.g. "I would like to make a collect call"),  domotic  appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, [2]  speech-to-text processing (e.g.,  word processors  or  emails ), and  aircraft  (usually termed  direct voice input ). Automatic  pronunciation assessment  is used in education such as for spoken language learning.
 The term  voice recognition [3] [4] [5]  or  speaker identification [6] [7] [8]  refers to identifying the speaker, rather than what they are saying.  Recognizing the speaker  can simplify the task of  translating speech  in systems that have been trained on a specific person's voice or it can be used to  authenticate  or verify the identity of a speaker as part of a security process.
 From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in  deep learning  and  big data . The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.
 The key areas of growth were: vocabulary size, speaker independence, and processing speed.
 Raj Reddy  was the first person to take on continuous speech recognition as a graduate student at  Stanford University  in the late 1960s. Previous systems required users to pause after each word. Reddy's system issued spoken commands for playing  chess .
 Around this time Soviet researchers invented the  dynamic time warping  (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. [15]  DTW processed speech by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique carried on. Achieving speaker independence remained unsolved at this time period.
 During the late 1960s  Leonard Baum  developed the mathematics of  Markov chains  at the  Institute for Defense Analysis . A decade later, at CMU, Raj Reddy's students  James Baker  and  Janet M. Baker  began using the  hidden Markov model  (HMM) for speech recognition. [20]  James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. [21]  The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.
 The 1980s also saw the introduction of the  n-gram  language model.
 Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the  PDP-10  with 4 MB ram. [28]  It could take up to 100 minutes to decode just 30 seconds of speech. [29] 
 Two practical products were:
 By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. [28]  Raj Reddy's former student,  Xuedong Huang , developed the  Sphinx-II  system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the  speech recognition group at Microsoft  in 1993. Raj Reddy's student  Kai-Fu Lee  joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.
 Lernout & Hauspie , a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the  Windows XP  operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became  Nuance  in 2005.  Apple  originally licensed software from Nuance to provide speech recognition capability to its digital assistant  Siri . [34] 
 In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and  Global Autonomous Language Exploitation  (GALE). Four teams participated in the EARS program:  IBM , a team led by  BBN  with  LIMSI  and  Univ. of Pittsburgh ,  Cambridge University , and a team composed of  ICSI ,  SRI  and  University of Washington . EARS funded the collection of the Switchboard telephone  speech corpus  containing 260 hours of recorded conversations from over 500 speakers. [35]  The GALE program focused on  Arabic  and  Mandarin  broadcast news speech.  Google 's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. [36]  The first product was  GOOG-411 , a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems.  Google Voice Search  is now supported in over 30 languages.
 In the United States, the  National Security Agency  has made use of a type of speech recognition for  keyword spotting  since at least 2006. [37]  This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and  IARPA 's  Babel program .
 In the early 2000s, speech recognition was still dominated by traditional approaches such as  hidden Markov models  combined with feedforward  artificial neural networks . [38] 
Today, however, many aspects of speech recognition have been taken over by a  deep learning  method called  Long short-term memory  (LSTM), a  recurrent neural network  published by  Sepp Hochreiter  &  Jürgen Schmidhuber  in 1997. [39]  LSTM RNNs avoid the  vanishing gradient problem  and can learn "Very Deep Learning" tasks [40]  that require memories of events that happened thousands of discrete time steps ago, which is important for speech.
Around 2007, LSTM trained by Connectionist Temporal Classification (CTC) [41]  started to outperform traditional speech recognition in certain applications. [42]  In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through  Google Voice  to all smartphone users. [43]   Transformers , a type of neural network based on solely on attention, have been widely adopted in computer vision [44] [45]  and language modeling, [46] [47]  sparking the interest of adapting such models to new domains, including speech recognition. [48] [49] [50]  Some recent papers reported superior performance levels using transformer models for speech recognition, but these models usually require large scale training datasets to reach high performance levels.
 The use of deep feedforward (non-recurrent) networks for  acoustic modeling  was introduced during the later part of 2009 by  Geoffrey Hinton  and his students at the University of Toronto and by Li Deng [51]  and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and the University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). [52] [53] [54]  A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979". [55]  In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. [55]  This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.
 In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s. [56] [57] [58] 
But these methods never won over the non-uniform internal-handcrafting  Gaussian mixture model / hidden Markov model  (GMM-HMM) technology based on generative models of speech trained discriminatively. [59]  A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing [60]  and weak temporal correlation structure in the neural predictive models. [61] [62]  All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition. [53] [54] [63] [64] 
 By early 2010s  speech  recognition, also called voice recognition [65] [66] [67]  was clearly differentiated from  speaker  recognition, and speaker independence was considered a major breakthrough. Until then, systems required a "training" period.  A 1987 ad for a doll had carried the tagline "Finally, the doll that understands you." – despite the fact that it was described as "which children could train to respond to their voice". [12] 
 In 2017, Microsoft researchers reached a historical human parity milestone of transcribing conversational telephony speech on the widely benchmarked Switchboard task. Multiple deep learning models were used to optimize speech recognition accuracy. The speech recognition word error rate was reported to be as low as 4 professional human transcribers working together on the same benchmark, which was funded by IBM Watson speech team on the same task. [68] 
 Both  acoustic modeling  and  language modeling  are important parts of modern statistically based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as  document classification  or  statistical machine translation .
 Modern general-purpose speech recognition systems are based on hidden Markov models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time scale (e.g., 10 milliseconds), speech can be approximated as a  stationary process . Speech can be thought of as a  Markov model  for many stochastic purposes.
 Another reason why HMMs are popular is that they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of  n -dimensional real-valued vectors (with  n  being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of  cepstral  coefficients, which are obtained by taking a  Fourier transform  of a short time window of speech and decorrelating the spectrum using a  cosine transform , then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each  phoneme , will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.
 Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need  context dependency  for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use  cepstral normalization  to normalize for a different speaker and recording conditions; for further speaker normalization, it might use vocal tract length normalization (VTLN) for male-female normalization and  maximum likelihood linear regression  (MLLR) for more general speaker adaptation. The features would have so-called  delta  and  delta-delta coefficients  to capture speech dynamics and in addition, might use  heteroscedastic linear discriminant analysis  (HLDA); or might skip the delta and delta-delta coefficients and use  splicing  and an  LDA -based projection followed perhaps by  heteroscedastic  linear discriminant analysis or a  global semi-tied co variance  transform (also known as  maximum likelihood linear transform , or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum  mutual information  (MMI), minimum classification error (MCE), and minimum phone error (MPE).
 Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the  Viterbi algorithm  to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information and combining it statically beforehand (the  finite state transducer , or FST, approach).
 A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function ( re scoring ) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the  N-best list  approach) or as a subset of the models (a  lattice ). Re scoring is usually done by trying to minimize the  Bayes risk [69]  (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the  Levenshtein distance , though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score  lattices  represented as weighted  finite state transducers  with  edit distances  represented themselves as a  finite state transducer  verifying certain assumptions. [70] 
 Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.
 Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.
 A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are "warped" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.
 Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, [71]  phoneme classification through multi-objective evolutionary algorithms, [72]  isolated word recognition, [73]   audiovisual speech recognition , audiovisual speaker recognition and speaker adaptation.
 Neural networks  make fewer explicit assumptions about feature statistical properties than HMMs and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, [74]  early neural networks were rarely successful for continuous recognition tasks because of their limited ability to model temporal dependencies.
 One approach to this limitation was to use neural networks as a pre-processing, feature transformation or dimensionality reduction, [75]  step prior to HMM based recognition. However, more recently, LSTM and related recurrent neural networks (RNNs), [39] [43] [76] [77]  Time Delay Neural Networks(TDNN's), [78]  and transformers [48] [49] [50]  have demonstrated improved performance in this area.
 Deep neural networks and denoising  autoencoders [79]  are also under investigation. A deep feedforward neural network (DNN) is an  artificial neural network  with multiple hidden layers of units between the input and output layers. [53]  Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data. [80] 
 A success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted. [81] [82] 
 [83]  See comprehensive reviews of this development and of the state of the art as of October 2014 in the recent Springer book from Microsoft Research. [84]  See also the related background of automatic speech recognition and the impact of various machine learning paradigms, notably including  deep learning , in
recent overview articles. [85] [86] 
 One fundamental principle of  deep learning  is to do away with hand-crafted  feature engineering  and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, [87]  showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results. [88] 
 Since 2014, there has been much research interest in "end-to-end" ASR. Traditional phonetic-based (i.e., all  HMM -based model) approaches required separate components and training for the pronunciation, acoustic, and  language model . End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a  n-gram language model  is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices. [89]  Consequently, modern commercial ASR systems from  Google  and  Apple  (as of 2017 [update] ) are deployed on the cloud and require a network connection as opposed to the device locally.
 The first attempt at end-to-end ASR was with  Connectionist Temporal Classification  (CTC)-based systems introduced by  Alex Graves  of  Google DeepMind  and Navdeep Jaitly of the  University of Toronto  in 2014. [90]  The model consisted of  recurrent neural networks  and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to  conditional independence  assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later,  Baidu  expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English. [91]  In 2016,  University of Oxford  presented  LipNet , [92]  the first end-to-end sentence-level lipreading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset. [93]  A large-scale CNN-RNN-CTC architecture was presented in 2018 by  Google DeepMind  achieving 6 times better performance than human experts. [94] 
 An alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of  Carnegie Mellon University  and  Google Brain  and Bahdanau et al. of the  University of Montreal  in 2016. [95] [96]  The model named "Listen, Attend and Spell" (LAS), literally "listens" to the acoustic signal, pays "attention" to different parts of the signal and "spells" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model). [97]  Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by  Carnegie Mellon University ,  MIT  and  Google Brain  to directly emit sub-word units which are more natural than English characters; [98]   University of Oxford  and  Google DeepMind  extended LAS to "Watch, Listen, Attend and Spell" (WLAS) to handle lip reading surpassing human-level performance. [99] 
 Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signaled to the driver by an audio prompt. Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.  [ citation needed ] 
 Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent [ when? ]  car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words. [ citation needed ] 
 Automatic  pronunciation  assessment is the use of speech recognition to verify the correctness of pronounced speech, [100]  as distinguished from manual assessment by an instructor or proctor. [101]  Also called speech verification, pronunciation evaluation, and pronunciation scoring, the main application of this technology is computer-aided pronunciation teaching (CAPT) when combined with  computer-aided instruction  for  computer-assisted language learning  (CALL), speech  remediation , or  accent reduction . Pronunciation assessment does not determine unknown speech (as in  dictation  or  automatic transcription ) but instead, knowing the expected word(s) in advance, it attempts to verify the correctness of the learner's pronunciation and ideally their  intelligibility  to listeners, [102] [103]  sometimes along with often inconsequential  prosody  such as  intonation ,  pitch ,  tempo ,  rhythm , and  stress . [104]  Pronunciation assessment is also used in  reading tutoring , for example in products such as  Microsoft Teams [105]  and from Amira Learning. [106]  Automatic pronunciation assessment can also be used to help diagnose and treat  speech disorders  such as  apraxia . [107] 
 Assessing authentic listener intelligibility is essential for avoiding inaccuracies from  accent  bias, especially in high-stakes assessments; [108] [109] [110]  from words with multiple correct pronunciations; [111]  and from phoneme coding errors in machine-readable pronunciation dictionaries. [112]  In 2022, researchers found that some newer speech to text systems, based on  end-to-end reinforcement learning  to map audio signals directly into words, produce word and phrase confidence scores very closely correlated with genuine listener intelligibility. [113]  In the  Common European Framework of Reference for Languages  (CEFR) assessment criteria for "overall phonological control", intelligibility outweighs formally correct pronunciation at all levels. [114] 
 In the  health care  sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a  digital dictation  system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.
 One of the major issues relating to the use of speech recognition in healthcare is that the  American Recovery and Reinvestment Act of 2009  ( ARRA ) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an  Electronic Health Record  or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a  controlled vocabulary ) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.
 A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice "macros", where the use of certain phrases – e.g., "normal report", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.
 Prolonged use of speech recognition software in conjunction with  word processors  has shown benefits to short-term-memory restrengthening in  brain AVM  patients who have been treated with  resection . Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques. [ citation needed ] 
 Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in  fighter aircraft . Of particular note have been the US program in speech recognition for the  Advanced Fighter Technology Integration (AFTI) / F-16  aircraft ( F-16 VISTA ), the program in France for  Mirage  aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.
 Working with Swedish pilots flying in the  JAS-39  Gripen cockpit, Englund (2004) found recognition deteriorated with increasing  g-loads . The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially. [115] 
 The  Eurofighter Typhoon , currently in service with the UK  RAF , employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot  workload , [116]  and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands. [117] 
 Speaker-independent systems are also being developed and are under test for the  F35 Lightning II  (JSF) and the  Alenia Aermacchi M-346 Master  lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%. [118] 
 The problems of achieving high recognition accuracy under stress and noise are particularly relevant in the  helicopter  environment as well as in the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a  facemask , which would reduce acoustic noise in the  microphone . Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the  U.S. Army  Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment ( RAE ) in the UK. Work in France has included speech recognition in the  Puma helicopter . There has also been much useful work in  Canada . Results have been encouraging, and voice applications have included: control of communication radios, setting of  navigation  systems, and control of an automated target handover system.
 As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall  speech technology  in order to consistently achieve performance improvements in operational settings.
 Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation. Speech recognition and  synthesis  techniques offer the potential to eliminate the need for a person to act as a pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.
 The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors. [ citation needed ] 
 ASR is now commonplace in the field of  telephony  and is becoming more widespread in the field of  computer gaming  and simulation. In telephony systems, ASR is now being predominantly used in contact centers by integrating it with  IVR  systems. Despite the high level of integration with word processing in general personal computing, in the field of document production, ASR has not seen the expected increases in use.
 The improvement of mobile processor speeds has made speech recognition practical in  smartphones . Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands.
 People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services. [119] 
 Students who are blind (see  Blindness and education ) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard. [120] 
 Students who are physically disabled have a  Repetitive strain injury /other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard. [120] 
 Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. [121]  Also, see  Learning disability .
 The use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term memory capacity, in stroke and craniotomy individuals.
 Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed  RSI  became an urgent early market for speech recognition. [122] [123]  Speech recognition is used in  deaf   telephony , such as voicemail to text,  relay services , and  captioned telephone . Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. [124]  Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability. [125] 
 This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it from being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word. [126] 
 The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. [131] [132]  Accuracy is usually rated with  word error rate  (WER), whereas speed is measured with the  real time factor . Other measures of accuracy include  Single Word Error Rate  (SWER) and  Command Success Rate  (CSR).
 Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following: [133] [ citation needed ] 
 As mentioned earlier in this article, the accuracy of speech recognition may vary depending on the following factors:
 With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech.  
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.
 Constraints are often represented by grammar. 
 Speech recognition is a multi-leveled pattern recognition task.
 e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at a lower level;
 For telephone speech the sampling rate is 8000 samples per second; 
 computed every 10 ms, with one 10 ms section called a frame;
 Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has two descriptions:  amplitude  (how strong is it), and  frequency  (how often it vibrates per second).
Accuracy can be computed with the help of word error rate (WER). Word error rate can be calculated by aligning the recognized word and referenced word using dynamic string alignment. The problem may occur while computing the word error rate due to the difference between the sequence lengths of the recognized word and referenced word.
 The formula to compute the word error rate (WER) is:
 
   
     
       
         W 
         E 
         R 
         = 
         
           
             
               ( 
               s 
               + 
               d 
               + 
               i 
               ) 
             
             n 
           
         
       
     
     {\displaystyle WER={(s+d+i) \over n}} 
   
 
 where  s  is the number of substitutions,  d  is the number of deletions,  i  is the number of insertions, and  n  is the number of word references.
 While computing, the word recognition rate (WRR) is used. The formula is:
 where  h  is the number of correctly recognized words:
 Speech recognition can become a means of attack, theft, or accidental operation. For example, activation words like "Alexa" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action. [135]  Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.
 Two attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing. [136]  The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system. [137] 
 Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe,  ICASSP ,  Interspeech /Eurospeech, and the IEEE ASRU. Conferences in the field of  natural language processing , such as  ACL ,  NAACL , EMNLP, and HLT, are beginning to include papers on  speech processing . Important journals include the  IEEE  Transactions on Speech and Audio Processing (later renamed  IEEE  Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed  IEEE /ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.
 Books like "Fundamentals of Speech Recognition" by  Lawrence Rabiner  can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be "Statistical Methods for Speech Recognition" by  Frederick Jelinek  and "Spoken Language Processing (2001)" by  Xuedong Huang  etc., "Computer Speech", by  Manfred R. Schroeder , second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. The updated textbook  Speech and Language Processing  (2008) by  Jurafsky  and Martin presents the basics and the state of the art for ASR.  Speaker recognition  also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A comprehensive textbook, "Fundamentals of Speaker Recognition" is an in depth source for up to date details on the theory and practice. [138]  A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by  DARPA  (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).
 A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. Building Computers That Understand Speech" by  Roberto Pieraccini  (2012).
 The most recent book on speech recognition is  Automatic Speech Recognition: A Deep Learning Approach  (Publisher: Springer) written by Microsoft researchers D. Yu and L. Deng and published near the end of 2014, with highly mathematically oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. [84]  A related book, published earlier in 2014, "Deep Learning: Methods and Applications" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning. [80] 
 In terms of freely available resources,  Carnegie Mellon University 's  Sphinx  toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the  HTK  book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques,  Kaldi  toolkit can be used. [139]  In 2017  Mozilla  launched the open source project called  Common Voice [140]  to gather big database of voices that would help build free speech recognition project DeepSpeech (available free at  GitHub ), [141]  using Google's open source platform  TensorFlow . [142]  When Mozilla redirected funding away from the project in 2020, it was forked by its original developers as Coqui STT [143]  using the same open-source license. [144] [145] 
 Google  Gboard  supports speech recognition on all  Android  applications. It can be activated through the  microphone   icon . [146] 
 The commercial cloud based speech recognition APIs are broadly available.
 For more software resources, see  List of speech recognition software .


Title: None

Content: Theoretical linguistics  is a term in linguistics that, [1]  like the related term  general linguistics , [2]  can be understood in different ways. Both can be taken as a reference to the  theory of language , or the branch of  linguistics  that inquires into the  nature of language  and seeks to answer fundamental questions as to what language is, or what the common ground of all languages is. [2]  The goal of theoretical linguistics can also be the construction of a general theoretical framework for the description of language. [1] 
 Another use of the term depends on the organisation of linguistics into different sub-fields. The term theoretical linguistics is commonly juxtaposed with  applied linguistics . [3]  This perspective implies that the aspiring language professional, e.g. a student, must first learn the  theory  i.e. properties of the linguistic system, or what  Ferdinand de Saussure  called  internal linguistics . [4]  This is followed by  practice,  or studies in the applied field. The dichotomy is not fully unproblematic because  language pedagogy ,  language technology  and other aspects of applied linguistics also include theory. [3] 
 Similarly, the term general linguistics is used to distinguish core  linguistics  from other types of study. However, because college and university linguistics is largely distributed with the institutes and departments of a relatively small number of  national languages , some larger universities also offer courses and research programmes in 'general linguistics' which may cover exotic and  minority languages ,  cross-linguistic studies  and various other topics outside the scope of the main  philological  departments. [5] 
 When the concept of theoretical linguistics is taken to refer to  core  or  internal linguistics , it means the study of the parts of the language system. This traditionally means  phonology ,  morphology ,  syntax  and  semantics .  Pragmatics  and  discourse  can also be included; delimitation varies between institutions. Furthermore, Saussure's definition of general linguistics consists of the dichotomy of  synchronic and diachronic linguistics , thus including  historical linguistics  as a core issue. [4] 
 There are various frameworks of linguistic theory which include a general theory of language and a general theory of  linguistic description . [6]  Current humanistic approaches include theories within  structural linguistics  and  functional linguistics . In addition to the humanistic approaches of structural linguistics and functional linguistics, the field of theoretical linguistics encompasses other frameworks and perspectives.  Evolutionary linguistics  is one such framework that investigates the origins and development of language from an evolutionary and cognitive perspective. It incorporates various models within  generative grammar , which seeks to explain language structure through formal rules and transformations.  Cognitive linguistics  and  cognitive approaches to grammar , on the other hand, focuses on the relationship between language and cognition, exploring how language reflects and influences our thought processes. [6] 


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: html.skin-theme-clientpref-night .mw-parser-output .infobox.biota tr{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .infobox.biota img{background:white}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .infobox.biota tr{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .infobox.biota img{background:white}} 
 Walshia calcarata  is a  moth  in the family  Cosmopterigidae . It was described by  Lord Walsingham  in 1909. It is found in  Mexico . [1] 
 
 This article relating to  Chrysopeleiinae  is a  stub . You can help Wikipedia by  expanding it .

Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: 
 To add a page to this category, use  {{ Pp }}  or one of the more specific  full-protection templates . This should only be done if the page is indeed protected; adding the template does not in itself protect the page.
 For a dynamic and sortable list of all protected pages, see  Special:ProtectedPages . For reports of protected pages, see  Wikipedia:Database reports#Protections .
 For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 This category has the following 12 subcategories, out of 12 total.
 The following 64 pages are in this category, out of  64 total.  This list may not reflect recent changes .


Title: None

Content: This category contains  project pages  which have been  protected  from  page moves . They can still be moved by  administrators . Fully protected pages – those listed in  Category:Wikipedia protected pages  and its subcategories – are also move-protected; they are protected from both editing  and  moves.
 Use {{ pp-move }} or {{ pp-move-indef }} to add pages to this category. This should only be done if the page is in fact protected – adding the template does not in itself protect the page. 
 The following 200 pages are in this category, out of approximately 583 total.  This list may not reflect recent changes .


Title: None

Content: 

 The  Wikimedia Foundation, Inc. , abbreviated  WMF , is an American  501(c)(3)   nonprofit organization  headquartered in  San Francisco ,  California , and registered there as  a charitable foundation . [5]  It is best known as the host of  Wikipedia , the seventh  most visited website  in the world. However, the foundation also hosts 14 other related content projects. It also supports the development of  MediaWiki , the  wiki  software that underpins them all. [6] [7] [8] 
 The Wikimedia Foundation was established, in 2003 in  St. Petersburg, Florida , by  Jimmy Wales  as a nonprofit way to fund  Wikipedia ,  Wiktionary , and other  crowdsourced  wiki projects. [1]  (Until then, they had been hosted by  Bomis , Wales's for-profit company.) [1]  The Foundation finances itself mainly through millions of small donations from Wikipedia readers, collected through email campaigns and annual fundraising banners placed on Wikipedia and its sister projects. [9]  These are complemented by grants from philanthropic organizations and tech companies, and starting in 2022, by services income from  Wikimedia Enterprise .
 The Foundation has grown rapidly throughout its existence. As of December 31, 2023, it has employed over 700 staff and contractors, with annual revenues of $180.2 million, annual expenses of $169 million, net assets of $255 million and a growing endowment, which surpassed $100 million in June 2021.
 Officially, the Wikimedia Foundation's mission is "to empower and engage people around the world to collect and develop educational content under a free license or in the public domain, and to disseminate it effectively and globally." [10] 
 To serve this mission, the Wikimedia Foundation provides the technical and organizational infrastructure to enable members of the public to develop wiki-based content in languages across the world. [10]  The foundation does not write or curate any of the content on the wikis itself. [11]  Instead, this is done by editors who work as volunteers, such as the  Wikipedians  who create and maintain Wikipedia. However, the foundation does collaborate with a network of individual volunteers and affiliated organizations, such as Wikimedia chapters, thematic organizations, user groups and other partners.
 The Wikimedia Foundation promises in its mission statement to make useful information from its projects available on the internet free of charge in perpetuity. [10]  It also engages in  political advocacy . [12]  The Foundation's strategic direction, formulated in 2017, envisages that it "will become the essential infrastructure of the ecosystem of free knowledge" by 2030. [13] 
 Jimmy Wales  and  Larry Sanger  founded Wikipedia in 2001 as a feeder project to supplement  Nupedia . The project was originally funded by  Bomis , Wales's for-profit business, and edited by a rapidly growing community of volunteer editors. The early community discussed a variety of ways to support the ongoing costs of upkeep, and was broadly opposed to running ads on the site, [14]  so the idea of setting up a charitable foundation gained prominence. [15]  That also addressed an open question of what entity should hold onto trademarks for the project.
 The name "Wikimedia", a  compound  of  wiki  and  media , was coined by American author  Sheldon Rampton  in a post to the English Wikipedia mailing list in March 2003, [16]  three months after  Wiktionary  became the second wiki-based project hosted on the original server. The Wikimedia Foundation itself was incorporated in  St. Petersburg, Florida  on June 20, 2003. [1] [17] [18]  A small fundraising campaign to keep the servers running was run in October 2003. [19]  In 2005, the Foundation was granted section  501(c)(3)  status by the U.S.  Internal Revenue Code  as a public charity, making donations to the Foundation  tax-deductible  for U.S. federal income tax purposes. [20]  Its  National Taxonomy of Exempt Entities  (NTEE) code is B60 ( Adult ,  Continuing education ). [21] [22] 
 The Foundation filed an application to trademark the name  Wikipedia   in the US  on September 14, 2004. The mark was granted registration status on January 10, 2006. Trademark protection was accorded also by Japan on December 16, 2004, and by the  European Union  on January 20, 2005. Subsets of Wikipedia were already being distributed in book and DVD form, and there were discussions about licensing the logo and wordmark. [23] 
 On December 11, 2006, the Foundation's board noted that it could not become a  membership organization , as initially planned but not implemented, due to an inability to meet the registration requirements of Florida statutory law. The bylaws were accordingly amended to remove all references to membership rights and activities. [24] 
 In 2007, the Foundation decided to move its headquarters from Florida to the  San Francisco Bay Area . Considerations cited for choosing San Francisco were proximity to like-minded organizations and potential partners, a better talent pool, as well as cheaper and more convenient international travel. [25] [26] [27]  The move was completed by January 31, 2008, into a headquarters on Stillman Street in San Francisco. [28]  It later moved to New Montgomery Street, and then to  One Montgomery Tower . [29] 
 On October 25, 2021, the Foundation launched  Wikimedia Enterprise , a commercial Wikimedia content delivery service aimed at groups that want to use high-volume APIs, starting with  Big Tech  enterprises. [30] [31]  In June 2022,  Google  and the  Internet Archive  were announced as the service's first customers, though only Google will pay for the service. [32]  The same announcement noted a shifting focus towards smaller companies with similar data needs, supporting the service through "a lot paying a little".
 Wikimedia Enterprise  is a commercial product by the Wikimedia Foundation to provide, in a more easily consumable way, the data of the Wikimedia projects, including  Wikipedia . [33]  It allows customers to retrieve data at large scale and high availability through different formats like  Web APIs , data snapshots or  streams .
 It was first announced in March 2021 [34] [35]  and launched on October 26, 2021. [36] [37] 
 Google  and the  Internet Archive  were its first customers, although Internet Archive is not paying for the product. [36]  A  New York Times Magazine  article was reporting that Wikimedia Enterprise made $3.1 million in total revenue in 2022. [33] 
 Content on most Wikimedia project  websites  is licensed for redistribution under  v4.0  of the  Attribution  and  Share-alike   Creative Commons licenses . The Foundation owns and operates 11 wikis that are written, curated, designed, and governed by their communities of volunteer editors. Any member of the public is welcome to contribute; registering a named user account is optional. These wikis follow a  free content  model, with the stated goal of disseminating knowledge to the world. They include, by launch date:
 Certain additional projects provide infrastructure or coordination of the free knowledge projects. These include:
 Wikimedia affiliates are independent and formally recognized groups of people working together to support and contribute to the Wikimedia movement. The Wikimedia Foundation officially recognizes three types of affiliates: chapters, thematic organizations, and user groups. Affiliates organize and engage in activities to support and contribute to the Wikimedia movement, such as regional conferences, outreach,  edit-a-thons ,  hackathons ,  public relations ,  public policy  advocacy,  GLAM  engagement, and  Wikimania . [38] [39] [40]  While many of these things are also done by individual contributors or less formal groups, they are not referred to as affiliates.
 Wikimedia chapters and thematic organizations are  incorporated  non-profit organizations. They are recognized by the Foundation as affiliates officially when its board does so. The board's decisions are based on recommendations of an  Affiliations Committee  (AffCom), composed of Wikimedia community members, which reports regularly to the board. The Affiliations Committee directly approves the recognition of unincorporated user groups. Affiliates are formally recognized by the Wikimedia Foundation, but are independent of it, with no legal control of or responsibility for Wikimedia projects and their content. [39] [40] [41] 
 The Foundation began recognizing chapters in 2004. [42]  In 2012, the Foundation approved, finalized and adopted the thematic organization and user group recognition models. An additional model for movement partners, was also approved, but as of May 19, 2022 [update]  has not yet been finalized or adopted. [40] [43] 
 Wikimania is an annual global conference for Wikimedians and Wikipedians, started in 2005. The first Wikimania was held in  Frankfurt , Germany, in 2005. Wikimania is organized by a committee supported usually by the local national chapter, with support from local institutions (such as a library or university) and usually from the Wikimedia Foundation. Wikimania has been held in cities such as  Buenos Aires , [44]   Cambridge , [45]   Haifa , [46]   Hong Kong , [47]   Taipei ,  London , [48]   Mexico City , [49]   Esino Lario ,  Italy , [50]   Montreal ,  Cape Town , and  Stockholm . The 2020 conference scheduled to take place in  Bangkok  was canceled due to the  COVID-19 pandemic , along with those of 2021 and 2022, which were held online as a series of virtual, interactive presentations. The in-person conference returned in 2023 when it was held in Singapore, at which  UNESCO  joined as a partner organization. [51] 
 The Wikimedia Foundation maintains the hardware that runs its projects in its own servers. It also maintains the MediaWiki platform and many other software libraries that run its projects.
 Wikipedia employed a single server until 2004 when the server setup was expanded into a distributed  multitier architecture . [52]  Server downtime in 2003 led to the first fundraising drive.
 By December 2009, Wikimedia ran on  co-located  servers, with 300 servers in Florida and 44 in  Amsterdam . [53]  In 2008, it also switched from multiple different  Linux  operating system vendors to  Ubuntu Linux . [54] [55]  In 2019, it switched to  Debian . [56] 
 By January 2013, Wikimedia transitioned to newer infrastructure in an  Equinix  facility in  Ashburn, Virginia , citing reasons of "more reliable connectivity" and "fewer  hurricanes ". [57] [58]  In years prior, the hurricane seasons had been a cause of distress. [59] 
 In October 2013, Wikimedia Foundation started looking for a second facility that would be used side by side with the main facility in Ashburn, citing reasons of redundancy (e.g.  emergency fallback ) and to prepare for simultaneous multi-datacenter service. [60] [61]  This followed a year in which a  fiber  cut caused the Wikimedia projects to be unavailable for one hour in August 2012. [62] [63] 
 Apart from the second facility for redundancy coming online in 2014, [64] [65]  the number of servers needed to run the infrastructure in a single facility has been mostly stable since 2009. As of November 2015, the main facility in Ashburn hosts 520 servers in total which includes servers for newer services besides Wikimedia project  wikis , such as  cloud services  (Toolforge) [66] [67]  and various services for metrics, monitoring, and other system administration. [68] 
 In 2017, Wikimedia Foundation deployed a caching cluster in an Equinix facility in  Singapore , the first of its kind in Asia. [69] 
 The operation of Wikimedia depends on  MediaWiki , a custom-made,  free  and  open-source   wiki software  platform written in  PHP  and built upon the  MariaDB  database since 2013; [70]  previously the MySQL  database  was used. [71]  The software incorporates programming features such as a  macro language ,  variables , a  transclusion  system for  templates , and  URL redirection . MediaWiki is licensed under the  GNU General Public License  and it is used by all Wikimedia projects.
 Originally, Wikipedia ran on  UseModWiki  written in  Perl  by  Clifford Adams  (Phase I), which initially required  CamelCase  for article hyperlinks; the double bracket style was incorporated later. Starting in January 2002 (Phase II), Wikipedia began running on a  PHP wiki  engine with a MySQL database; this software was custom-made for Wikipedia by  Magnus Manske . The Phase II software was repeatedly modified to accommodate the  exponentially increasing  demand. In July 2002 (Phase III), Wikipedia shifted to the third-generation software, MediaWiki, originally written by  Lee Daniel Crocker .
 Some MediaWiki extensions are  installed  to extend the functionality of MediaWiki software. In April 2005, an  Apache Lucene  extension [72] [73]  was added to MediaWiki's built-in search and Wikipedia switched from MySQL to  Lucene  and later switched to CirrusSearch which is based on  Elasticsearch  for searching. [74]  The Wikimedia Foundation also uses  CiviCRM [75]  and  WordPress . [76] 
 The Foundation published official Wikipedia  mobile apps  for  Android  and  iOS  devices and in March 2015, the apps were updated to include mobile user-friendly features. [77] 
 The Wikimedia Foundation mainly finances itself through donations from the public, collected through email campaigns and annual fundraising banners placed on Wikipedia, as well as grants from various tech companies and philanthropic organizations. [9] [79]  Campaigns for the Wikimedia Endowment have included emails asking donors to leave Wikimedia money in their will. [80] 
 As a 501(c)(3) charity, the Foundation is exempt from federal and state income tax. [81] [82]  It is not a private foundation, and contributions to it qualify as tax-deductible charitable contributions. [79]  In 2007, 2008 and 2009,  Charity Navigator  gave Wikimedia an overall rating of four out of four possible stars, [83]  increased from three to four stars in 2010. [84]  As of January 2020 [update] , the rating was still four stars (overall score 98.14 out of 100), based on data from FY2018. [85] 
 The Foundation also increases its revenue through  federal grants , sponsorship, services and brand merchandising. The Wikimedia  OAI-PMH  update feed service, targeted primarily at search engines and similar bulk analysis and republishing, was a source of revenue for a number of years. [86] [87]   DBpedia  was given access to this feed free of charge. [88]  An expanded version of data feeds and content services was launched in 2021 as Wikimedia Enterprise, an LLC subsidiary of the Foundation. [89] 
 In July 2014, the Foundation announced it would accept  Bitcoin  donations. [90]  In 2021,  cryptocurrencies  accounted for just 0.08% of all donations [91] [92]  and on May 1, 2022, the Foundation stopped accepting cryptocurrency donations, following a  Wikimedia community  vote. [92] [93] 
 The Foundation's net assets grew from an initial $57,000 at the end of its first fiscal year, ending June 30, 2004, [94]  to $53.5 million in mid-2014 [95] [96]  and $231 million (plus a $100 million endowment) by the end of June 2021; that year, the Foundation also announced plans to launch Wikimedia Enterprise, to let large organizations pay by volume for high-volume access to otherwise rate-limited APIs. [97] 
 In 2020, the Foundation donated $4.5 million to  Tides Advocacy  to create a "Knowledge Equity Fund", to provide grants to organizations whose work would not otherwise be covered by Wikimedia grants but addresses racial inequities in accessing and contributing to free knowledge resources. [98] [99] 
 In January 2016, the Foundation announced the creation of an  endowment  to safeguard its future. [100]  The Wikimedia Endowment was established as a donor-advised fund at the  Tides Foundation , with a stated goal to raise $100 million in the next 10 years. [101]   Craig Newmark  was one of the initial donors, giving $1 million. [102]   Peter Baldwin  and his wife,  Lisbet Rausing , donated $5 million to it in 2017. [103] 
 In 2018, major donations to the endowment were received from  Amazon  and  Facebook  ($1 million each) and  George Soros  ($2 million). [104] [105] [106]  In 2019, donations included $2 million from Google, [107]  $3.5 million more from Baldwin and Rausing, [103]  $2.5 million more from Newmark, [108]  and another $1 million from Amazon in October 2019 and again in September 2020. [109] [110] 
 As of 2023, [update]  the advisory board consists of  Jimmy Wales ,  Peter Baldwin , former Wikimedia Foundation Trustees  Patricio Lorente  and  Phoebe Ayers , former Wikimedia Foundation Board Visitor  Doron Weber  of the  Sloan Foundation , investor  Annette Campbell-White , venture capitalist Michael Kim, portfolio manager Alexander M. Farman-Farmaian, and strategist Lisa Lewin. [103] 
 The Foundation itself has provided annual grants of $5 million to its Endowment since 2016. [111]  These amounts have been recorded as part of the Foundation's "awards and grants" expenses. [112]  In September 2021, the Foundation announced that the Wikimedia Endowment had reached its initial $100 million fundraising goal in June 2021, five years ahead of its initial target. [4]  In January 2024, the endowment was reported to have a value of $140 million. [113] 
 The Foundation summarizes its assets in the "Statements of Activities" in its audited reports. These do not include funds in the Wikimedia Endowment, however expenses from the 2015–16 financial year onward include payments to the Wikimedia Endowment. [114] 
 A  plurality  of Wikimedia Foundation expenses are salaries and wages, followed by community and affiliate grants, contributions to the endowment, and other professional operating expenses and services. [115] [78] 
 The Wikimedia Foundation has received a steady stream of grants from other foundations throughout its history.
In 2008, the Foundation received a $40,000 grant from the  Open Society Institute  to create a printable version of Wikipedia. [116]  It also received a $262,000 grant from the  Stanton Foundation  to purchase  hardware , [117]  a $500,000 unrestricted grant from  Vinod  and  Neeru Khosla , [118]  who later that year joined the Foundation advisory board, [119]  and $177,376 from the historians  Lisbet Rausing  and  Peter Baldwin  ( Arcadia Fund ), among others. [117]  In March 2008, the Foundation announced what was then its largest donation yet: a three-year, $3 million grant from the  Sloan Foundation . [120] 
 In 2009, the Foundation received four grants. The first was a $890,000 Stanton Foundation grant to help study and simplify the user interface for first-time authors of Wikipedia. [121]  The second was a $300,000  Ford Foundation  grant in July 2009 for  Wikimedia Commons , to improve the interface for uploading multimedia files. [122]  In August 2009, the Foundation received a $500,000 grant from The William and Flora  Hewlett Foundation . [123]  Also in August 2009, the  Omidyar Network  committed up to $2 million over two years to Wikimedia. [124] 
 In 2010,  Google  donated $2 million [125]  and the Stanton Foundation granted $1.2 million to fund the Public Policy Initiative, a pilot program for what later became the Wikipedia Education Program (and the spin-off  Wiki Education Foundation ). [126] [127] [128] 
 In March 2011, the Sloan Foundation authorized another $3 million grant, to be funded over three years, with the first $1 million to come in July 2011 and the remaining $2 million to be funded in August 2012 and 2013. As a donor,  Doron Weber  from the Sloan Foundation gained Board Visitor status at the Wikimedia Foundation Board of Trustees. [129]  In August 2011, the Stanton Foundation pledged to fund a $3.6 million grant of which $1.8 million was funded and the remainder was to come in September 2012. As of 2011, this was the largest grant the Wikimedia Foundation had ever received. [130]  In November 2011, the Foundation received a $500,000 donation from the  Brin Wojcicki Foundation . [131] [132] 
 In 2012, the Foundation was awarded a grant of $1.25 million from  Lisbet Rausing [131]  and  Peter Baldwin  through the  Charities Aid Foundation , scheduled to be funded in five equal installments from 2012 through 2015. In 2014, the Foundation received the largest single gift in its history, a $5 million unrestricted donation from an anonymous donor supporting $1 million worth of expenses annually for the next five years. [133]  In March 2012, The  Gordon and Betty Moore Foundation , established by the  Intel  co-founder and his wife, awarded the Wikimedia Foundation a $449,636 grant to develop  Wikidata . [134]  This was part of a larger grant, much of which went to Wikimedia Germany, which took on ownership of the development effort. [135] 
 Between 2014 and 2015, the Foundation received $500,000 from the Monarch Fund, $100,000 from the Arcadia Fund and an undisclosed amount from the  Stavros Niarchos Foundation  to support the  Wikipedia Zero  initiative. [136] [137] [138] 
 In 2015, a grant agreement was reached with the  John S. and James L. Knight Foundation  to build a search engine called the " Knowledge Engine ", a project that  proved controversial . [139] [140]  In 2017, the Sloan Foundation awarded another $3 million grant for a three-year period, [129]  and Google donated another $1.1 million to the Foundation in 2019. [141] 
 The following have donated $500,000 or more each (2008–2019, not including gifts to the Wikimedia Endowment; list may be incomplete):
 The Foundation's  board of trustees  supervises the activities of the Foundation. The founding board had three members, to which two community-elected trustees were added. Starting in 2008 it was composed of ten members:
 Over time, the size of the board and details of the selection processes have evolved. As of 2020, the board may have up to 16 trustees: [144] 
 In 2015,  James Heilman , a trustee recently elected to the board by the community, [145]  was removed from his position by a vote of the rest of the board. [146] [147]  This decision generated dispute among members of the Wikipedia community. [148] [149]  Heilman later said that he "was given the option of resigning [by the Board] over the last few weeks. As a community elected member I see my mandate as coming from the community which elected me and thus declined to do so. I saw such a move as letting down those who elected me." [150]  He subsequently added that while on the Board, he had pushed for greater transparency regarding the Wikimedia Foundation's  Knowledge Engine  project and its financing, [151]  and indicated that his attempts to make public the  Knight Foundation  grant for the engine had been a factor in his dismissal. [152]  Heilman was reelected to the board by the community in 2017. [153] 
 In January 2016,  Arnnon Geshuri  joined the board before stepping down amid community controversy about a " no poach " agreement he executed when at  Google , which violated  United States antitrust law  and for which the participating companies paid US$415 million in a class action suit on behalf of affected employees. [154] [155] 
 As of January 2024, the board comprised six community-and-affiliate-selected trustees (Shani Evenstein Sigalov,  Dariusz Jemielniak ,  Rosie Stephenson-Goodknight , Victoria Doronina, Mike Peel and Lorenzo Losa); [156]  five Board-appointed trustees ( McKinsey & Company  director  Raju Narisetti , [157]  Bahraini human rights activist and blogger  Esra'a Al Shafei , [158]  technology officer Luis Bitencourt-Emilio, Nataliia Tymkiv, and financial expert Kathy Collins); and Wales. [143]  Tymkiv chairs the board, with Al Shafei and Sigalov as vice chairs. [159] 
 As of March 2024 there are six committees of the Board of Trustees: the Executive Committee (Chair: Nataliia Tymkiv, as the chair of the Board), the Audit Committee (Chair: Kathy Collins, appointed in 2023), the Governance Committee (Chair: Dariusz Jemielniak, appointed in 2021), the Talent and Culture Committee (Chair: Rosie Stephenson-Goodknight, appointed in 2023), the Community Affairs Committee (Chair: Shani Evenstein Sigalov, appointed in 2021), and the Product and Technology Committee (Chair: Lorenzo Losa, appointed in 2023). [160] 
 In 2004, the Foundation appointed Tim Starling as developer liaison to help improve the  MediaWiki  software, Daniel Mayer as chief financial officer ( finance ,  budgeting , and coordination of fund drives), and  Erik Möller  as content partnership coordinator. In May 2005, the Foundation announced seven more official appointments. [161] 
 In January 2006, the Foundation created a number of committees, including the Communication Committee, in an attempt to further organize activities somewhat handled by volunteers at that time. [162]  Starling resigned that month to spend more time on his PhD program.
 As of October 4, 2006 [update] , the Foundation had five paid employees: [163]  two programmers, an administrative assistant, a coordinator handling fundraising and grants, and an interim  executive director , [164]  Brad Patrick, previously the Foundation's  general counsel . Patrick ceased his activity as interim director in January 2007 and then resigned from his position as legal counsel, effective April 1, 2007. He was replaced by  Mike Godwin  who served as general counsel and legal coordinator from July 2007 [165]  to 2010.
 In January 2007, Carolyn Doran was named chief operating officer and Sandy Ordonez joined as  head of communications . [166]  Doran began working as a part-time bookkeeper in 2006 after being sent by a  temporary agency . Doran, found to have had a criminal record, [167]  left the Foundation in July 2007 and  Sue Gardner  was hired as consultant and special advisor; she became the executive director in December 2007. [168]  Florence Devouard cited Doran's departure from the organization as one of the reasons the Foundation took about seven months to release its fiscal 2007 financial audit. [169] 
 Danny Wool, officially the grant coordinator and also involved in  fundraising  and business development, resigned in March 2007. He accused Wales of misusing the Foundation's funds for recreational purposes and said that Wales had his Wikimedia credit card taken away in part because of his spending habits, a claim Wales denied. [170]  In February 2007, the Foundation added a position, chapters coordinator, and hired Delphine Ménard, [171]  who had been occupying the position as a volunteer since August 2005. Cary Bass was hired in March 2007 in the position of volunteer coordinator. In January 2008, the Foundation appointed Veronique Kessler as the new chief financial and operating officer, Kul Wadhwa as head of business development and Jay Walsh as head of communications.
 In March 2013, Gardner announced she would be leaving her position at the Foundation. [172]   Lila Tretikov  was appointed executive director in May 2014; [173] [174]  she resigned in March 2016. Former chief communications officer  Katherine Maher  (joined Wikimedia in 2014 [113] ) was appointed the interim executive director, a position made permanent in June 2016. [175]  Maher served as  executive director  until April 2021 [176] [177]  and is credited with building the Foundation  endowment  in her tenure. [113] 
 As of October 23, 2023, [update]  there were over 700 people working at the Foundation. [178]   Maryana Iskander  was named the incoming CEO in September 2021, and took over that role in January 2022. [179] 
 As of July 2022, the WMF has the following department structure: [180] 
 A number of disputes have resulted in  litigation [181] [182] [183] [184]  while others have not. [185]  Attorney Matt Zimmerman has said, "Without strong liability protection, it would be difficult for Wikipedia to continue to provide a platform for user-created encyclopedia content." [186] 
 In December 2011, the Foundation hired Washington, D.C., lobbyist  Dow Lohnes  Government Strategies LLC to lobby  Congress . [187]  At the time of the hire, the Foundation was concerned about a bill known as the  Stop Online Piracy Act . [188]  The communities were as well, organizing some of the most visible  protest  against the bill on the Internet alongside other popular websites.
 In October 2013, a German court ruled that the Wikimedia Foundation can be held liable for content added to Wikipedia when there has been a specific complaint; otherwise, the Wikimedia Foundation does not check the content Wikipedia publishes and has no duty to do so. [189] 
 In June 2014, Bildkonst Upphovsrätt i Sverige filed a copyright infringement lawsuit against  Wikimedia Sweden . [190] 
 On June 20, 2014, a defamation lawsuit (Law Division civil case No. L-1400-14) involving Wikipedia editors was filed with the Mercer County Superior Court in New Jersey seeking, inter alia, compensatory and punitive damages. [191] [192] 
 In a March 10, 2015, op-ed for  The New York Times , Wales and Tretikov announced the Foundation was filing  a lawsuit  against the  National Security Agency  and five other government agencies and officials, including  DOJ , calling into question its practice of  mass surveillance , which they argued infringed the constitutional rights of the Foundation's readers, editors and staff. They were joined in the suit by eight additional plaintiffs, including  Amnesty International  and  Human Rights Watch . [193] [194] [195]  On October 23, 2015, the  United States District Court for the District of Maryland  dismissed the suit  Wikimedia Foundation v. NSA  on grounds of  standing . U.S. District Judge  T. S. Ellis III  ruled that the plaintiffs could not plausibly prove they were subject to  upstream surveillance , and that their argument is "riddled with assumptions", "speculations" and "mathematical gymnastics". [196] [197]  The plaintiffs filed an appeal with the  United States Court of Appeals for the Fourth Circuit  on February 17, 2016. [198] 
 In September 2020, WMF's application to become an observer at the  World Intellectual Property Organization  (WIPO) was blocked after objections from the government of China [199]  over the existence of a Wikimedia Foundation affiliate in  Taiwan . [200]  In October 2021, WMF's second application was blocked by the government of China for the same reason. [201]  In May 2022, six Wikimedia movement affiliate chapters were blocked from being accredited to WIPO's Standing Committee on Copyright and Related Rights (SCCR) by China, claiming that the chapters were spreading disinformation. [202]  In July 2022, China blocked an application by seven Wikimedia chapters to be accredited as permanent observers to WIPO; [203]  China's position was supported by a number of other countries, including Russia, Pakistan, Iran, Algeria, Zimbabwe and Venezuela. [204] 
 In 2014, Jimmy Wales was confronted with allegations that WMF had "a miserable cost/benefit ratio and for years now has spent millions on software development without producing anything that actually works". He acknowledged that he had "been frustrated as well about the endless controversies about the rollout of inadequate software not developed with sufficient community consultation and without proper incremental rollout to catch show-stopping bugs". [205] 
 During the 2015 fundraising campaign, some members of the community voiced their concerns about the fundraising banners. They argued that they were obtrusive and could deceive potential donors by giving the impression that Wikipedia had immediate financial problems, which was not true. The Wikimedia Foundation vowed to improve wording on further fundraising campaigns to avoid these issues. [206]  Despite this, the Foundation has continued to come under criticism for running campaigns seemingly designed to "make[] its readers feel guilty." Such campaigns have additionally been condemned for, in 2021, being run in countries that had been badly affected by the  COVID-19 pandemic , such as  Argentina  and  Brazil , [207]  as well as for sparking fears in  India  that Wikipedia might be "dying". [208]  This is despite the Foundation being in ownership of "vast money reserves", in 2021 reaching its 10-year goal of compiling a $100 million endowment fund in only 5 years. [207] 
 In February 2017, an op-ed published by  The Signpost , the  English Wikipedia 's online newspaper, titled "Wikipedia has Cancer", [209] [210]  produced a debate in both the Wikipedian community and the wider public. The author criticized the Wikimedia Foundation for its ever-increasing annual spending, which, he argued, could put the project at financial risk should an unexpected event happen. The author proposed to cap spending, build up the endowment, and restructure the endowment so that WMF cannot dip into the principal when times get bad. [211] 
 Knowledge Engine was a  search engine  project initiated in 2015 by WMF to locate and display verifiable and trustworthy information on the Internet. [212]  The KE's goal was to be less reliant on traditional search engines. It was funded with a $250,000 grant from the  Knight Foundation . [213]  Some perceived the project as a scandal, mainly because it was conceived in secrecy, and the project proposal was even a surprise to some staff, in contrast with a general culture of transparency in the organization and on the projects. Some of the information available to the community was received through leaked documents published by  The Signpost  in 2016. [214] [212]  Following this dispute, Wikimedia Foundation Executive Director  Lila Tretikov  resigned. [215] [216] [217] 
 In 2022, in a recent "personal appeal" displayed in an advertising banner on Wikipedia, Jimmy Wales, one of the founders, emphasized that "Wikipedia is not for sale." This statement highlights the non-profit nature of the Wikimedia Foundation (WMF), a non-profit organization based in California that owns intellectual property assets, such as the Wikipedia name and branding. However, the WMF does not own or control the global communities that maintain the site. [218] 
 In 2022, the WMF announced new recipients for its "knowledge equity grants". As of last June, the WMF reported $239 million in net assets. It is expected to raise $174 million in revenue in the 2023. [218]  Despite expenses on the foundation staff's salaries, there's a significant surplus left. To manage these funds, the WMF has created an endowment composed of investments and cash. This is managed not by the WMF but by the  Tides Foundation , a charitable organization that channels funds to  social justice  causes and campaigns. [218] 
 The endowment aims to grow this capital to $130.4 million in the next fiscal year. Some of these funds are allocated to the knowledge equity fund, which provides grants. [218] 
 However, there has been some controversy over the administration of the funds. While the Tides Foundation has promised to become a more transparent  501(c)(3)  organization to reveal how it manages funds, details on expenses and salaries are still lacking seven years later. [218] 
 Additionally, the WMF's salary costs have risen from $7 million in 2010/11 to $88 million in 2021/22, yet only 2% of the raised money goes towards hosting costs. [218] 
 .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap} 37°47′21″N   122°24′12″W ﻿ / ﻿ 37.78917°N 122.40333°W ﻿ /  37.78917; -122.40333 


Title: None

Content: This category combines all articles with unsourced statements from February 2024  (2024-02)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=February 2024}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 9,313 total.  This list may not reflect recent changes .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 513,907 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 4 subcategories, out of 4 total.
 The following 36 pages are in this category, out of  36 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: 
 The following 200 pages are in this category, out of approximately 2,334,643 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoToc does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 590,637 total.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,407 total.  This list may not reflect recent changes .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: 

Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoTOC does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 10,572 total.


Title: Log in

Content: 

Title: None

Content: Populated by pages where  Template:Large category TOC  has been used  by {{ CatAutoTOC }}  on a  category with 10,001–20,000 pages . 
 Purge page to update totals 
 This category has the following 200 subcategories, out of 987 total.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: Create account

Content: edits articles recent contributors

Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,407 total.  This list may not reflect recent changes .


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,407 total.  This list may not reflect recent changes .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Search

Content: 

Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: 

Title: None

Content: 

Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoTOC does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 10,572 total.


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoTOC does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 10,572 total.


Title: Log in

Content: 

Title: Log in

Content: 

Title: None

Content: Populated by pages where  Template:Large category TOC  has been used  by {{ CatAutoTOC }}  on a  category with 10,001–20,000 pages . 
 Purge page to update totals 
 This category has the following 200 subcategories, out of 987 total.


Title: None

Content: Populated by pages where  Template:Large category TOC  has been used  by {{ CatAutoTOC }}  on a  category with 10,001–20,000 pages . 
 Purge page to update totals 
 This category has the following 200 subcategories, out of 987 total.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: Search

Content: 

Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,407 total.  This list may not reflect recent changes .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: None

Content: 
 
 Being unable to create an account for yourself is typically due to: 
 
 To prevent the automated creation of Wikipedia accounts using  bots  or  scripts , Wikipedia uses an image verification method (called a  CAPTCHA ) to assure that new accounts are being created by a real person. To further combat account creation abuse, Wikipedia also prevents the creation of new accounts with usernames that are too similar to other existing Wikipedia accounts. 
 If you have attempted to  create an account  but are having trouble with the  CAPTCHA  image verification step (perhaps because you are using a  screen reader  or a browser that does not support images), or if you have chosen a username that is too similar to an existing username, you can  request an account  be created for you. 
 Usernames are not allowed on Wikipedia if they:
 Usernames are not allowed on Wikipedia and will be  immediately blocked  upon discovery if they:
 To request an account using the 'Request an Account' process, please read and follow  all  of the instructions below: 
 We hope to process an account request the same day of its submission. However, it is not uncommon for requests to take 2-3 days depending on the work load and volume of requests currently pending review. In rare cases (such as server downtime or the sudden submission of an extremely high amount of legitimate account requests), requests may take longer. Please be patient; your request will be processed as soon as possible.
 Before submitting a new request or inquiring about the progress of your current request: 
 
 Request an account 


Title: Search

Content: 

Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoTOC does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 10,572 total.


Title: Log in

Content: 

Title: None

Content: Populated by pages where  Template:Large category TOC  has been used  by {{ CatAutoTOC }}  on a  category with 10,001–20,000 pages . 
 Purge page to update totals 
 This category has the following 200 subcategories, out of 987 total.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
  This article was the subject of a Wiki Education Foundation-supported course assignment, between  26 August 2019  and  11 December 2019 . Further details are available  on the course page . Student editor(s):  Wendell guan .
 Above undated message substituted from  Template:Dashboard.wikiedu.org assignment  by  PrimeBOT  ( talk ) 05:00, 17 January 2022 (UTC) [ reply ] 
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON   While they're related, they're not really the same thing.  Computational linguistics tries to use computer techniques to better understand linguistics as a discipline, while NLP tries to build ways for a computer to understand language.  Obviously many things overlap, but they have much different focus: NLP doesn't explicitly care if it's making new contributions to linguistics, and computational linguistics doesn't explicitly care if it's making it easier for computers to understand natural languages. -- Delirium  22:58, Feb 22, 2004 (UTC)
 Unclear  My take on this (I'm a grad student studying NLP/CL) is that CL and NLP are the endpoints on a continuum, and so a lot of work in the middle is hard to classify as one or the other.  They don't have separate conferences - the Association for Computational Linguistics (annual) and Computational Linguistics (biannual) are the main conferences for both NLP and CL research.   24.59.194.44  13:26, 23 June 2006 (UTC) [ reply ] 
 CON   There's a fine distinction between NLP and Computational Linguistics that has to do primarily with the distinction between computing and linguistics. Historically, NLP is associated with computing and CL with linguistics. I would be opposed to the merge for that reason. Investigations into the nature of language are misplaced in applied computing and practical aspects of parsing for say commercial applications are misplaced in Linguistics.  74.78.162.229  ( talk ) 21:30, 10 July 2008 (UTC) [ reply ] 
 PRO   CL and NLP should be be merged.  There are other fields:  (I call) "Natural Language Understanding" or "Machine Reading" that have more ambitious goals:  get a computer to "understand" some natural language.  NLP and CL have made more progress, but are application driven --the technology behind them is often just perl scrips making statistics from NL corpora.  In any case, certainly NLP should merge with NLU  or  CL, but definitely not both. ----Dustin
 PRO   I think that this article should probably be merged with  Computational linguistics , but I'm fairly new to the Wikipedia, so I'm not sure. 
 CON  -- see my suggestions under  #Clean-up/Major edit . -- Thüringer ☼  ( talk ) 08:47, 15 January 2009 (UTC) [ reply ] 
 PRO   I have worked in CL/NLP for two decades, and as far as I am aware, there is no clear distinction in practice between CL and NLP, both have the same conferences, the same publications, the same research communities. In my opinion, it would be better to have one merged article, with mention of the different subfields within CL/NLP. 
 Gor  ( talk ) 06:45, 27 March 2009 (UTC) [ reply ] 
 PRO  I work as a researcher in CL/NLP/Text Analytics/AI/Machine Learning/etc.  I think CL and NLP should be merged, in the grand scheme of things, there is not much difference (if any). Either way, as I said under the CL article: It seems to me that the state of things is that the boundary between NLP and CL is unclear. I think the goal of any related Wikipedia articles should be to represent the state of things as accurately as possible, NOT to solve the clarity problem. Thus, both articles should clearly :) state that various opinions about these fields.  Indquimal  ( talk ) 23:15, 20 June 2009 (UTC) [ reply ] 
 PRO  There might be a fine difference between NLP and CL but the difference is tiny and unclear. As some have mentioned, the term NLP is used more by people with Computer Science backgrounds and the term CL is used more by people with Linguistics background, also I believe that CL is somewhat more the theoretical side and NLP the practical side. However, you cannot do one without the other, all NLP applications are based on CL theory, and all CL research is based on experimenting with NLP applications.
 CON  It is important to keep them apart even if today they are both dominated by the computing-oriented approaches.  NLP people generally don't have any formal background in Linguistics, and don't really know much about language (and virtually nothing about Linguistics), and the people tend to sit in Computer Science Departments. CL people have the formal background in Linguistics, and CL is often taught in Linguistics Departments along with the necessary computing skills. The aims of CL are to understand more about language, whilst the aims of NLP are to achieve specific performance goals in a computational context - e.g. specific computer applications, or as an abstract problem in machine learning.  Both are valid approaches from their own disciplinary perspectives, but the current dominance of NLP tends to stifle CL.
 dP (ML/NL/AI/CogSci)  ( talk ) 03:51, 25 August 2012 (UTC) [ reply ] 
 PRO  This is misleading to have two pages for the  same thing . At least at  Paris III University , Master degrees in NLP/CL accept people with background in linguistic and math/cs. I think we should keep  Computational Linguistics  to avoid the confusion with the other  NLP . i⋅am⋅amz3  ( talk ) 23:18, 17 March 2018 (UTC) [ reply ] 
 PRO  I think they should be merged. Not that there are no differences, but at least, these differences (and overlaps) could be made transparent then. The  Computational Linguistics  page is comparably weak, and merging both would lead to a better article. Likewise,  Language technology  should be merged as well, for the same reasons. In either way, Both terms should be defined in their respective subsections after merge.
 Chiarcos  ( talk ) 21:47, 19 January 2021 (UTC) [ reply ] 
 I would like to mention my company, Creative Virtual, because we have over 10 years experience working with virtual assistant natural language web applications, and link to the automated online assistant page.   — Preceding  unsigned  comment added by  75.99.227.213  ( talk ) 20:46, 9 November 2011 (UTC) [ reply ]  
 I append the content from that page, in case anyone wants to merge it in here. 
 Charles Matthews  09:35, 6 May 2004 (UTC) [ reply ] 
 Natural Language Processing (NLP) is inside the topic of  the Artificial Intelligence  and linguistics. It treats the problems inherent in the processing and manipulation of natural language.
 Some examples of the major tasks in Natural Language Processing are: 
 Some problematic things in NLP are:
 In the known spoken language, there are no gaps between words; where to situate the word boundary many times depends on what choice makes the most sense grammatically and given the context.
 
 Any word that we can think of has many different meanings. That is why, we have to select the meaning which makes the most sense in our context. 
 
 – Sign  The grammar for natural languages is ambiguous. Selecting the most appropriate grammatical element requires semantic and contextual information. 
 
 
 Sometimes what we write doesn't mean literaly what is written; for instance a good answer to "Can you give the pencil?" is to give the pencil; in most contexts "Yes" is not the best thing to answer; when you want to say literaly "No" it is better to say "I'm afraid that I can't see it".
 Question edited into the article by  User:129.27.236.115 :
 Cadr 
 It is now.  Yaron  22:40, May 17, 2004 (UTC)
 Removed a spam link (several times) to a website called ivrdictionary.  This is a thinly veiled attempt to put advertising on Wikipedia.  Links were added by several anonymous users within a tight IP range.  Website purports to list ivr terminology, but in reality it prominently displays an advertisement to Angel dot com, which is a commercial company that sells IVR related products.  The same links were added to other articles that are related to IVR technology.   Calltech  16:59, 17 November 2006 (UTC) [ reply ] 
 
 I suggest adding a link to  stemming  in the see also or subtasks or challenges. I am not sure who is responsible for editing this article though, and I don't want to edit it myself without asking. Is stemming too detailed, or a subtask of another subtask only like IR? Not sure. I thought it was a pretty popular problem.  Josh Froelich  19:46, 13 December 2006 (UTC) [ reply ] 
 "I am not sure who is responsible for editing this article though"
You are, feel free to edit any wikipedia page. Yes it feels very wrong the first few time, but your fine to do so. Someone will fix it if your wrong anyhow.
 Scott A Herbert  ( talk ) 13:56, 24 February 2011 (UTC) [ reply ] 
 I think everyone would agree the external links section is a complete mess and full of spam, vanity links, and other links that don't add anything to the article.  I count 47 external links.  I'm sure there is someone out there who supports each one, but I think we all can agree that 47 is too many and there is certainly some redundancy.
 I know it can be hard to part with large chunks of an article, but I propose the following: we assume that we are going to delete all of them and anyone who wants a link kept should nominate it here on the talk page.  We can then discuss whether it actually adds something unique.  Please keep in mind  WP:EL , also.
 -- Selket  22:50, 1 February 2007 (UTC) [ reply ] 
 The Implementations links seem alright. However the R & D groups links are way too many. Unfortunatly, each group would want there own link up there. Also, there were a few links to blogs. Am I right in believing that those links should be deleted?
 Ummonk  22:06, 4 February 2007 (UTC) [ reply ] 
 I cleaned the section up quickly because it had become quite the linkfarm once again. -- Ronz  ( talk ) 15:11, 18 September 2011 (UTC) [ reply ] 
 Just got rid of all references to commercial or even open source software from this section. Let's keep it that way.  Dtunkelang  ( talk ) 22:48, 19 August 2012 (UTC) [ reply ] 
 I expected to find the word "software" to be used more than once on a topic like this.  Software is sort of important in this field, and having a page that lists extant software (regardless of license) with a meaningful comparison of the various options (e.g. key features, license, programming language, APIs)
 My vague understanding is that  maximum entropy methods  represent the state of the art in NLP these days; yet this article seems to fail to mention them. Could an expert clarify/elucidate?  linas  13:17, 13 June 2007 (UTC) [ reply ] 
 Does anyone feel it necessary to distinguish between NLP and HLT?  If so, please visit that article—it desperately needs work.  On the other hand, perhaps it should simply redirect here to the NLP article. — johndburger  02:47, 22 June 2007 (UTC) [ reply ] 
 The following were added to the External links section.  Perhaps one or more might be used as a reference someday?
 -- Ronz  17:36, 14 November 2007 (UTC) [ reply ] 
 I was going to add this in, but I thought it might not be a good Idea.  If you guys can incorporate it well and fit it in, please do:  (I was going to put it after the 'I never said she stole my money' part.)
Accenting words can be very helpful in giving meaning to a sentence that contains negatives, because the speaker is saying that a specific fact is not true, and usually something else without one expressed specific  is .  Sometimes accenting words in a sentence can still lead to confusion, like in "Go  over  there" because "over" is being used to describe the relative position of the destination, but when taken by itself, "over" means ontop of something.  The accent in this case implies a literal meaning of the word...
 24.250.97.223  ( talk ) 04:56, 14 December 2007 (UTC) [ reply ] 
 PRO  As stated on my talk page. Not much there but don't see anything here either so maybe better to do a little something here. Perhaps a  § (NLU, Semantics, Discourse, Top Level Protocols, etc.) to which the NLU article can redirect.  74.78.162.229  ( talk ) 21:38, 10 July 2008 (UTC) [ reply ] 
 PRO  Similarly to  Computer linguistics , I think NLU should be merged into CL because all three of them deal with natural language comprehension by computers.  i⋅am⋅amz3  ( talk ) 01:36, 18 March 2018 (UTC) [ reply ] 
 Set these to values that seemed reasonable to me and manually created the Comments page.  74.78.162.229  ( talk ) 22:01, 10 July 2008 (UTC) [ reply ] 
 As noted in the article header, this article needs major rewriting, restructuring and clean-up. Would anyone like to team up with me to get it done? I'm a wiki-novice but know a fair amount about NLP (and have plenty of references that I can consult).  Sunfishy  ( talk ) 17:39, 5 November 2008 (UTC)sunfishy [ reply ] 
 A significant subproblem not mentioned (directly) is that the great majority of people use words and grammar incorrectly. For example, one of the most frequently seen errors in written text is using "loose" for "lose", as in "Did anyone loose this book?". A typical grammatical error is a golf analyst talking about something being "between he and the hole" instead of "between the hole and him". In fact, if you listen to sportscasters on TV, hardly five minutes will go by without some kind of gross grammatical error or misuse of words. Tens of millions of people are often subjected to this for hours at a time, week after week, possibly having a negative effect on the way they speak.
 Ironically, even the article is guilty of speech misuse under the "Subproblems: Speech acts and plans" heading where it says:  "Can you pass the salt?" is requesting a physical action to be performed.  Actually, the verb "can" means "able to" and as such, DOES request a yes or no answer rather than requesting a physical action. The correct, unambiguous wording is: "Please pass the salt." or at the very least: "Would you pass the salt, please." The question mark is intentionally not used because we are not really asking a question. Also notice that adding "please", like your mother surely told you, instantly clarifies that a physical action is being requested. 
 Speech is only half of communication; the other half is the cooperation of the listener in trying to understand what the speaker means regardless of errors in speech. So any computerized natural language processor must be programmed not only with proper grammar and word meanings, but also with the ability to recognize and correct for IMPROPER speech. Any NLP program which requires perfect word usage, spelling, and grammar is not going to work very well.   71.154.253.96  ( talk ) 14:02, 8 October 2009 (UTC) [ reply ] 
 I do not see a discussion of the July 2008 merge suggestion.  Natural language understanding  is a field unto itself, and I am going to rewrite that article 99.99999% and put a "main link" so there is really no need for a merge. This article is not in good shape either, but it is a much larger field and will need much more attention. It does have several good points in it, but overall a new computer science student would be well advised not to read it until it has been cleaned up. Unless there are objections I will remove the merge flag later. Cheers.  History2007  ( talk ) 21:12, 18 February 2010 (UTC) [ reply ] 
 The second bullet point in the section 'Concrete problems' is copied verbatim from its source,  http://www.kurzweilai.net/articles/art0311.html?printable=1 . Is there permission?   —Preceding  unsigned  comment added by  Jann.poppinga  ( talk  •  contribs ) 14:17, 3 May 2010 (UTC) [ reply ]  
 When I began, concrete problems was essentially a list of largely unelucidated examples; It seems better to work the examples in with some level of explanation (or work some level of explanation in with the examples). I began to do that, and now I'm wondering whether ultimately it wouldn't be better to end up combining this section with the Major tasks section. What that would entail would be including examples along with appropriate tasks to illustrate why that particularly task isn't yet solved, or what's difficult about the task. There's one fairly rich example, the "time flies like an arrow" example, subparts of which could be used under several different problems, so perhaps this example would be set up at the beginning of the list and then different aspects of it referred to appropriately.
 Alternately, it could be interesting to use the examples before the task list as sort of a teaser, a "this is what we have to deal with", followed by a sort of "because of that, these are tasks that must be handled" type thematic progression.
 Opinions?  TehMorp  ( talk ) 15:04, 23 June 2010 (UTC) [ reply ] 
 I think that the 'Concrete Problems' section should be dropped. The "problems" all boil down to the same issue: not being able to determine the intended meanings of words outside of their context. 
 The letter "A" can have many different meanings: the first letter of the English alphabet, a musical note, a grade, etc., just as the phrase "pretty little girls' school" (or any of the other phrases given) can have any of the meanings shown in the section. In each case, the meaning should be determinable by the surrounding context. It is ridiculous to say that understanding such phrases is a problem any more than is understanding which meaning of "A" is intended when no context is given for either.
 Determining the intended meanings of words based on their context is not a "problem" so much as it is the essential goal of NLP. This is not to say that there cannot be ambiguities resulting from poorly worded text, but when when an NLP program detects abiguities which cannot be resolved given the surrounding context, the simple solution is to request clarification from the source of the text.
 75.46.215.114  ( talk ) 12:10, 11 August 2010 (UTC) [ reply ] 
 I did drop this section.  It was repetitive and didn't seem especially useful.  The section on tasks gives a fair amount of explanation of what the issues are for the individual tasks.  For more examples, refer to the articles on specific tasks.  Benwing  ( talk ) 22:18, 3 October 2010 (UTC) [ reply ] 
 "And ALL fruit flies in the same manner - like bananas do;"
 I don't think any program would parse "Time flies like an arrow" this way, given that neither "fruit" nor "bananas" appears in the source sentence.  I suspect this was copied incorrectly, but the original link is now dead.
 Should it read "And ALL time flies in the same manner - like an arrow does"?  That's a pretty big change for a typo.   —Preceding  unsigned  comment added by  216.163.72.2  ( talk ) 00:45, 1 October 2010 (UTC) [ reply ]  
 I don't know if the "Resources" section makes it redundant, but the text doesn't have too many citations. The "NLP using machine learning" section, which is a fairly long piece of text, hasn't got any citations at all. Isn't this needed?  90.233.154.111  ( talk ) 15:38, 11 November 2010 (UTC) [ reply ] 
 
The comment(s) below were originally left at  Talk:Natural language processing/Comments , and are posted here for posterity. Following  several discussions in past years , these subpages are now deprecated. The comments may be irrelevant or outdated; if so, please feel free to remove this section. Last edited at 21:59, 10 July 2008 (UTC).
Substituted at 00:57, 30 April 2016 (UTC)
 @ Biografer :  What is the reason for  this cleanup tag  that you added to this article?  Jarble  ( talk ) 00:47, 8 January 2018 (UTC) [ reply ] 
 I'm not happy with the mismatch between the  Major_evaluations_and_tasks  section (and subsections) and  Category:Tasks_of_natural_language_processing .  mendicott.com  ( talk ) 19:10, 21 March 2018 (UTC) [ reply ] 
 Tried to systematize the  Major_evaluations_and_tasks  section a bit. Did not address mismatch with  Category:Tasks_of_natural_language_processing . IMHO, this cannot be really resolved because the pages in the category focus have no consistent level of granularity.  Chiarcos  ( talk ) 20:41, 17 August 2020 (UTC) [ reply ] 
 Shouldn’t “natural-language processing” be written with a hyphen, as it means “processing of natural language”, not “natural processing of language”?  palpalpalpal  ( talk ) 19:20, 29 September 2019 (UTC) [ reply ] 
 No, conventional spelling is Natural Language Processing (with or without capitalization).  Chiarcos  ( talk ) 20:42, 17 August 2020 (UTC) [ reply ] 
 The current image in the infobox in the top right shows an automated online assistant built (presumably) using NLP technologies. The problem is that it shows a cartoon woman as the assistant. Do we really want to reinforce the stereotype of women assistants by showing it as the first (and only) image for the NLP page on Wikipedia? NLP has a gender bias problem and this image only magnifies it (not to mention alienating women who might be interested in the field). I don't think this particular accurately reflects an application of NLP today in any case.
 I don't have any suggestions for alternative images at the moment, but I feel that an infobox linking NLP to other research areas (Machine Learning, Computational Linguistics, etc) would be more appropriate. For example, look at the infobox for the  Machine Learning  page. Surely the NLP page can be part of some portal/series?   — Preceding  unsigned  comment added by  Venkatasg  ( talk  •  contribs ) 20:07, 4 July 2020 (UTC) [ reply ]  
 While the overlap between cognitive science and NLP (or CL) is important, indeed, this passage does not describe an NLP task and simply doesn't fit the overall text. Either revise and move to an independent section or remove it. I'm inclined to the latter because I see no way to repair that easily.  Chiarcos  ( talk ) 20:45, 17 August 2020 (UTC) [ reply ] 
 An experience towards humanity and all act that promotes human sustainability as ways of transforming human right act to reality including underprivileged communities and to have the greatest purpose by fighting against obstacles and challenges  41.223.132.196  ( talk ) 17:53, 25 May 2023 (UTC) [ reply ] 
 For the disambiguation note at the top, there should be link to 'NLP' page  124.150.139.62  ( talk ) 00:21, 23 July 2023 (UTC) [ reply ] 


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: This category is for articles that are part of  WikiProject Linguistics .
 This category has the following 5 subcategories, out of 5 total.
 The following 200 pages are in this category, out of approximately 13,407 total.  This list may not reflect recent changes .


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: User contributions for 105.60.136.242

Content: No changes were found matching these criteria.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: Search

Content: 

Title: None

Content: Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand  copyright  and the  image use policy  before proceeding.
 
 Uploads to  Wikimedia Commons 
 
 Upload a non-free file 
 Uploads locally to Wikipedia; must comply with the  non-free content  criteria
 
 You do not have JavaScript enabled 
 Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain  Special:Upload  page to upload files to the English Wikipedia without JavaScript.
 You are not currently logged in. 
 Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please  log in  and then try again.
 Your account has not become confirmed yet. 
 Sorry, in order to upload files on the English Wikipedia, you need to have a  confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
 You may already be able to upload files on the  Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
 Important note:  if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at  Wikipedia:Files for upload . 
 In very rare cases an administrator may make your account confirmed manually through a request at  Wikipedia:Requests for permissions/Confirmed . 
 
 
 
 
 
 
 
 
 
 Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for  # < > [ ] | : { } /   and  ~~~ . Your filename has been modified to avoid these. Please check if it is okay now.
 The filename you chose seems to be very short, or overly generic. Please don't use:
 A file of this name already exists on Commons! 
 If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
 This should not be done, except in very rare exceptional cases.
 Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
 A file of this name already exists. 
 If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
 It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. 
 Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the  Wikimedia Commons .
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
   Please consider  uploading your file on Commons . 
 However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
 Please note that by "entirely self-made" we really mean just that. 
 Do not  use this section for any of the following:
 Editors who falsely declare such items as their "own work"  will be blocked from editing .
 Use this  only  if there is an  explicit licensing statement  in the source. 
 The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
 If the source website doesn't say so explicitly, please  do not upload the file .
 Public Domain  means that nobody owns any copyrights on this work. It does  not  mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
 This is  not  for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then  please do not upload it. 
  Please remember that you will need to demonstrate that:
 This file will be used in the following article:
 Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code.  It has to be an actual article, not a talkpage, template, user page, etc.  If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. 
   Example  – article okay.
   This article doesn't exist! 
 The article  Example  could not be found.
 Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
 If this is an article you are only planning to write, please write it first and upload the file afterwards.
   This is not an actual encyclopedia article! 
 The page  Example  is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
 Please upload this file only if it is going to be used in an actual article.
 If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
   This is a disambiguation page! 
 The page  Example  is not a real article, but a disambiguation page pointing to a number of other pages.
 Please check and enter the exact title of the actual target article you meant.
 If neither of these two statements applies, then please  do not upload this image. 
 This section is  not  for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
 In view of this, please explain how the use of this file will be minimal.
 Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
 Please don't upload it. 
 Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is  assumed  to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
 In particular, please don't upload:
 If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at  Wikipedia:Media copyright questions . Thank you.
 This is the data that will be submitted to upload:
 
 
 
 Your file is being uploaded. 
 This might take a minute or two, depending on the size of the file and the speed of your internet connection.
 Once uploading is completed, you will find your new file at this link:
 File:Example.jpg 
 Your file has been uploaded successfully and can now be found here:
 File:Example.jpg 
 Please follow the link and check that the image description page has all the information you meant to include.
 If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
 To insert this file into an article, you may want to use code similar to the following:
 If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
 See  Wikipedia:Picture tutorial  for more detailed help on how to insert and position images in pages.
 Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the  talk page .
 


Title: None

Content: 

Title: Recent changes

Content: This is a list of recent changes to Wikipedia.


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: Categories which use {{ CatAutoTOC }}, grouped by what CatAutoTOC does on each page:
 Templates which transclude {{ CatAutoTOC }} are categorised in
 This category has the following 200 subcategories, out of 10,572 total.


Title: Log in

Content: 

Title: None

Content: People on Wikipedia can use this  talk page  to post a public message about edits made from the IP address you are currently using.
 Many IP addresses change periodically, and are often shared by several people. You may  create an account  or  log in  to avoid future confusion with other logged out users. Creating an account also hides your IP address.


Title: None

Content: Populated by pages where  Template:Large category TOC  has been used  by {{ CatAutoTOC }}  on a  category with 10,001–20,000 pages . 
 Purge page to update totals 
 This category has the following 200 subcategories, out of 987 total.


Title: None

Content: A  regional Internet registry  ( RIR ) is an organization that manages the allocation and registration of  Internet number  resources within a region of the world. Internet number resources include  IP addresses  and  autonomous system (AS)  numbers. 
 The regional Internet registry system evolved, eventually dividing the responsibility for management to a registry for each of five regions of the world. The regional Internet registries are informally liaised through the unincorporated  Number Resource Organization  (NRO), which is a coordinating body to act on matters of global importance. [1] 
 As of 2005, there are currently five regional registries:
 Regional Internet registries  are components of the Internet Number Registry System, which is described in  IETF  RFC 7020, [7]  where IETF stands for the Internet Engineering Task Force. The  Internet Assigned Numbers Authority  (IANA) delegates Internet resources to the RIRs who, in turn, follow their regional policies to delegate resources to their customers, which include  Internet service providers  and end-user organizations. [8]  Collectively, the RIRs participate in the  Number Resource Organization  (NRO), [9]  formed as a body to represent their collective interests, undertake joint activities, and coordinate their activities globally. The NRO has entered into an agreement with  ICANN  for the establishment of the  Address Supporting Organisation  (ASO), [10]  which undertakes coordination of global IP addressing policies within the ICANN framework.
 The  Number Resource Organization  ( NRO ) is an unincorporated organization uniting the five RIRs. [9]  It came into existence on October 24, 2003, when the four existing RIRs entered into a  memorandum of understanding  (MoU) in order to undertake joint activities, including joint technical projects and policy coordination. The youngest RIR,  AFRINIC , joined in April 2005.
 The NRO's main objectives are to:
 A  local Internet registry  ( LIR ) is an organization that has been allocated a block of  IP addresses  by a RIR, and that assigns most parts of this block to its own customers. [11]  Most LIRs are  Internet service providers , enterprises, or academic institutions. Membership in a regional Internet registry is required to become a LIR.


Title: Create account

Content: edits articles recent contributors

Title: Search

Content: 

Title: None

Content: 

Title: None

Content:  The individual episode articles for this series are now being  reviewed  according to  episode notability guidelines .  Please contribute to the discussion on  Talk:List of Hannah Montana episodes#2nd episode article review . Thanks. --  Ned Scott  05:38, 9 July 2007 (UTC) [ reply ] 


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: Log in

Content: 

Title: None

Content: 
 The  World Intellectual Property Organization Copyright Treaty  ( WIPO Copyright Treaty  or  WCT ) is an international  treaty  on  copyright law  adopted by the member states of the  World Intellectual Property Organization  (WIPO) in 1996. It provides additional protections for  copyright  to respond to advances in information technology since the formation of previous copyright treaties before it. [4]  As of August 2023, the treaty has 115 contracting parties. [5]  The WCT and  WIPO Performances and Phonograms Treaty , are together termed WIPO "internet treaties". [6] 
 During the earlier stages of negotiations, the WCT was seen as a protocol to the  Berne Convention , constituting an update of that agreement since the 1971 Stockholm Conference. [7]  However, as any amendment to the Berne Convention required unanimous consent of all parties, the WCT was conceptualized as an additional treaty which supplemented the Berne Convention. [8]  The collapse of negotiations around the extension of the Berne Convention during the 1980s saw the shifting of the forum to the GATT, resulting in  the TRIPS Agreement . [9] [10]  Thus, the nature of any copyright treaty by the  World Intellectual Property Organization  became considerably narrower, being limited to addressing the challenges posed by digital technologies.
 The WCT emphasizes the incentive nature of copyright protection, claiming its importance to creative endeavours. [7]  It ensures that  computer programs  are protected as literary works (Article 4), and that the arrangement and selection of material in  databases  is protected (Article 5). It provides authors of works with control over their rental and distribution in Articles 6 to 8, which they may not have under the  Berne Convention  alone. It also  prohibits circumvention of technological measures  for the protection of works (Article 11) and unauthorized modification of rights management information contained in works (Article 12).
 The treaty has been criticised for being too broad (for example in its prohibition of circumvention of technical protection measures, even where such circumvention is used in the pursuit of legal and fair use rights) and for applying a "one size fits all" standard to all signatory countries, despite their widely differing stages of economic development and knowledge industry.
 The WIPO Copyright Treaty is implemented in United States law by the  Digital Millennium Copyright Act  (DMCA). By Decision 2000/278/EC of 16 March 2000, the  Council of the European Union  approved the treaty on behalf of the European Community.  European Union Directives  which largely cover the subject matter of the treaty are:  Directive 91/250/EC , creating copyright protection for software;  Directive 96/9/EC  on copyright protection for databases; and  Directive 2001/29/EC , prohibiting devices for circumventing "technical protection measures", such as  digital rights management  (also known as DRM).


Title: None

Content: The Internet is not controlled by a single person or body in the way that a corporation or a state might be. Instead, different functions are carried out by different bodies, in a way which is still evolving. This category pulls together articles on the organization of the Internet and on the bodies and groups which have a role in its development and operation. 
 The use of the word "governance" here probably implies more power than actually exists in these bodies, as they essentially represent just a collection of standards for sharing traffic between networks. 
 This category has the following 4 subcategories, out of 4 total.
 The following 54 pages are in this category, out of  54 total.  This list may not reflect recent changes .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: 
 Works anywhere in the text         
 ''italics'', '''bold''', and '''''both''''' 
 italics ,  bold , and   both 
 [[copy edit]] 
 [[copy edit]]ors 
 copy edit 
 copy editors 
 " Pipe " a link to change the link's text
 [[Android (operating system)|Android]] 
 Android 
 Link to a section
 [[Frog#Locomotion]] 
 [[Frog#Locomotion|locomotion in frogs]] 
 {{slink|Frog#Locomotion}} 
 Frog#Locomotion 
 locomotion in frogs 
 Frog § Locomotion 
 [[Red link example]] 
 Red link example 
 https://www.wikipedia.org 
 https://www.wikipedia.org 
 [https://www.wikipedia.org] 
 [1] 
 [https://www.wikipedia.org/ Wikipedia] 
 Wikipedia 
 Hello [1]  World! [2] 
 Hello again! [1] [3] 
 This statement is true.{{cn}} 
 This statement is true. [ citation needed ] 
 ~~~~ do not sign in an article, only on talk pages 
 Username  ( talk ) 04:19, 22 April 2024 (UTC)
 [[User:Example]]  or  {{u|Example}} 
 User:Example  or  Example 
 <s> This topic isn't [[ WP:N | notable ]]. </s> 
 This topic isn't  notable . 
 <u>This topic is notable</u> 
 This topic is notable 
 <!-- This had consensus, discuss at talk page --> 
 
 [[File:Wiki.png|thumb|Caption]] 
 
 #REDIRECT [[Target page]] 
   Target page 
 #REDIRECT [[Target page#anchorName]] 
   Target page#anchorName 
 == Level 2 == 
 === Level 3 === 
 ==== Level 4 ==== 
 ===== Level 5 ===== 
 ====== Level 6 ====== 
 do not use   = Level 1 =   as it is for page titles 
 * One 
 * Two 
 ** Two point one 
 * Three 
 # One 
 # Two 
 ## Two point one 
 # Three 
 no indent (normal) 
 : first indent 
 :: second indent 
 ::: third indent 
 :::: fourth indent 
 {{Outdent|4}} return to left margin 
 no indent (normal) 
 
   Kindness Campaign 


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: None

Content: 
This category is used for tracking pages with  excerpts  that are not articles. For articles, there's  Category:Articles with excerpts  and there's also  Category:Articles with broken excerpts . This category has only the following subcategory.


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 Works anywhere in the text         
 ''italics'', '''bold''', and '''''both''''' 
 italics ,  bold , and   both 
 [[copy edit]] 
 [[copy edit]]ors 
 copy edit 
 copy editors 
 " Pipe " a link to change the link's text
 [[Android (operating system)|Android]] 
 Android 
 Link to a section
 [[Frog#Locomotion]] 
 [[Frog#Locomotion|locomotion in frogs]] 
 {{slink|Frog#Locomotion}} 
 Frog#Locomotion 
 locomotion in frogs 
 Frog § Locomotion 
 [[Red link example]] 
 Red link example 
 https://www.wikipedia.org 
 https://www.wikipedia.org 
 [https://www.wikipedia.org] 
 [1] 
 [https://www.wikipedia.org/ Wikipedia] 
 Wikipedia 
 Hello [1]  World! [2] 
 Hello again! [1] [3] 
 This statement is true.{{cn}} 
 This statement is true. [ citation needed ] 
 ~~~~ do not sign in an article, only on talk pages 
 Username  ( talk ) 04:19, 22 April 2024 (UTC)
 [[User:Example]]  or  {{u|Example}} 
 User:Example  or  Example 
 <s> This topic isn't [[ WP:N | notable ]]. </s> 
 This topic isn't  notable . 
 <u>This topic is notable</u> 
 This topic is notable 
 <!-- This had consensus, discuss at talk page --> 
 
 [[File:Wiki.png|thumb|Caption]] 
 
 #REDIRECT [[Target page]] 
   Target page 
 #REDIRECT [[Target page#anchorName]] 
   Target page#anchorName 
 == Level 2 == 
 === Level 3 === 
 ==== Level 4 ==== 
 ===== Level 5 ===== 
 ====== Level 6 ====== 
 do not use   = Level 1 =   as it is for page titles 
 * One 
 * Two 
 ** Two point one 
 * Three 
 # One 
 # Two 
 ## Two point one 
 # Three 
 no indent (normal) 
 : first indent 
 :: second indent 
 ::: third indent 
 :::: fourth indent 
 {{Outdent|4}} return to left margin 
 no indent (normal) 
 
   Kindness Campaign 


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 This is a tracking category for  CS1 citations  that use a value of  |volume=  longer than four characters and not wholly numeric or uppercase Roman numerals. 
 No error or maintenance message is displayed. No changes are required to remove pages from this category, but some values of  |volume=  may fit better in other parameters.
 Pages in this category should only be added by  Module:Citation/CS1 . 
 The following 200 pages are in this category, out of approximately 64,180 total.  This list may not reflect recent changes .
 The following 7 files are in this category, out of 7 total.


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: ← 
 → 
 Events from the year  1740 in Canada .
 
 Description of making and using  Mi'kmaw  canoes, both  moosehide  (in past) and  birchbark  currently used [4] 
 Woman in  Montreal  who needs money sells enslaved 20-year-old  Pawnee  named Manon for 300  livres  "in receipts from the  Beaver trade " [5] 
 In spring and summer,  Joseph La France  canoes  Rainy Lake  and  Lake of the Woods , meeting  Monsoni  Ojibwe and "Sturgeon Indians" [6] 
 Suffering from  gout ,  Jesuit  ministers to  Indigenous people , parish of 400 and distant members of his flock near Montreal (Note: "savages" used) [7] 
 Council  president  Paul Mascarene  "notifies the Indians and inhabitants" of Nova Scotia that King has declared war on King of Spain [8] 
 To preserve "Indulgence they have heitherto Enjoyed,"  Acadians  are reminded to conform to government orders and decisions [9] 
 Mascarene letter (summary) ends with warning to Acadians to be loyal or face reaction that "will involve the innocent with the guilty" [10] 
 Acadian deputies  to handle "restless spirits" so that "community may not make itself suspected, and avoid the ruin which may overtake it" [11] 
 Mascarene specifies some civil service roles, and is concerned that in "these thirty years past,"  Protestants  have not peopled Nova Scotia [12] 
 Handling Acadians' need for new land when it is allowed only to Protestants means letting them take land anyway or expelling them [13] 
 Fearing unauthorized priest will direct when "a stroke" is to be given their government, Council decides his community must expel him [14] 
 Priests forbidden to  excommunicate  "Whereby to Deprive His Majesty's Subjects[...]of Assistance or means To Procure their  Livelyhood " [15] 
 Mascarene advises missionary priest of  King's supremacy  over both Catholic Church and his conduct in Nova Scotia [16] 
 Mascarene reports that some shippers into and out of Nova Scotia are not  clearing  with port authorities [17] 
 "Succeeded far above our Expectations" - " Indian trade " at  Oswego  has undercut prices at Montreal by half and increased trade fivefold [18] 
 "Be always on your Guard" -  Hudson's Bay Company  urges Bay staff to be prepared for (probably unlikely) attack by Spanish [19] 
 Given war with Spain and perhaps France, chief factor at  Prince of Wales Fort  cancels next year's northern expedition in order to augment defences [20] 


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: 
 This is a tracking category for  CS1 citations  that use a value of  |volume=  longer than four characters and not wholly numeric or uppercase Roman numerals. 
 No error or maintenance message is displayed. No changes are required to remove pages from this category, but some values of  |volume=  may fit better in other parameters.
 Pages in this category should only be added by  Module:Citation/CS1 . 
 The following 200 pages are in this category, out of approximately 64,180 total.  This list may not reflect recent changes .
 The following 7 files are in this category, out of 7 total.


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: 
 The  World Intellectual Property Organization Copyright Treaty  ( WIPO Copyright Treaty  or  WCT ) is an international  treaty  on  copyright law  adopted by the member states of the  World Intellectual Property Organization  (WIPO) in 1996. It provides additional protections for  copyright  to respond to advances in information technology since the formation of previous copyright treaties before it. [4]  As of August 2023, the treaty has 115 contracting parties. [5]  The WCT and  WIPO Performances and Phonograms Treaty , are together termed WIPO "internet treaties". [6] 
 During the earlier stages of negotiations, the WCT was seen as a protocol to the  Berne Convention , constituting an update of that agreement since the 1971 Stockholm Conference. [7]  However, as any amendment to the Berne Convention required unanimous consent of all parties, the WCT was conceptualized as an additional treaty which supplemented the Berne Convention. [8]  The collapse of negotiations around the extension of the Berne Convention during the 1980s saw the shifting of the forum to the GATT, resulting in  the TRIPS Agreement . [9] [10]  Thus, the nature of any copyright treaty by the  World Intellectual Property Organization  became considerably narrower, being limited to addressing the challenges posed by digital technologies.
 The WCT emphasizes the incentive nature of copyright protection, claiming its importance to creative endeavours. [7]  It ensures that  computer programs  are protected as literary works (Article 4), and that the arrangement and selection of material in  databases  is protected (Article 5). It provides authors of works with control over their rental and distribution in Articles 6 to 8, which they may not have under the  Berne Convention  alone. It also  prohibits circumvention of technological measures  for the protection of works (Article 11) and unauthorized modification of rights management information contained in works (Article 12).
 The treaty has been criticised for being too broad (for example in its prohibition of circumvention of technical protection measures, even where such circumvention is used in the pursuit of legal and fair use rights) and for applying a "one size fits all" standard to all signatory countries, despite their widely differing stages of economic development and knowledge industry.
 The WIPO Copyright Treaty is implemented in United States law by the  Digital Millennium Copyright Act  (DMCA). By Decision 2000/278/EC of 16 March 2000, the  Council of the European Union  approved the treaty on behalf of the European Community.  European Union Directives  which largely cover the subject matter of the treaty are:  Directive 91/250/EC , creating copyright protection for software;  Directive 96/9/EC  on copyright protection for databases; and  Directive 2001/29/EC , prohibiting devices for circumventing "technical protection measures", such as  digital rights management  (also known as DRM).


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: 
 Works anywhere in the text         
 ''italics'', '''bold''', and '''''both''''' 
 italics ,  bold , and   both 
 [[copy edit]] 
 [[copy edit]]ors 
 copy edit 
 copy editors 
 " Pipe " a link to change the link's text
 [[Android (operating system)|Android]] 
 Android 
 Link to a section
 [[Frog#Locomotion]] 
 [[Frog#Locomotion|locomotion in frogs]] 
 {{slink|Frog#Locomotion}} 
 Frog#Locomotion 
 locomotion in frogs 
 Frog § Locomotion 
 [[Red link example]] 
 Red link example 
 https://www.wikipedia.org 
 https://www.wikipedia.org 
 [https://www.wikipedia.org] 
 [1] 
 [https://www.wikipedia.org/ Wikipedia] 
 Wikipedia 
 Hello [1]  World! [2] 
 Hello again! [1] [3] 
 This statement is true.{{cn}} 
 This statement is true. [ citation needed ] 
 ~~~~ do not sign in an article, only on talk pages 
 Username  ( talk ) 04:19, 22 April 2024 (UTC)
 [[User:Example]]  or  {{u|Example}} 
 User:Example  or  Example 
 <s> This topic isn't [[ WP:N | notable ]]. </s> 
 This topic isn't  notable . 
 <u>This topic is notable</u> 
 This topic is notable 
 <!-- This had consensus, discuss at talk page --> 
 
 [[File:Wiki.png|thumb|Caption]] 
 
 #REDIRECT [[Target page]] 
   Target page 
 #REDIRECT [[Target page#anchorName]] 
   Target page#anchorName 
 == Level 2 == 
 === Level 3 === 
 ==== Level 4 ==== 
 ===== Level 5 ===== 
 ====== Level 6 ====== 
 do not use   = Level 1 =   as it is for page titles 
 * One 
 * Two 
 ** Two point one 
 * Three 
 # One 
 # Two 
 ## Two point one 
 # Three 
 no indent (normal) 
 : first indent 
 :: second indent 
 ::: third indent 
 :::: fourth indent 
 {{Outdent|4}} return to left margin 
 no indent (normal) 
 
   Kindness Campaign 


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: 
 Year  1298  ( MCCXCVIII ) was a  common year starting on Wednesday  (link will display the full calendar) of the  Julian calendar .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: 
 Works anywhere in the text         
 ''italics'', '''bold''', and '''''both''''' 
 italics ,  bold , and   both 
 [[copy edit]] 
 [[copy edit]]ors 
 copy edit 
 copy editors 
 " Pipe " a link to change the link's text
 [[Android (operating system)|Android]] 
 Android 
 Link to a section
 [[Frog#Locomotion]] 
 [[Frog#Locomotion|locomotion in frogs]] 
 {{slink|Frog#Locomotion}} 
 Frog#Locomotion 
 locomotion in frogs 
 Frog § Locomotion 
 [[Red link example]] 
 Red link example 
 https://www.wikipedia.org 
 https://www.wikipedia.org 
 [https://www.wikipedia.org] 
 [1] 
 [https://www.wikipedia.org/ Wikipedia] 
 Wikipedia 
 Hello [1]  World! [2] 
 Hello again! [1] [3] 
 This statement is true.{{cn}} 
 This statement is true. [ citation needed ] 
 ~~~~ do not sign in an article, only on talk pages 
 Username  ( talk ) 04:19, 22 April 2024 (UTC)
 [[User:Example]]  or  {{u|Example}} 
 User:Example  or  Example 
 <s> This topic isn't [[ WP:N | notable ]]. </s> 
 This topic isn't  notable . 
 <u>This topic is notable</u> 
 This topic is notable 
 <!-- This had consensus, discuss at talk page --> 
 
 [[File:Wiki.png|thumb|Caption]] 
 
 #REDIRECT [[Target page]] 
   Target page 
 #REDIRECT [[Target page#anchorName]] 
   Target page#anchorName 
 == Level 2 == 
 === Level 3 === 
 ==== Level 4 ==== 
 ===== Level 5 ===== 
 ====== Level 6 ====== 
 do not use   = Level 1 =   as it is for page titles 
 * One 
 * Two 
 ** Two point one 
 * Three 
 # One 
 # Two 
 ## Two point one 
 # Three 
 no indent (normal) 
 : first indent 
 :: second indent 
 ::: third indent 
 :::: fourth indent 
 {{Outdent|4}} return to left margin 
 no indent (normal) 
 
   Kindness Campaign 


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: 
 The  World Intellectual Property Organization Copyright Treaty  ( WIPO Copyright Treaty  or  WCT ) is an international  treaty  on  copyright law  adopted by the member states of the  World Intellectual Property Organization  (WIPO) in 1996. It provides additional protections for  copyright  to respond to advances in information technology since the formation of previous copyright treaties before it. [4]  As of August 2023, the treaty has 115 contracting parties. [5]  The WCT and  WIPO Performances and Phonograms Treaty , are together termed WIPO "internet treaties". [6] 
 During the earlier stages of negotiations, the WCT was seen as a protocol to the  Berne Convention , constituting an update of that agreement since the 1971 Stockholm Conference. [7]  However, as any amendment to the Berne Convention required unanimous consent of all parties, the WCT was conceptualized as an additional treaty which supplemented the Berne Convention. [8]  The collapse of negotiations around the extension of the Berne Convention during the 1980s saw the shifting of the forum to the GATT, resulting in  the TRIPS Agreement . [9] [10]  Thus, the nature of any copyright treaty by the  World Intellectual Property Organization  became considerably narrower, being limited to addressing the challenges posed by digital technologies.
 The WCT emphasizes the incentive nature of copyright protection, claiming its importance to creative endeavours. [7]  It ensures that  computer programs  are protected as literary works (Article 4), and that the arrangement and selection of material in  databases  is protected (Article 5). It provides authors of works with control over their rental and distribution in Articles 6 to 8, which they may not have under the  Berne Convention  alone. It also  prohibits circumvention of technological measures  for the protection of works (Article 11) and unauthorized modification of rights management information contained in works (Article 12).
 The treaty has been criticised for being too broad (for example in its prohibition of circumvention of technical protection measures, even where such circumvention is used in the pursuit of legal and fair use rights) and for applying a "one size fits all" standard to all signatory countries, despite their widely differing stages of economic development and knowledge industry.
 The WIPO Copyright Treaty is implemented in United States law by the  Digital Millennium Copyright Act  (DMCA). By Decision 2000/278/EC of 16 March 2000, the  Council of the European Union  approved the treaty on behalf of the European Community.  European Union Directives  which largely cover the subject matter of the treaty are:  Directive 91/250/EC , creating copyright protection for software;  Directive 96/9/EC  on copyright protection for databases; and  Directive 2001/29/EC , prohibiting devices for circumventing "technical protection measures", such as  digital rights management  (also known as DRM).


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: 
 Works anywhere in the text         
 ''italics'', '''bold''', and '''''both''''' 
 italics ,  bold , and   both 
 [[copy edit]] 
 [[copy edit]]ors 
 copy edit 
 copy editors 
 " Pipe " a link to change the link's text
 [[Android (operating system)|Android]] 
 Android 
 Link to a section
 [[Frog#Locomotion]] 
 [[Frog#Locomotion|locomotion in frogs]] 
 {{slink|Frog#Locomotion}} 
 Frog#Locomotion 
 locomotion in frogs 
 Frog § Locomotion 
 [[Red link example]] 
 Red link example 
 https://www.wikipedia.org 
 https://www.wikipedia.org 
 [https://www.wikipedia.org] 
 [1] 
 [https://www.wikipedia.org/ Wikipedia] 
 Wikipedia 
 Hello [1]  World! [2] 
 Hello again! [1] [3] 
 This statement is true.{{cn}} 
 This statement is true. [ citation needed ] 
 ~~~~ do not sign in an article, only on talk pages 
 Username  ( talk ) 04:19, 22 April 2024 (UTC)
 [[User:Example]]  or  {{u|Example}} 
 User:Example  or  Example 
 <s> This topic isn't [[ WP:N | notable ]]. </s> 
 This topic isn't  notable . 
 <u>This topic is notable</u> 
 This topic is notable 
 <!-- This had consensus, discuss at talk page --> 
 
 [[File:Wiki.png|thumb|Caption]] 
 
 #REDIRECT [[Target page]] 
   Target page 
 #REDIRECT [[Target page#anchorName]] 
   Target page#anchorName 
 == Level 2 == 
 === Level 3 === 
 ==== Level 4 ==== 
 ===== Level 5 ===== 
 ====== Level 6 ====== 
 do not use   = Level 1 =   as it is for page titles 
 * One 
 * Two 
 ** Two point one 
 * Three 
 # One 
 # Two 
 ## Two point one 
 # Three 
 no indent (normal) 
 : first indent 
 :: second indent 
 ::: third indent 
 :::: fourth indent 
 {{Outdent|4}} return to left margin 
 no indent (normal) 
 
   Kindness Campaign 


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: 
This category is used for tracking pages with  excerpts  that are not articles. For articles, there's  Category:Articles with excerpts  and there's also  Category:Articles with broken excerpts . This category has only the following subcategory.


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: 
 Works anywhere in the text         
 ''italics'', '''bold''', and '''''both''''' 
 italics ,  bold , and   both 
 [[copy edit]] 
 [[copy edit]]ors 
 copy edit 
 copy editors 
 " Pipe " a link to change the link's text
 [[Android (operating system)|Android]] 
 Android 
 Link to a section
 [[Frog#Locomotion]] 
 [[Frog#Locomotion|locomotion in frogs]] 
 {{slink|Frog#Locomotion}} 
 Frog#Locomotion 
 locomotion in frogs 
 Frog § Locomotion 
 [[Red link example]] 
 Red link example 
 https://www.wikipedia.org 
 https://www.wikipedia.org 
 [https://www.wikipedia.org] 
 [1] 
 [https://www.wikipedia.org/ Wikipedia] 
 Wikipedia 
 Hello [1]  World! [2] 
 Hello again! [1] [3] 
 This statement is true.{{cn}} 
 This statement is true. [ citation needed ] 
 ~~~~ do not sign in an article, only on talk pages 
 Username  ( talk ) 04:19, 22 April 2024 (UTC)
 [[User:Example]]  or  {{u|Example}} 
 User:Example  or  Example 
 <s> This topic isn't [[ WP:N | notable ]]. </s> 
 This topic isn't  notable . 
 <u>This topic is notable</u> 
 This topic is notable 
 <!-- This had consensus, discuss at talk page --> 
 
 [[File:Wiki.png|thumb|Caption]] 
 
 #REDIRECT [[Target page]] 
   Target page 
 #REDIRECT [[Target page#anchorName]] 
   Target page#anchorName 
 == Level 2 == 
 === Level 3 === 
 ==== Level 4 ==== 
 ===== Level 5 ===== 
 ====== Level 6 ====== 
 do not use   = Level 1 =   as it is for page titles 
 * One 
 * Two 
 ** Two point one 
 * Three 
 # One 
 # Two 
 ## Two point one 
 # Three 
 no indent (normal) 
 : first indent 
 :: second indent 
 ::: third indent 
 :::: fourth indent 
 {{Outdent|4}} return to left margin 
 no indent (normal) 
 
   Kindness Campaign 


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This category has the following 19 subcategories, out of 19 total.
 The following 90 pages are in this category, out of  90 total.  This list may not reflect recent changes .


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: Natural language processing  ( NLP ) is an  interdisciplinary  subfield of  computer science  and  information retrieval . It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing  natural language  datasets, such as  text corpora  or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based)  machine learning  approaches. The goal is a computer capable of "understanding" [ citation needed ]  the contents of documents, including the  contextual  nuances of the language within them. To this end, natural language processing often borrows ideas from  theoretical linguistics . The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve  speech recognition ,  natural-language understanding , and  natural-language generation .
 Natural language processing has its roots in the 1940s. [1]  Already in 1940,  Alan Turing  published an article titled " Computing Machinery and Intelligence " which proposed what is now called the  Turing test  as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by  John Searle 's  Chinese room  experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  machine learning  algorithms for language processing.  This was due to both the steady increase in computational power (see  Moore's law ) and the gradual lessening of the dominance of  Chomskyan  theories of linguistics (e.g.  transformational grammar ), whose theoretical underpinnings discouraged the sort of  corpus linguistics  that underlies the machine-learning approach to language processing. [8]  
 In 2003,  word n-gram model , at the time the best statistical algorithm, was overperformed by a  multi-layer perceptron  (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in  language modelling ) by  Yoshua Bengio  with co-authors. [9]  
 In 2010,  Tomáš Mikolov  (then a PhD student at  Brno University of Technology ) with co-authors applied a simple  recurrent neural network  with a single hidden layer to language modelling, [10]  and in the following years he went on to develop  Word2vec . In the 2010s,  representation learning  and  deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [11] [12]  can achieve state-of-the-art results in many natural language tasks, e.g., in  language modeling [13]  and parsing. [14] [15]  This is increasingly important  in medicine and healthcare , where NLP helps analyze notes and text in  electronic health records  that would otherwise be inaccessible for study when seeking to improve care [16]  or protect patient privacy. [17] 
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [18] [19]  such as by writing grammars or devising heuristic rules for  stemming .
 Machine learning  approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of  LLMs  in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of  AI winter , which was caused by the inefficiencies of the rule-based approaches. [20] [21] 
 The earliest  decision trees , producing systems of hard  if–then rules , were still very similar to the old rule-based approaches.
Only the introduction of hidden  Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate  feature engineering . Since 2015, [22]  the statistical approach was replaced by the  neural networks  approach, using  word embeddings  to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation , based on then-newly-invented  sequence-to-sequence  transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for  statistical machine translation .
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [44] 
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition  refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." [45]   Cognitive science  is the interdisciplinary, scientific study of the mind and its processes. [46]   Cognitive linguistics  is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [47]  Especially during the age of  symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies.
 As an example,  George Lakoff  offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [48]  with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [51]  functional grammar, [52]  construction grammar, [53]  computational psycholinguistics and cognitive neuroscience (e.g.,  ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [54]  of the  ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve  explainability , e.g., under the notion of "cognitive AI". [55]  Likewise, ideas of cognitive NLP are inherent to neural models  multimodal  NLP (although rarely made explicit) [56]  and developments in  artificial intelligence , specifically tools and technologies using  large language model  approaches [57]  and new directions in  artificial general intelligence  based on the  free energy principle [58]  by British neuroscientist and theoretician at University College London  Karl J. Friston .


Title: None

Content: 
 A  language model  is a probabilistic model of a natural language. [1]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [2] 
 Language models are useful for a variety of tasks, including  speech recognition [3]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [4]   natural language generation  (generating more human-like text),  optical character recognition ,  handwriting recognition , [5]   grammar induction , [6]  and  information retrieval . [7] [8] 
 Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public internet),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model .
 A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [9]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [10]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is
 where  
   
     
       
         Z 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
             − 
             1 
           
         
         ) 
       
     
     {\displaystyle Z(w_{1},\ldots ,w_{m-1})} 
   
  is the  partition function ,  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  is the parameter vector, and  
   
     
       
         f 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle f(w_{1},\ldots ,w_{m})} 
   
  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  
   
     
       
         a 
       
     
     {\displaystyle a} 
   
  or some form of regularization.
 The log-bilinear model is another example of an exponential language model.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [11] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [14]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [15] 
 A  large language model  (LLM) is a language model notable for its ability to achieve general-purpose language generation and other  natural language processing  tasks such as  classification . LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive  self-supervised  and  semi-supervised  training process. [16]  LLMs can be used for text generation, a form of  generative AI , by taking an input text and repeatedly predicting the next token or word. [17] 
 LLMs are  artificial neural networks . The largest and most capable, as of March 2024 [update] , are built with a decoder-only  transformer -based architecture while some recent implementations are based on other architectures, such as  recurrent neural network  variants and  Mamba  (a  state space  model). [18] [19] [20] 
 Up to 2020,  fine tuning  was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as  GPT-3 , however, can be  prompt-engineered  to achieve similar results. [21]  They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and  biases  present in the corpora. [22] 
 Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [23] 
 Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [24] 
 Various data sets have been developed for use in evaluating language processing systems. [25]  These include:


Title: None

Content: A  feedforward neural network  ( FNN ) is one of the two broad types of  artificial neural network , characterized by direction of the flow of information between its layers. [2]  Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the  hidden nodes  (if any) and to the output nodes, without any cycles or loops, [2]  in contrast to  recurrent neural networks , [3]  which have a bi-directional flow. Modern feedforward networks are trained using the  backpropagation  method [4] [5] [6] [7] [8]  and are colloquially referred to as the "vanilla" neural networks. [9] 
 The two historically common  activation functions  are both  sigmoids , and are described by
 The first is a  hyperbolic tangent  that ranges from -1 to 1, while the other is the  logistic function , which is similar in shape but ranges from 0 to 1. Here  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is the output of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 th node (neuron) and  
   
     
       
         
           v 
           
             i 
           
         
       
     
     {\displaystyle v_{i}} 
   
  is the weighted sum of the input connections. Alternative activation functions have been proposed, including the  rectifier and softplus  functions. More specialized activation functions include  radial basis functions  (used in  radial basis networks , another class of supervised neural network models).
 In recent developments of  deep learning  the  rectified linear unit (ReLU)  is more frequently used as one of the possible ways to overcome the numerical  problems  related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of  supervised learning , and is carried out through  backpropagation .
 We can represent the degree of error in an output node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  in the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point (training example) by  
   
     
       
         
           e 
           
             j 
           
         
         ( 
         n 
         ) 
         = 
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
         − 
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} 
   
 , where  
   
     
       
         
           d 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle d_{j}(n)} 
   
  is the desired target value for  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
 , and  
   
     
       
         
           y 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{j}(n)} 
   
  is the value produced at node  
   
     
       
         j 
       
     
     {\displaystyle j} 
   
  when the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the  
   
     
       
         n 
       
     
     {\displaystyle n} 
   
 th data point, given by
 Using  gradient descent , the change in each weight  
   
     
       
         
           w 
           
             i 
             j 
           
         
       
     
     {\displaystyle w_{ij}} 
   
  is
 where  
   
     
       
         
           y 
           
             i 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle y_{i}(n)} 
   
  is the output of the previous neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 , and  
   
     
       
         η 
       
     
     {\displaystyle \eta } 
   
  is the  learning rate , which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression,  
   
     
       
         
           
             
               ∂ 
               
                 
                   E 
                 
               
               ( 
               n 
               ) 
             
             
               ∂ 
               
                 v 
                 
                   j 
                 
               
               ( 
               n 
               ) 
             
           
         
       
     
     {\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} 
   
  denotes the partial derivate of the error  
   
     
       
         
           
             E 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\mathcal {E}}(n)} 
   
  according to the weighted sum  
   
     
       
         
           v 
           
             j 
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle v_{j}(n)} 
   
  of the input connections of neuron  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 .
 The derivative to be calculated depends on the induced local field  
   
     
       
         
           v 
           
             j 
           
         
       
     
     {\displaystyle v_{j}} 
   
 , which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where  
   
     
       
         
           ϕ 
           
             ′ 
           
         
       
     
     {\displaystyle \phi ^{\prime }} 
   
  is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the  
   
     
       
         k 
       
     
     {\displaystyle k} 
   
 th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. [24] 
 The simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The  mean squared errors  between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the  method of least squares  or  linear regression . It was used as a means of finding a good rough linear fit to a set of points by  Legendre  (1805) and  Gauss  (1795) for the prediction of planetary movement. [25] [26] [27] [12] [28] 
 If using a threshold, i.e. a linear  activation  function,  the resulting  linear threshold unit  is called a  perceptron . (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to  approximate any continuous function  from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer,  Harald Burgsteiner  and  Wolfgang Maass  "A learning rule for very simple universal approximators consisting of a single layer of perceptrons". [29] 
 Perceptrons can be trained by a simple learning algorithm that is usually called the  delta rule . It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of  gradient descent .
 A  multilayer perceptron  ( MLP ) is a  misnomer  for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not  linearly separable . [30]  It is a misnomer because the original  perceptron  used a  Heaviside step function , instead of a  nonlinear  kind of activation function (used by modern networks).
 Examples of other feedforward networks include  convolutional neural networks  and  radial basis function networks , which use a different activation function.


Title: Word 

Content: A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network -based models, which have been superseded by  large language models .  [1]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word was considered, it was called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [2]  Special tokens were introduced to denote the start and end of a sentence  
   
     
       
         ⟨ 
         s 
         ⟩ 
       
     
     {\displaystyle \langle s\rangle } 
   
  and  
   
     
       
         ⟨ 
         
           / 
         
         s 
         ⟩ 
       
     
     {\displaystyle \langle /s\rangle } 
   
 .
 To prevent a zero probability being assigned to unseen words, each word's probability is slightly lower than its frequency count in a corpus. To calculate it, various methods were used, from simple "add-one" smoothing (assign a count of 1 to unseen  n -grams, as an  uninformative prior ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .
 A special case, where  n  = 1, is called a unigram model. Probability of each word in a sequence is independent from probabilities of other word in the sequence. Each word's probability in the sequence is equal to the word's probability in an entire document.  
 The model consists of units, each treated as one-state  finite automata . [3]   Words with their probabilities in a document can be illustrated as follows. 
 Total mass of word probabilities distributed across the document's vocabulary, is 1. 
 The probability generated for a specific query is calculated as
 Unigram models of different documents have different probabilities of words in it. The probability distributions from different documents are used to generate hit probabilities for each query. Documents can be ranked for a query according to the probabilities. Example of unigram models of two documents:
 In a bigram word ( n  = 2) language model, the probability of the sentence  I saw the red house  is approximated as
 In a trigram ( n  = 3) language model, the approximation is
 Note that the context of the first  n  – 1  n -grams is filled with start-of-sentence markers, typically denoted <s>.
 Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence  *I saw the  would always be higher than that of the longer sentence  I saw the red house. 
 The approximation method calculates the probability  
   
     
       
         P 
         ( 
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
         ) 
       
     
     {\displaystyle P(w_{1},\ldots ,w_{m})} 
   
  of observing the sentence  
   
     
       
         
           w 
           
             1 
           
         
         , 
         … 
         , 
         
           w 
           
             m 
           
         
       
     
     {\displaystyle w_{1},\ldots ,w_{m}} 
   
 
 It is assumed that the probability of observing the  i th  word  w i  (in the context window consisting of the preceding  i  − 1 words) can be approximated by the probability of observing it in the shortened context window consisting of the preceding  n  − 1 words ( n th -order  Markov property ). To clarify, for  n  = 3 and  i  = 2 we have  
   
     
       
         P 
         ( 
         
           w 
           
             i 
           
         
         ∣ 
         
           w 
           
             i 
             − 
             ( 
             n 
             − 
             1 
             ) 
           
         
         , 
         … 
         , 
         
           w 
           
             i 
             − 
             1 
           
         
         ) 
         = 
         P 
         ( 
         
           w 
           
             2 
           
         
         ∣ 
         
           w 
           
             1 
           
         
         ) 
       
     
     {\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})=P(w_{2}\mid w_{1})} 
   
 .
 The conditional probability can be calculated from  n -gram model frequency counts:
 An issue when using  n -gram language models are out-of-vocabulary (OOV) words. They are encountered in  computational linguistics  and  natural language processing  when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the  n -grams in the  corpus  that contain an out-of-vocabulary word are ignored. The  n -gram probabilities are smoothed over all the words in the vocabulary even if they were not observed. [4] 
 Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g.  <unk> ) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before  n -grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of  n -grams involving out-of-vocabulary words. [5] 
 n -grams were also used for approximate matching. If we convert strings (with only letters in the English alphabet) into character 3-grams, we get a  
   
     
       
         
           26 
           
             3 
           
         
       
     
     {\displaystyle 26^{3}} 
   
 -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string.  However, we know empirically that if two strings of real text have a similar vector representation (as measured by  cosine distance ) then they are likely to be similar. Other metrics have also been applied to vectors of  n -grams with varying, sometimes better, results. For example,  z-scores  have been used to compare documents by examining how many standard deviations each  n -gram differs from its mean occurrence in a large collection, or  text corpus , of documents (which form the "background" vector).  In the event of small counts, the  g-score  (also known as  g-test ) gave better results.
 It is also possible to take a more principled approach to the statistics of  n -grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in  Bayesian inference .
 n -gram-based searching was also used for  plagiarism detection .
 To choose a value for  n  in an  n -gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.
 There are problems of balance weight between  infrequent grams  (for example, if a proper name appeared in the training data) and  frequent grams .   Also, items not seen in the training data will be given a  probability  of 0.0 without  smoothing . For unseen but plausible data from a sample, one can introduce  pseudocounts .  Pseudocounts are generally motivated on Bayesian grounds.
 In practice it was necessary to  smooth  the probability distributions by also assigning non-zero probabilities to unseen words or  n -grams.  The reason is that models derived directly from the  n -gram frequency counts have severe problems when confronted with any  n -grams that have not explicitly been seen before –  the zero-frequency problem .  Various smoothing methods were used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen  n -grams; see  Rule of succession ) to more sophisticated models, such as  Good–Turing discounting  or  back-off models .  Some of these methods are equivalent to assigning a  prior distribution  to the probabilities of the  n -grams and using  Bayesian inference  to compute the resulting  posterior   n -gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.
 Skip-gram language model is an attempt at overcoming the data sparsity problem that preceding (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [6] 
 Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other.
 For example, in the input text:
 the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences
 In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then
 where ≈ is made precise by stipulating that its right-hand side must be the  nearest neighbor  of the value of the left-hand side. [7] [8]  
 Syntactic  n -grams are  n -grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. [9] [10] [11]  For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic  n -grams following the tree structure of its  dependency relations : news-economic, effect-little, effect-on-markets-financial. [9] 
 Syntactic  n -grams are intended to reflect syntactic structure more faithfully than linear  n -grams, and have many of the same applications, especially as features in a  vector space model . Syntactic  n -grams for certain tasks gives better results than the use of standard  n -grams, for example, for authorship attribution. [12] 
 Another type of syntactic  n -grams are part-of-speech  n -grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech  n -grams have several applications, most commonly in information retrieval. [13] 
 n -grams find use in several areas of computer science,  computational linguistics , and applied mathematics.
 They have been used to:


Title: Editing 

Content: Copy and paste:  – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §     Cite your sources:  <ref></ref> 
 
{{}}   {{{}}}   |   []   [[]]   [[Category:]]   #REDIRECT [[]]   &nbsp;   <s></s>   <sup></sup>   <sub></sub>   <code></code>   <pre></pre>   <blockquote></blockquote>   <ref></ref> <ref name="" />   {{Reflist}}   <references />   <includeonly></includeonly>   <noinclude></noinclude>   {{DEFAULTSORT:}}   <nowiki></nowiki>   <!-- -->   <span class="plainlinks"></span> 
 
 
 Symbols:  ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ∞   ‹› «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   𝄫 ♭ ♮ ♯ 𝄪   © ® ™ 
 Latin:  A a Á á À à Â â Ä ä Ǎ ǎ Ă ă Ā ā Ã ã Å å Ą ą Æ æ Ǣ ǣ   B b   C c Ć ć Ċ ċ Ĉ ĉ Č č Ç ç   D d Ď ď Đ đ Ḍ ḍ Ð ð   E e É é È è Ė ė Ê ê Ë ë Ě ě Ĕ ĕ Ē ē Ẽ ẽ Ę ę Ẹ ẹ Ɛ ɛ Ǝ ǝ Ə ə   F f   G g Ġ ġ Ĝ ĝ Ğ ğ Ģ ģ   H h Ĥ ĥ Ħ ħ Ḥ ḥ   I i İ ı Í í Ì ì Î î Ï ï Ǐ ǐ Ĭ ĭ Ī ī Ĩ ĩ Į į Ị ị   J j Ĵ ĵ   K k Ķ ķ   L l Ĺ ĺ Ŀ ŀ Ľ ľ Ļ ļ Ł ł Ḷ ḷ Ḹ ḹ   M m Ṃ ṃ   N n Ń ń Ň ň Ñ ñ Ņ ņ Ṇ ṇ Ŋ ŋ   O o Ó ó Ò ò Ô ô Ö ö Ǒ ǒ Ŏ ŏ Ō ō Õ õ Ǫ ǫ Ọ ọ Ő ő Ø ø Œ œ   Ɔ ɔ   P p   Q q   R r Ŕ ŕ Ř ř Ŗ ŗ Ṛ ṛ Ṝ ṝ   S s Ś ś Ŝ ŝ Š š Ş ş Ș ș Ṣ ṣ ß   T t Ť ť Ţ ţ Ț ț Ṭ ṭ Þ þ   U u Ú ú Ù ù Û û Ü ü Ǔ ǔ Ŭ ŭ Ū ū Ũ ũ Ů ů Ų ų Ụ ụ Ű ű Ǘ ǘ Ǜ ǜ Ǚ ǚ Ǖ ǖ   V v   W w Ŵ ŵ   X x   Y y Ý ý Ŷ ŷ Ÿ ÿ Ỹ ỹ Ȳ ȳ   Z z Ź ź Ż ż Ž ž   ß Ð ð Þ þ Ŋ ŋ Ə ə  
 Greek:  Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω   {{Polytonic|}}  
 Cyrillic:  А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я   ́  
 IPA:  t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ β θ ð ʃ ʒ ɕ ʑ ʂ ʐ ç ʝ ɣ χ ʁ ħ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ⱱ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ʼ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ø ɘ ɵ ɤ   ə ɚ   ɛ œ ɜ ɝ ɞ ʌ ɔ   æ   ɐ ɶ ɑ ɒ   ʰ ʱ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   {{IPA|}}
 
 This page is a member of 14 hidden categories  ( help ) :


Title: None

Content: In  theoretical computer science , the  time complexity  is the  computational complexity  that describes the amount of computer time it takes to run an  algorithm . Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a  constant factor .
 Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the  worst-case time complexity , which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the  average-case complexity , which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a  function  of the size of the input. [1] : 226   Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the  asymptotic behavior  of the complexity. Therefore, the time complexity is commonly expressed using  big O notation , typically  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 ,   
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
 ,   
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
 ,   
   
     
       
         O 
         ( 
         
           2 
           
             n 
           
         
         ) 
       
     
     {\displaystyle O(2^{n})} 
   
 ,  etc., where  n  is the size in units of  bits  needed to represent the input.
 Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  is a  linear time algorithm  and an algorithm with time complexity  
   
     
       
         O 
         ( 
         
           n 
           
             α 
           
         
         ) 
       
     
     {\displaystyle O(n^{\alpha })} 
   
  for some constant  
   
     
       
         α 
         > 
         1 
       
     
     {\displaystyle \alpha >1} 
   
  is a  polynomial time algorithm .
 The following table summarizes some classes of commonly encountered time complexities. In the table,  poly( x ) =  x O (1) , i.e., polynomial in  x .
 Calculating   (−1) n  
 Kadane's algorithm .
 Linear search 
 Fast Fourier transform .
 Calculating  partial correlation .
 AKS primality test [3] [4] 
 formerly-best algorithm for  graph isomorphism 
 An algorithm is said to be  constant time  (also written as  
   
     
       
         O 
         ( 
         1 
         ) 
       
     
     {\textstyle O(1)} 
   
  time) if the value of  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)} 
   
  (the complexity of the algorithm)  is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an  array  takes constant time as only one  operation  has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each  element  in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle O(n)} 
   
  time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
 Despite the name "constant time", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task "exchange the values of  a  and  b  if necessary so that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 " is called constant time even though the time may depend on whether or not it is already true that  
   
     
       
         a 
         ≤ 
         b 
       
     
     {\textstyle a\leq b} 
   
 . However, there is some constant  t  such that the time required is always  at most   t .
 An algorithm is said to take  logarithmic time  when  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(\log n)} 
   
 .  Since  
   
     
       
         
           log 
           
             a 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{a}n} 
   
  and  
   
     
       
         
           log 
           
             b 
           
         
         ⁡ 
         n 
       
     
     {\displaystyle \log _{b}n} 
   
  are related by a  constant multiplier , and such a  multiplier is irrelevant  to big O classification, the standard usage for logarithmic-time algorithms is  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  regardless of the base of the logarithm appearing in the expression of  T .
 Algorithms taking logarithmic time are commonly found in operations on  binary trees  or when using  binary search .
 An  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when  n  increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size  n  is of the order of  n .
 An example of logarithmic time is given by dictionary search. Consider a  dictionary   D  which contains  n  entries, sorted in  alphabetical order . We suppose that, for  
   
     
       
         1 
         ≤ 
         k 
         ≤ 
         n 
       
     
     {\displaystyle 1\leq k\leq n} 
   
 , one may access the  k th entry of the dictionary in a constant time. Let  
   
     
       
         D 
         ( 
         k 
         ) 
       
     
     {\displaystyle D(k)} 
   
  denote this  k th entry. Under these hypotheses, the test to see if a word  w  is in the dictionary may be done in logarithmic time: consider  
   
     
       
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 , where  
   
     
       
         ⌊ 
         
         ⌋ 
       
     
     {\displaystyle \lfloor \;\rfloor } 
   
  denotes the  floor function . If  
   
     
       
         w 
         = 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w=D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --that is to say, the word  w  is exactly in the middle of the dictionary--then we are done. Else, if  
   
     
       
         w 
         < 
         D 
         
           ( 
           
             ⌊ 
             
               
                 n 
                 2 
               
             
             ⌋ 
           
           ) 
         
       
     
     {\displaystyle w<D\left(\left\lfloor {\frac {n}{2}}\right\rfloor \right)} 
   
 --i.e., if the word  w  comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word.
 An algorithm is said to run in  polylogarithmic  time  if its time  
   
     
       
         T 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)} 
   
  is  
   
     
       
         O 
         
           
             ( 
           
         
         ( 
         log 
         ⁡ 
         n 
         
           ) 
           
             k 
           
         
         
           
             ) 
           
         
       
     
     {\displaystyle O{\bigl (}(\log n)^{k}{\bigr )}} 
   
  for some constant  k . Another way to write this is  
   
     
       
         O 
         ( 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{k}n)} 
   
 .
 For example,  matrix chain ordering  can be solved in polylogarithmic time on a  parallel random-access machine , [7]  and  a graph  can be  determined to be planar  in a  fully dynamic  way in  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  time per insert/delete operation. [8] 
 An algorithm is said to run in  sub-linear time  (often spelled  sublinear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         n 
         ) 
       
     
     {\displaystyle T(n)=o(n)} 
   
 . In particular this includes algorithms with the time complexities defined above. 
 The specific term  sublinear time algorithm  commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to  approximately  infer properties of the entire instance. [9]  This type of sublinear time algorithm is closely related to  property testing  and  statistics .
 Other settings where algorithms can run in sublinear time include:
 An algorithm is said to take  linear time , or  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
  time, if its time complexity is  
   
     
       
         O 
         ( 
         n 
         ) 
       
     
     {\displaystyle O(n)} 
   
 . Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant  c  such that the running time is at most  
   
     
       
         c 
         n 
       
     
     {\displaystyle cn} 
   
  for every input of size  n . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
 Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit  parallelism  to provide this. An example is  content-addressable memory . This concept of linear time is used in string matching algorithms such as the  Boyer–Moore string-search algorithm  and  Ukkonen's algorithm .
 An algorithm is said to run in  quasilinear time  (also referred to as  log-linear time ) if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         O 
         ( 
         n 
         
           log 
           
             k 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle T(n)=O(n\log ^{k}n)} 
   
  for some positive constant  k ; [11]   linearithmic time  is the case  
   
     
       
         k 
         = 
         1 
       
     
     {\displaystyle k=1} 
   
 . [12]  Using  soft O notation  these algorithms are  
   
     
       
         
           
             
               O 
               ~ 
             
           
         
         ( 
         n 
         ) 
       
     
     {\displaystyle {\tilde {O}}(n)} 
   
 . Quasilinear time algorithms are also  
   
     
       
         O 
         ( 
         
           n 
           
             1 
             + 
             ε 
           
         
         ) 
       
     
     {\displaystyle O(n^{1+\varepsilon })} 
   
  for every constant  
   
     
       
         ε 
         > 
         0 
       
     
     {\displaystyle \varepsilon >0} 
   
  and thus run faster than any polynomial time algorithm whose time bound includes a term  
   
     
       
         
           n 
           
             c 
           
         
       
     
     {\displaystyle n^{c}} 
   
  for any  
   
     
       
         c 
         > 
         1 
       
     
     {\displaystyle c>1} 
   
 .
 Algorithms which run in quasilinear time include:
 In many cases, the  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  running time is simply the result of performing a  
   
     
       
         Θ 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Theta (\log n)} 
   
  operation  n  times (for the notation, see  Big O notation § Family of Bachmann–Landau notations ). For example,  binary tree sort  creates a  binary tree  by inserting each element of the  n -sized array one by one. Since the insert operation on a  self-balancing binary search tree  takes  
   
     
       
         O 
         ( 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log n)} 
   
  time, the entire algorithm takes  
   
     
       
         O 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(n\log n)} 
   
  time.
 Comparison sorts  require at least  
   
     
       
         Ω 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \Omega (n\log n)} 
   
  comparisons in the worst case because  
   
     
       
         log 
         ⁡ 
         ( 
         n 
         ! 
         ) 
         = 
         Θ 
         ( 
         n 
         log 
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle \log(n!)=\Theta (n\log n)} 
   
 , by  Stirling's approximation . They also frequently arise from the  recurrence relation   
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         2 
         T 
         
           ( 
           
             
               n 
               2 
             
           
           ) 
         
         + 
         O 
         ( 
         n 
         ) 
       
     
     {\textstyle T(n)=2T\left({\frac {n}{2}}\right)+O(n)} 
   
 .
 An  algorithm  is said to be  subquadratic time  if  
   
     
       
         T 
         ( 
         n 
         ) 
         = 
         o 
         ( 
         
           n 
           
             2 
           
         
         ) 
       
     
     {\displaystyle T(n)=o(n^{2})} 
   
 .
 For example, simple, comparison-based  sorting algorithms  are quadratic (e.g.  insertion sort ), but more advanced algorithms can be found that are subquadratic (e.g.  shell sort ). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
 An algorithm is said to be of  polynomial time  if its running time is  upper bounded  by a  polynomial  expression in the size of the input for the algorithm, that is,  T ( n ) =  O ( n k )  for some positive constant  k . [1] [13]   Problems  for which a deterministic polynomial-time algorithm exists belong to the  complexity class   P , which is central in the field of  computational complexity theory .  Cobham's thesis  states that polynomial time is a synonym for "tractable", "feasible", "efficient", or "fast". [14] 
 Some examples of polynomial-time algorithms:
 These two concepts are only relevant if the inputs to the algorithms consist of integers.
 The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following.
 P is the smallest time-complexity class on a deterministic machine which is  robust  in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given  abstract machine  will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine.
 An algorithm is defined to take  superpolynomial time  if  T ( n ) is not bounded above by any polynomial. Using  little omega notation , it is  ω ( n c ) time for all constants  c , where  n  is the input parameter, typically the number of bits in the input.
 For example, an algorithm that runs for 2 n  steps on an input of size  n  requires superpolynomial time (more specifically, exponential time).
 An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the  Adleman–Pomerance–Rumely primality test  runs for  n O (log log  n )  time on  n -bit inputs; this grows faster than any polynomial for large enough  n , but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
 An algorithm that requires superpolynomial time lies outside the  complexity class   P .  Cobham's thesis  posits that these algorithms are impractical, and in many cases they are. Since the  P versus NP problem  is unresolved, it is unknown whether  NP-complete  problems require superpolynomial time.
 Quasi-polynomial time  algorithms are algorithms whose running time exhibits  quasi-polynomial growth , a type of behavior that may be slower than polynomial time but yet is significantly faster than  exponential time . The worst case running time of a quasi-polynomial time algorithm is  
   
     
       
         
           2 
           
             O 
             ( 
             
               log 
               
                 c 
               
             
             ⁡ 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{O(\log ^{c}n)}} 
   
  for some fixed  
   
     
       
         c 
         > 
         0 
       
     
     {\displaystyle c>0} 
   
 .  When  
   
     
       
         c 
         = 
         1 
       
     
     {\displaystyle c=1} 
   
  this gives polynomial time, and for  
   
     
       
         c 
         < 
         1 
       
     
     {\displaystyle c<1} 
   
  it gives sub-linear time.
 There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed  Steiner tree problem , for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of  
   
     
       
         O 
         ( 
         
           log 
           
             3 
           
         
         ⁡ 
         n 
         ) 
       
     
     {\displaystyle O(\log ^{3}n)} 
   
  ( n  being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
 Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the  planted clique  problem in which the goal is to  find a large clique  in the union of a clique and a  random graph . Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a  computational hardness assumption  to prove the difficulty of several other problems in computational  game theory ,  property testing , and  machine learning . [15] 
 The complexity class  QP  consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of  DTIME  as follows. [16] 
 In complexity theory, the unsolved  P versus NP  problem asks if all problems in NP have polynomial-time algorithms. All the best-known algorithms for  NP-complete  problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here "sub-exponential time" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the  exponential time hypothesis . [17]  Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of  approximation algorithms  make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the  set cover  problem.
 The term  sub-exponential  time  is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of "sub-exponential" is not generally agreed upon, [18]  however the two most widely used are below.
 A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every  ε  > 0  there exists an algorithm which solves the problem in time  O (2 n ε ). The set of all such problems is the complexity class  SUBEXP  which can be defined in terms of  DTIME  as follows. [6] [19] [20] [21] 
 This notion of sub-exponential is non-uniform in terms of  ε  in the sense that  ε  is not part of the input and each ε may have its own algorithm for the problem.
 Some authors define sub-exponential time as running times in  
   
     
       
         
           2 
           
             o 
             ( 
             n 
             ) 
           
         
       
     
     {\displaystyle 2^{o(n)}} 
   
 . [17] [22] [23]  This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the  general number field sieve , which runs in time about  
   
     
       
         
           2 
           
             
               
                 
                   O 
                   ~ 
                 
               
             
             ( 
             
               n 
               
                 1 
                 
                   / 
                 
                 3 
               
             
             ) 
           
         
       
     
     {\displaystyle 2^{{\tilde {O}}(n^{1/3})}} 
   
 ,  where the length of the input is  n . Another example was the  graph isomorphism problem , which the best known algorithm from 1982 to 2016 solved in  
   
     
       
         
           2 
           
             O 
             
               ( 
               
                 
                   n 
                   log 
                   ⁡ 
                   n 
                 
               
               ) 
             
           
         
       
     
     {\displaystyle 2^{O\left({\sqrt {n\log n}}\right)}} 
   
 .  However, at  STOC  2016 a quasi-polynomial time algorithm was presented. [24] 
 It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In  parameterized complexity , this difference is made explicit by considering pairs  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  of  decision problems  and parameters  k .  SUBEPT  is the class of all parameterized problems that run in time sub-exponential in  k  and polynomial in the input size  n : [25] 
 More precisely, SUBEPT is the class of all parameterized problems  
   
     
       
         ( 
         L 
         , 
         k 
         ) 
       
     
     {\displaystyle (L,k)} 
   
  for which there is a  computable function   
   
     
       
         f 
         : 
         
           N 
         
         → 
         
           N 
         
       
     
     {\displaystyle f:\mathbb {N} \to \mathbb {N} } 
   
  with  
   
     
       
         f 
         ∈ 
         o 
         ( 
         k 
         ) 
       
     
     {\displaystyle f\in o(k)} 
   
  and an algorithm that decides  L  in time  
   
     
       
         
           2 
           
             f 
             ( 
             k 
             ) 
           
         
         ⋅ 
         
           poly 
         
         ( 
         n 
         ) 
       
     
     {\displaystyle 2^{f(k)}\cdot {\text{poly}}(n)} 
   
 .
 The  exponential time hypothesis  ( ETH ) is that  3SAT , the satisfiability problem of Boolean formulas in  conjunctive normal form  with at most three literals per clause and with  n  variables, cannot be solved in time 2 o ( n ) . More precisely, the hypothesis is that there is some absolute constant  c  > 0  such that 3SAT cannot be decided in time 2 cn  by any deterministic Turing machine. With  m  denoting the number of clauses, ETH is equivalent to the hypothesis that  k SAT cannot be solved in time 2 o ( m )  for any integer  k  ≥ 3 . [26]  The exponential time hypothesis implies  P ≠ NP .
 An algorithm is said to be  exponential time , if  T ( n ) is upper bounded by 2 poly( n ) , where poly( n ) is some polynomial in  n . More formally, an algorithm is exponential time if  T ( n ) is bounded by  O (2 n k ) for some constant  k . Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as  EXP .
 Sometimes, exponential time is used to refer to algorithms that have  T ( n ) = 2 O ( n ) , where the exponent is at most a linear function of  n . This gives rise to the complexity class  E .
 An algorithm is said to be  factorial time  if  T(n)  is upper bounded by the  factorial function   n! . Factorial time is a subset of exponential time (EXP) because  
   
     
       
         n 
         ! 
         ≤ 
         
           n 
           
             n 
           
         
         = 
         
           2 
           
             n 
             log 
             ⁡ 
             n 
           
         
         = 
         O 
         
           ( 
           
             2 
             
               
                 n 
                 
                   1 
                   + 
                   ϵ 
                 
               
             
           
           ) 
         
       
     
     {\displaystyle n!\leq n^{n}=2^{n\log n}=O\left(2^{n^{1+\epsilon }}\right)} 
   
  for all  
   
     
       
         ϵ 
         > 
         0 
       
     
     {\displaystyle \epsilon >0} 
   
 . However, it is not a subset of E.
 An example of an algorithm that runs in factorial time is  bogosort , a notoriously inefficient sorting algorithm based on  trial and error .  Bogosort sorts a list of  n  items by repeatedly  shuffling  the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the  n ! orderings of the  n  items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the  infinite monkey theorem .
 An algorithm is said to be  double exponential  time if  T ( n ) is upper bounded by 2 2 poly( n ) , where poly( n ) is some polynomial in  n . Such algorithms belong to the complexity class  2-EXPTIME .
 Well-known double exponential time algorithms include:
 


Title: None

Content: 
 

 The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [1]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a "universal linked information system". [3] [4]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs).
 The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information.
 The Web has become the world's dominant  information systems platform . [5] [6] [7] [8]  It is the primary tool that billions of people worldwide use to interact with the Internet. [2] 
 The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [9] [10] [11]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing  CERNDOC  documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the  VAX/NOTES  system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with "hot spots" embedded in the text, it helped to confirm the validity of his concept. [12] [13] 
 The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived "gateways" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Uucp News ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [3] [9] [10] [11] 
 Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [3]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [14]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [15] [16] 
 CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [17] [18]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [19] [20]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [21] [22]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [23]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [24] 
 Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [25]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [26]  In 2009, the W3C conceded and abandoned XHTML. [27]  In 2019, it ceded control of the HTML specification to the WHATWG. [28] 
 The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [29] [30] [31] [8] 
 Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [32]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means "10,000-dimensional net", a translation that reflects the design concept and proliferation of the World Wide Web.
 Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding "www." (or, indeed, ".com") to the domain. [33] 
 In English,  www  is usually read as  double-u double-u double-u . [34]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [35]  Stephen Fry, in his "Podgrams" series of podcasts, pronounces it  wuh wuh wuh . [36]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): "The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for". [37] 
 The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet's transport protocols. [2] 
 Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behavior investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [38] 
 The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text:
 The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the webserver can fulfil the request it sends an HTTP response back to the browser indicating success:
 followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this:
 The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources.
 Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [39] 
 Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document.
 HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
 HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [40] 
 Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:
 < a   href = "http://example.org/home.html" > Example.org Homepage </ a > . 
 Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [41] 
 The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  "dead" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts.
 Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many web sites do not use it; the first web server was  nxoc01.cern.ch . [42]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [43] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [44] [ dubious    –  discuss ] 
 When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix "www" to the beginning of it and possibly ".com", ".org" and ".net" at the end, depending on what might be missing. For example, entering "microsoft" may be transformed to  http://www.microsoft.com/  and "openoffice" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [45]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [46] 
 The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.
 A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device .
 The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page.
 On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server .
 A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc.
 A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application .
 Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so.
 A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.
 A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way.
 A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server.
 Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.
 JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [47]  The standardised version is  ECMAScript . [47]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [48] 
 A  website [49]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com .
 A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site.
 Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet .
 Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal .
 Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs .
 A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer.
 In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites.
 The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge .
 A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols.
 The primary function of a web server is to store, process and deliver  web pages  to  clients . [50]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content.
 A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented .
 While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files.
 Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behavior of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  ("on-the-fly") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content .
 Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ).
 An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers.
 Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [51] 
 Tracking cookies, and especially  third-party tracking cookies , are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [52]  and U.S. lawmakers to take action in 2011. [53] [54]  European law requires that all websites targeting  European Union  member states gain "informed consent" from users before storing non-essential cookies on their device.
 Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. He recommends using the browser in  incognito mode  in such circumstances. [55] 
 A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web .
 The deep web, [56]   invisible web , [57]  or  hidden web [58]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [59]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [60] 
 The content of the deep web is hidden behind  HTTP  forms, [61] [62]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others.
 The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page.
 A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites.
 For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [63]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [64] [65]  and as measured by  Google , about one in ten web pages may contain malicious code. [66]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [67]  The most common of all malware  threats  is  SQL injection  attacks against websites. [68]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [69]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [70]  Today [ as of? ]  by one estimate, 70% of all websites are open to XSS attacks on their users. [71]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [72]  Two of the well-known phishing methods are Covert Redirect and Open Redirect.
 Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [73]  and some, like  Finjan  have recommended active real-time inspection of programming code and all content regardless of its source. [63]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [74]  while others call for "ubiquitous, always-on  digital rights management " enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [75]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [76] 
 Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  online traffic and masks the original IP address lowering the chance of user identification.
 When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web.
 Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [77]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web.
 Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites.
 Web standards, in the broader sense, consist of the following:
 Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [84]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level).
 There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [85]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [86]  Tim Berners-Lee once noted, "The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect." [85]  Many countries regulate web accessibility as a requirement for websites. [87]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [85] [88] 
 The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [89]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character encoding . [90]  Originally  .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}} RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [91] 


Title: None

Content: Supervised learning  ( SL ) is a paradigm in  machine learning  where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled  supervisory signal ) train a model. The training data is processed, building a function that maps new data on expected output values. [1]   An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see  inductive bias ). This statistical quality of an algorithm is measured through the so-called  generalization error .
 To solve a given problem of supervised learning, one has to perform the following steps:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the  No free lunch theorem ).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between  bias  and  variance . [2]  Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
 . A learning algorithm has high variance for a particular input  
   
     
       
         x 
       
     
     {\displaystyle x} 
   
  if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. [3]  Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for  feature selection  that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of  dimensionality reduction , which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory  target variables ). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to  overfitting . You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data - this phenomenon has been called  deterministic noise . When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as  early stopping  to prevent  overfitting  as well as  detecting  and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased  generalization error  with  statistical significance . [4] [5] 
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross validation ). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of  
   
     
       
         N 
       
     
     {\displaystyle N} 
   
  training examples of the form  
   
     
       
         { 
         ( 
         
           x 
           
             1 
           
         
         , 
         
           y 
           
             1 
           
         
         ) 
         , 
         . 
         . 
         . 
         , 
         ( 
         
           x 
           
             N 
           
         
         , 
         
         
           y 
           
             N 
           
         
         ) 
         } 
       
     
     {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}} 
   
  such that  
   
     
       
         
           x 
           
             i 
           
         
       
     
     {\displaystyle x_{i}} 
   
  is the  feature vector  of the  
   
     
       
         i 
       
     
     {\displaystyle i} 
   
 -th example and  
   
     
       
         
           y 
           
             i 
           
         
       
     
     {\displaystyle y_{i}} 
   
  is its label (i.e., class), a learning algorithm seeks a function  
   
     
       
         g 
         : 
         X 
         → 
         Y 
       
     
     {\displaystyle g:X\to Y} 
   
 , where  
   
     
       
         X 
       
     
     {\displaystyle X} 
   
  is the input space and  
   
     
       
         Y 
       
     
     {\displaystyle Y} 
   
  is the output space. The function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is an element of some space of possible functions  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
 , usually called the  hypothesis space . It is sometimes convenient to represent  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  using a  scoring function   
   
     
       
         f 
         : 
         X 
         × 
         Y 
         → 
         
           R 
         
       
     
     {\displaystyle f:X\times Y\to \mathbb {R} } 
   
  such that  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as returning the  
   
     
       
         y 
       
     
     {\displaystyle y} 
   
  value that gives the highest score:  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         f 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)} 
   
 . Let  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  denote the space of scoring functions.
 Although  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  and  
   
     
       
         F 
       
     
     {\displaystyle F} 
   
  can be any space of functions, many learning algorithms are probabilistic models where  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  takes the form of a  conditional probability  model  
   
     
       
         g 
         ( 
         x 
         ) 
         = 
         
           
             
               arg 
               ⁡ 
               max 
             
             y 
           
         
         
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)} 
   
 , or  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  takes the form of a  joint probability  model  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
 . For example,  naive Bayes  and  linear discriminant analysis  are joint probability models, whereas  logistic regression  is a conditional probability model.
 There are two basic approaches to choosing  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  or  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 :  empirical risk minimization  and  structural risk minimization . [6]  Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a  penalty function  that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of  independent and identically distributed pairs ,  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 . In order to measure how well a function fits the training data, a  loss function   
   
     
       
         L 
         : 
         Y 
         × 
         Y 
         → 
         
           
             R 
           
           
             ≥ 
             0 
           
         
       
     
     {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}} 
   
  is defined. For training example  
   
     
       
         ( 
         
           x 
           
             i 
           
         
         , 
         
         
           y 
           
             i 
           
         
         ) 
       
     
     {\displaystyle (x_{i},\;y_{i})} 
   
 , the loss of predicting the value  
   
     
       
         
           
             
               y 
               ^ 
             
           
         
       
     
     {\displaystyle {\hat {y}}} 
   
  is  
   
     
       
         L 
         ( 
         
           y 
           
             i 
           
         
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
       
     
     {\displaystyle L(y_{i},{\hat {y}})} 
   
 .
 The  risk   
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
  of function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is defined as the expected loss of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 . This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes  
   
     
       
         R 
         ( 
         g 
         ) 
       
     
     {\displaystyle R(g)} 
   
 . Hence, a supervised learning algorithm can be constructed by applying an  optimization algorithm  to find  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 When  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a conditional probability distribution  
   
     
       
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle P(y|x)} 
   
  and the loss function is the negative log likelihood:  
   
     
       
         L 
         ( 
         y 
         , 
         
           
             
               y 
               ^ 
             
           
         
         ) 
         = 
         − 
         log 
         ⁡ 
         P 
         ( 
         y 
         
           | 
         
         x 
         ) 
       
     
     {\displaystyle L(y,{\hat {y}})=-\log P(y|x)} 
   
 , then empirical risk minimization is equivalent to  maximum likelihood estimation .
 When  
   
     
       
         G 
       
     
     {\displaystyle G} 
   
  contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called  overfitting .
 Structural risk minimization  seeks to prevent overfitting by incorporating a  regularization penalty  into the optimization. The regularization penalty can be viewed as implementing a form of  Occam's razor  that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  is a linear function of the form
 A popular regularization penalty is  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           β 
           
             j 
           
           
             2 
           
         
       
     
     {\displaystyle \sum _{j}\beta _{j}^{2}} 
   
 , which is the squared  Euclidean norm  of the weights, also known as the  
   
     
       
         
           L 
           
             2 
           
         
       
     
     {\displaystyle L_{2}} 
   
  norm. Other norms include the  
   
     
       
         
           L 
           
             1 
           
         
       
     
     {\displaystyle L_{1}} 
   
  norm,  
   
     
       
         
           ∑ 
           
             j 
           
         
         
           | 
         
         
           β 
           
             j 
           
         
         
           | 
         
       
     
     {\displaystyle \sum _{j}|\beta _{j}|} 
   
 , and the  
   
     
       
         
           L 
           
             0 
           
         
       
     
     {\displaystyle L_{0}} 
   
  "norm" , which is the number of non-zero  
   
     
       
         
           β 
           
             j 
           
         
       
     
     {\displaystyle \beta _{j}} 
   
 s. The penalty will be denoted by  
   
     
       
         C 
         ( 
         g 
         ) 
       
     
     {\displaystyle C(g)} 
   
 .
 The supervised learning optimization problem is to find the function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that minimizes
 The parameter  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  controls the bias-variance tradeoff. When  
   
     
       
         λ 
         = 
         0 
       
     
     {\displaystyle \lambda =0} 
   
 , this gives empirical risk minimization with low bias and high variance. When  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  is large, the learning algorithm will have high bias and low variance. The value of  
   
     
       
         λ 
       
     
     {\displaystyle \lambda } 
   
  can be chosen empirically via  cross validation .
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 ,  
   
     
       
         − 
         log 
         ⁡ 
         P 
         ( 
         g 
         ) 
       
     
     {\displaystyle -\log P(g)} 
   
 , in which case  
   
     
       
         J 
         ( 
         g 
         ) 
       
     
     {\displaystyle J(g)} 
   
  is the  posterior probability  of  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
 .
 The training methods described above are  discriminative training  methods, because they seek to find a function  
   
     
       
         g 
       
     
     {\displaystyle g} 
   
  that discriminates well between the different output values (see  discriminative model ). For the special case where  
   
     
       
         f 
         ( 
         x 
         , 
         y 
         ) 
         = 
         P 
         ( 
         x 
         , 
         y 
         ) 
       
     
     {\displaystyle f(x,y)=P(x,y)} 
   
  is a  joint probability distribution  and the loss function is the negative log likelihood  
   
     
       
         − 
         
           ∑ 
           
             i 
           
         
         log 
         ⁡ 
         P 
         ( 
         
           x 
           
             i 
           
         
         , 
         
           y 
           
             i 
           
         
         ) 
         , 
       
     
     {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),} 
   
  a risk minimization algorithm is said to perform  generative training , because  
   
     
       
         f 
       
     
     {\displaystyle f} 
   
  can be regarded as a  generative model  that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in  naive Bayes  and  linear discriminant analysis .
 There are several ways in which the standard supervised learning problem can be generalized:


Title: None

Content: 
 Wikipedia makes no guarantee of validity 
 Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.
 That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However,  Wikipedia cannot guarantee the validity of the information found here.  The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works  also have disclaimers .
 Our active community of editors uses tools such as the  Special:RecentChanges  and  Special:NewPages  feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual  peer review , they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or  featured article  processes may later have been edited inappropriately, just before you view them.
 None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages. 
 Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in  any way connected  with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.
 There is  no agreement or understanding between you and Wikipedia  regarding your use or modification of this information beyond the  Creative Commons Attribution-Sharealike 4.0 Unported License  (CC BY-SA) and the  GNU Free Documentation License  (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.
 Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.
 Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to  personality rights , independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use.  You are solely responsible for ensuring that you do not infringe someone else's personality rights. 
 Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.
 If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.


Title: None

Content: You are free:
 for any purpose, even commercially.
 The licensor cannot revoke these freedoms as long as you follow the license terms.
 Under the following terms:
 Notices:
 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.
 Your exercise of the Licensed Rights is expressly made subject to the following conditions.
 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
 For the avoidance of doubt, this Section  4  supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
 Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the "Licensor." The text of the Creative Commons public licenses is dedicated to the public domain under the  CC0 Public Domain Dedication . Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at  creativecommons.org/policies , Creative Commons does not authorize the use of the trademark "Creative Commons" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


Title: None

Content: 
This tracking category includes pages which transclude {{ multiple image }} with auto scaled images.
 This category has the following 7 subcategories, out of 7 total.
 The following 200 pages are in this category, out of approximately 19,352 total.  This list may not reflect recent changes .


Title: None

Content: This category combines all articles with unsourced statements from January 2022  (2022-01)  to enable us to work through the backlog more systematically. It is a member of  Category:Articles with unsourced statements . 
To add an article to this category add      {{ Citation needed |date=January 2022}}  to the article. If you omit the date a  bot  will add it for you at some point.
 The following 200 pages are in this category, out of approximately 5,778 total.  This list may not reflect recent changes .


Title: None

Content: 
 Works anywhere in the text         
 ''italics'', '''bold''', and '''''both''''' 
 italics ,  bold , and   both 
 [[copy edit]] 
 [[copy edit]]ors 
 copy edit 
 copy editors 
 " Pipe " a link to change the link's text
 [[Android (operating system)|Android]] 
 Android 
 Link to a section
 [[Frog#Locomotion]] 
 [[Frog#Locomotion|locomotion in frogs]] 
 {{slink|Frog#Locomotion}} 
 Frog#Locomotion 
 locomotion in frogs 
 Frog § Locomotion 
 [[Red link example]] 
 Red link example 
 https://www.wikipedia.org 
 https://www.wikipedia.org 
 [https://www.wikipedia.org] 
 [1] 
 [https://www.wikipedia.org/ Wikipedia] 
 Wikipedia 
 Hello [1]  World! [2] 
 Hello again! [1] [3] 
 This statement is true.{{cn}} 
 This statement is true. [ citation needed ] 
 ~~~~ do not sign in an article, only on talk pages 
 Username  ( talk ) 04:19, 22 April 2024 (UTC)
 [[User:Example]]  or  {{u|Example}} 
 User:Example  or  Example 
 <s> This topic isn't [[ WP:N | notable ]]. </s> 
 This topic isn't  notable . 
 <u>This topic is notable</u> 
 This topic is notable 
 <!-- This had consensus, discuss at talk page --> 
 
 [[File:Wiki.png|thumb|Caption]] 
 
 #REDIRECT [[Target page]] 
   Target page 
 #REDIRECT [[Target page#anchorName]] 
   Target page#anchorName 
 == Level 2 == 
 === Level 3 === 
 ==== Level 4 ==== 
 ===== Level 5 ===== 
 ====== Level 6 ====== 
 do not use   = Level 1 =   as it is for page titles 
 * One 
 * Two 
 ** Two point one 
 * Three 
 # One 
 # Two 
 ## Two point one 
 # Three 
 no indent (normal) 
 : first indent 
 :: second indent 
 ::: third indent 
 :::: fourth indent 
 {{Outdent|4}} return to left margin 
 no indent (normal) 
 
   Kindness Campaign 


Title: None

Content: Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the  first main contributor  rule or by virtue of  close national ties  to the subject belong in this category. Use  {{ Use dmy dates }}  to add an article to this category. See  MOS:DATE .
 This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and  not  as a clean up.
 The following 200 pages are in this category, out of approximately 22,994 total.  This list may not reflect recent changes .


Title: None

Content: 
 This category has only the following subcategory.
 The following 200 pages are in this category, out of approximately 22,081 total.  This list may not reflect recent changes .


Title: None

Content: The following 5 pages are in this category, out of  5 total.  This list may not reflect recent changes .


Title: None

Content: This category and its subcategories contain project pages which have been  fully protected  in line with the  protection policy . Full protection prevents a page from being edited except by  administrators . For semi-protected pages, which cannot be edited by  anonymous and newly registered users , see  Category:Wikipedia semi-protected pages .
 To add a page to this category, use  {{ pp-protected }} . This should only be done if the page is in fact protected – adding the template does not in itself protect the page.
 The following 111 pages are in this category, out of  111 total.  This list may not reflect recent changes .


Title: None

Content: This page serves as a directory of  copyright resources  on Wikipedia.


Title: None

Content: This category is used for tracking articles with  excerpts . Add it to your watchlist to be notified when a new article includes an excerpt.
 The following 200 pages are in this category, out of approximately 10,251 total.  This list may not reflect recent changes .


